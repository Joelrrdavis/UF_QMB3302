{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1221cf-a54b-4990-9bec-f7f93875262f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e4da9-9f8e-432e-aebb-b4a0aa7aa042",
   "metadata": {},
   "source": [
    "#### Lecture example\n",
    "\n",
    "The below is just provided in case you need/want a more detailed example of measuring the distance. Most of you can safely ignore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2fcac6-e9a4-44bf-a212-13c27a6a9b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "\n",
    "a = np.array([3, 4, 1])\n",
    "b = np.array([1, 2, 2])\n",
    "print( a-b)\n",
    "edist = m.sqrt(sum((a-b)**2))\n",
    "print(\"Distance from a to b: \", edist) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8fb886-e67b-40fe-8d6a-83726295822e",
   "metadata": {},
   "source": [
    "## Textbook Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03da7bd-02a8-4287-8e10-a85387ee5f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "\n",
    "#if you are following along in the text book, you need to add the below code.\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1ef61-9fdb-4992-a30b-5f25460fc2e4",
   "metadata": {},
   "source": [
    "### Blobs?\n",
    "\n",
    "What are we doing here? This is just a cheesy but quick way to create a large number of data samples, so we can demonstrate the algorithm. That is all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435eae3-1451-4547-bd74-41b8eaf10d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=.6, random_state=0)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1a8b3-fc89-4c70-aefa-0e6db179f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f28648-15ef-42e5-8723-41ed81b7b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364484d-f466-4da5-92e2-e69772548fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac636c6a-b956-47ed-b7c3-4587fd3017e2",
   "metadata": {},
   "source": [
    "The choice of how many clusters can get a little confusing... lets just try this with a different k, and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee242f2c-63b3-4392-af2b-5d276d1ae830",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=7)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e29a5c8-d093-4c1c-ae6e-bdc37d91ab75",
   "metadata": {},
   "source": [
    "## Example using Simulated Retail Sales Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fbdb4f-26d2-4729-9d25-16f900d8082e",
   "metadata": {},
   "source": [
    "This examples uses data derived from the UCI Machine Learning dataset Online Retail. This is a well used data set, and is intended to be used to show clustering and classification tasks. The upside of that: lots of versions and ideas on how to cluster on this dataset can be found online. The below code comes from various sources and texts including the scikit learn help pages. This is a fairly standard (if uninspiring) way to do kmeans, and a standard (if uninspiring) well documented example. \n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Online+Retail\n",
    "\n",
    "https://scikit-learn.org/stable/search.html?q=kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f141d5-f6e5-41ea-bcf8-88dc8199249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "path = \"../data/rfmfile.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13d44e9-08af-48cc-b0e9-0d4fcdc69c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x= 'recency', y='frequency', data=df, kind = 'scatter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022610a-fa5b-43a4-9e84-3be85cad94d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13685f33-6d58-4e96-9d58-aa5ecb448fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelvar = df.loc[:,\n",
    "    ['recency', 'frequency', 'monetary_value']]\n",
    "\n",
    "sns.pairplot(modelvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3dce6a-a81c-4fb0-90b5-93e9b0c4e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust plot size\n",
    "sns.heatmap(modelvar.corr(), cmap = 'Wistia', annot = True)\n",
    "plt.title('Correl. for model data', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215bf94-f0c4-490b-8f82-dd317597d410",
   "metadata": {},
   "source": [
    "## Getting ready for K means in a nutshell\n",
    "\n",
    "(or a Python Shell anyway)\n",
    "\n",
    "K means has some assumptions, we won't go into a lot of detail in this overview, but the data we have probably needs some cleaning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c9483-e196-46ac-bc88-11787b794d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('customer_id', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a40f6-0580-4b24-bb43-c13edf8ff858",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab6ff6-4f45-4613-b454-c0dcfc793816",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['recency'].plot(kind='kde', figsize=(15, 3))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a9512-7ef2-4040-bf52-ccc9c0c9094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['frequency'].plot(kind='kde', figsize=(15, 3))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ee3dd-ec83-48c3-82f7-bfb0bbafe199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['monetary_value'].plot(kind='kde', figsize=(15, 3))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0740b99-2c0c-429c-b183-a3020075ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#... it is also easy to do this all in one line\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "df.plot(kind='density', subplots=True, sharex=False, figsize=(16, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded75c8-109b-4587-a72d-f9037562c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standarizing \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "rfm_std = scale.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56456d83-7520-4ce8-a12d-185e817e25b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa274b-55f3-4cf9-a23a-c17c12f185a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std = pd.DataFrame(data = rfm_std, \n",
    "                            index = df.index, \n",
    "                            columns = df.columns)\n",
    "df_std.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8dae81-aa5d-4175-b570-993c0e6c7d42",
   "metadata": {},
   "source": [
    "### Let's do some K means\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a5067-28f5-400e-9a6c-2f3a2993c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c795b48-f02c-4e44-bc95-9b33552aac58",
   "metadata": {},
   "source": [
    "#### Deciding on K\n",
    "\n",
    "A popular way of determining K is through silhoutte coefficients. If you'd like to read more on these, links are below. They are somewhat problematic, for reasons we won't delve into, but are easier to understand than some of the other methods. A silhouette coefficient of 1 means that the value(s) are far away from other clusters. This is a good thing, because we want our clusters to be seperated. -1 means the values are very close to other clusters, so they be mis-assigned. This is bad.\n",
    "\n",
    "Shortcut to the above:\n",
    "\n",
    "* Silhouette coefficients are okay to use if you have nothing better (like domain knowledge or even a graph)\n",
    "* +1 = good\n",
    "* -1 = bad\n",
    "\n",
    "Ok... now to do the work. Don't try and memorize this code. Just copy paste it. \n",
    "\n",
    "##### Option 1\n",
    "The first \"hack\" will get us a quick a dirty graphical representation of the scores. Where the plot stops the steep decline a starts leveling out, thats your K. Yup, that sounds about as unscientific as it is! This happens to be my go to approach, because it is just so easy. \n",
    "\n",
    "##### Option 2\n",
    "This one comes from the scikit learn documentation. I think this is a better approach-- but better in clustering is pretty subjective. It does just give you a nice \"this is the score\" output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f939c41-5d36-4423-a32c-ce8f97a3a3d0",
   "metadata": {},
   "source": [
    "##### Here is Option 1\n",
    "\n",
    "We need to decide on the number of clusters. We google and find lot's of solutions using \"elbow plots\". Here is one of them:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "This is a fairly common way to develop this elbow plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33586b-a219-45df-9b6f-9842fe1dc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_sse = {}\n",
    "for k in range(1, 11):\n",
    "    km = KMeans(n_clusters=k,\n",
    "                random_state=1957) \n",
    "    km.fit(df_std)\n",
    "    elbow_sse[k] = km.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435076d-33c0-43ce-b525-70574ab4994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(x=list(elbow_sse.keys()), y=list(elbow_sse.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3e6c4-c537-4edb-bcfa-a30682531f2a",
   "metadata": {},
   "source": [
    "##### Option 2\n",
    "\n",
    "For this option we need to use our array data, not the pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5710ce3-b628-40b1-b499-851d3f62f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfm_std\n",
    "\n",
    "X = rfm_std\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc331b8-789c-4292-8442-67f35165edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aadcd5-d2b8-44d9-a01a-5c3048118ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets start with 2\n",
    "k = 2\n",
    "kmeans = KMeans(n_clusters=k, random_state=1957)\n",
    "kmeans.fit(df_std)\n",
    "\n",
    "df_cluster2 = df_std.assign(Cluster=kmeans.labels_)\n",
    "\n",
    "df_cluster2.groupby('Cluster').agg({\n",
    "    'recency': 'mean',\n",
    "    'frequency': 'mean',\n",
    "    'monetary_value': ['mean', 'count'],\n",
    "}).round(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761451db-4930-4daf-b475-b762435e4c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans = kmeans.predict(X)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c024d0ad-d1f4-40e5-b181-05d5782ddda6",
   "metadata": {},
   "source": [
    "darn it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c97691-5f17-4f76-a0f2-27f1d16ff436",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=1957)\n",
    "kmeans.fit(df_std)\n",
    "df_cluster3 = df_std.assign(Cluster=kmeans.labels_)\n",
    "df_cluster3.groupby('Cluster').agg({\n",
    "    'recency': 'mean',\n",
    "    'frequency': 'mean',\n",
    "    'monetary_value': ['mean', 'count'],\n",
    "}).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7d2e1-10f1-4c51-8337-9d13165616cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans = kmeans.predict(X)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95f82b-f8cf-4efc-a334-dd17f872a22a",
   "metadata": {},
   "source": [
    "super darn it... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa23855-0f05-47f3-8c2b-540c458c51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "kmeans = KMeans(n_clusters=k, random_state=1957)\n",
    "kmeans.fit(df_std)\n",
    "df_cluster4 = df_std.assign(Cluster=kmeans.labels_)\n",
    "df_cluster4.groupby('Cluster').agg({\n",
    "    'recency': 'mean',\n",
    "    'frequency': 'mean',\n",
    "    'monetary_value': ['mean', 'count'],\n",
    "}).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022dddb0-444d-460b-a6aa-803256c3bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans = kmeans.predict(X)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b92ff1-76c8-4373-ad9b-f8ce21e7fe48",
   "metadata": {},
   "source": [
    "... this is getting stupid.... are any of these actually useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b6087-bb87-4d78-86cc-85fa5f27d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = pd.melt(df_std.assign(Cluster=kmeans.labels_).reset_index(),\n",
    "                        id_vars=['customer_id', 'Cluster'],\n",
    "                        value_vars=['recency', 'frequency', 'monetary_value'],\n",
    "                        var_name='rfm category', \n",
    "                        value_name='Value'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b286c6-aa0f-448c-acff-aa170540fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Plot of variables and clusters')\n",
    "sns.lineplot(data=df_m, x='rfm category', y='Value', hue='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79e78a-59af-45f5-bdb6-2549d0adbf08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b277d2f-e138-46d3-b2ac-94d4923defe1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Agglolmerative/Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c548fa-eb5f-4092-80c6-63bca4b1ae69",
   "metadata": {},
   "source": [
    "## Example using Simulated Credit Card Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7061cb8e-678b-4e0e-b91c-6e61f350459c",
   "metadata": {},
   "source": [
    "This dataset comes to us from Kaggle. Check it out at the link below. One reason I really like using these types of examples is the many derivative works (code in Python and other languages) that you can find using this data. It makes it easier to learn when you can follow along with multiple examples. \n",
    "\n",
    "https://www.kaggle.com/datasets/arjunbhasin2013/ccdata?resource=download\n",
    "\n",
    "Here is just one example (I am not arguing it is good or bad...but it is a nicely written example!)\n",
    "\n",
    "https://www.kaggle.com/code/ankits29/credit-card-customer-clustering-with-explanation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22385c-18d8-4e4d-933f-16930f9d53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "path = \"../data/cc_data.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c966b57-e5e7-4de2-9710-3428b797bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f12d0b-d475-43ef-bfca-10bb080a14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57d79e-fec9-441c-ad34-4ce77ded9d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b69714-b13a-4b77-978b-f6d376388065",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CUST_ID.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747ef9f-0e30-43d6-9e17-7cb25d195214",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('CUST_ID', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded0501-72ad-4e39-93e8-f83ca981f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns= df.columns.str.lower()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621f440-808a-4c47-9931-4fe24faddd30",
   "metadata": {},
   "source": [
    "Before we do anything with the missing data. We should check to see if it is skewed. Dealing with missing data is always a little tricky... and sometimes takes some time with a dataset to get right. \n",
    "\n",
    "One way to check for skew is a graph (I have shown those before!). Another is using the .skew method in Pandas. Skew is a measure of the asymmetry of the distribution. O would mean it is not skewed, - means it has a tail on the left... and + it has a tail on the right. \n",
    "\n",
    "Would we EXPECT to see a strong negative in something like minimum_payment? Not really. The column is bound by 0 at the low end. So it should be positive (to the right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58333018-0023-4fc3-bfd8-d2566da46c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651f0ac-ccaf-4d92-8f03-19cca0c9917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.minimum_payments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e0fc0-5115-43a9-96cf-20e23fd8798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix the missing values by filling them in\n",
    "\n",
    "#frontfill\n",
    "#df.fillna(method='ffill', inplace = True)\n",
    "\n",
    "#backfill\n",
    "#df.fillna(method='bfill', inplace= True)\n",
    "\n",
    "#median..this code also be mean or mode\n",
    "df.fillna(df.median(), inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40712b-135b-43e1-9d37-4b70ffcd6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d31e83-0603-4d54-bbc1-c913b947f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust plot size\n",
    "sns.heatmap(df.corr(), cmap = 'Wistia')\n",
    "plt.title('Correl. for model data', fontsize = 0\n",
    "         )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed17ad-c05a-45bd-a96d-d7dda6c20c64",
   "metadata": {},
   "source": [
    "## Let's scale these values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f022435-c900-4359-ac99-1140a43d13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standarizing \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# unskewing\n",
    "scale = StandardScaler()\n",
    "df_std = scale.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a23689-0bc5-4f9c-ac05-2bacb8b107bf",
   "metadata": {},
   "source": [
    "Here we are going to apply an approach called principle component analysis (PCA) to our data. PCA (in a nutshell) reduces the dimensions of our data while trying to preserve as much of the items information as possible. \n",
    "\n",
    "If you are interested in reading more on PCA, I have linked a few resources. We won't cover it more in this video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584893e-08b0-48ec-b59f-34355be24451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "df_pca = pca.fit_transform(df_std)\n",
    "df_dr = pd.DataFrame(df_pca)\n",
    "df_dr.columns = ['P1', 'P2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a643be-fee2-40e9-92bb-61133deff268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4bac17-a666-4d91-aa49-521d0bddb01b",
   "metadata": {},
   "source": [
    "The method we are using below (Ward linkage) is a little different than the simplified lecture video. The documentation has more detail if you are interested in learning more...  but that is optional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c23de04-c074-4aa6-a9b8-3c04e269b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(8, 8))\n",
    "plt.title('Our Nice Clusters')\n",
    "Dendrogram = shc.dendrogram((shc.linkage(df_dr, \n",
    "                                         method ='ward')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74dafb-e2d5-4f27-ad97-841ec975f726",
   "metadata": {},
   "source": [
    "What is the right number of clusters? How many clusters should these transactions be binned into?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf02d8-c17a-4994-af1d-40c83bd7b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this simple code should tell us!\n",
    "return(truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eacaba-e4bc-44c3-bcc4-75206ad7ad6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba30ba-9bdf-4009-8d05-d56084d499e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clustering_model = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='ward')\n",
    "clustering_model.fit(df_dr)\n",
    "segment_labels = clustering_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ef706-e9da-4058-a9c3-96a8956f20c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(segment_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79579929-89c0-40c7-8546-4637ac60905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets add these to our data...\n",
    "df['newSegments'] = segment_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca747099-2378-42e4-ae29-91de4c8219c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a1729-36ca-4a27-b226-8de1680fee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(df, \n",
    "               index=df.newSegments,\n",
    "               aggfunc='mean')\n",
    "\n",
    "#mean is the default agg function for pivot tables... just sharing the full(er) code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d9ed2-8e60-4ea1-a289-7e0653913a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5857e-255d-4d61-b9ef-907911d6615f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
