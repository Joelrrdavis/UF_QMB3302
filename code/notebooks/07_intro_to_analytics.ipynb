{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Analytics\n",
    "\n",
    "## Introduction to Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#make the plots show up inline\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn's Estimator API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most commonly, the steps in using the Scikit-Learn estimator API are as follows (we will step through a handful of detailed examples in the sections that follow).\n",
    "\n",
    "1. Choose a class of model by importing the appropriate estimator class from Scikit-Learn.\n",
    "\n",
    "2. Choose model hyperparameters by instantiating this class with desired values.\n",
    "\n",
    "3. Arrange data into a features matrix and target vector following the discussion above.\n",
    "\n",
    "4. Fit the model to your data by calling the fit() method of the model instance.\n",
    "\n",
    "5. Apply the model to new data:\n",
    "    * For supervised learning, often we predict labels for unknown data using the predict() method.\n",
    "    * For unsupervised learning, we often transform or infer properties of the data using the transform() or predict() method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using Fake Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0- Generating Synthetic data\n",
    "\n",
    "This is a fancy way of saying fake data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate the synthetic data\n",
    "np.random.seed(0)  # this seed allows us to reproduce the data across machines\n",
    "X = 2.5 * np.random.randn(1000) + 1.5   # Array of 1000 values with mean = 1.5, stddev = 2.5\n",
    "y = X * 2 + np.random.randn(1000) * 2  # Actual values of Y\n",
    "\n",
    "# Splitting the data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Visualizing the generated data\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training data')\n",
    "plt.scatter(X_test, y_test, color='green', label='Testing data')\n",
    "plt.title(\"Synthetic Linear Data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Choosing a Model Class\n",
    "For a linear regression problem, we will use the LinearRegression estimator class from Scikit-Learn's linear_model module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Choosing Model Hyperparameters\n",
    "When instantiating the LinearRegression class, we can specify hyperparameters. For our simple linear regression, we'll use the default settings, meaning no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 - choose hyperparamenters\n",
    "model = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Arranging Data into a Features Matrix and Target Vector\n",
    "Our data is already split into a features matrix X and a target vector y, as required by Scikit-Learn's API. We need to ensure X is in the correct shape (a two-dimensional array), especially when dealing with a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = X_test.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Fitting the Model to Your Data\n",
    "Now, we train our model on the training data using the fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Applying the Model to New Data\n",
    "Finally, we can make predictions using our trained model. For supervised learning tasks like ours, we use the predict() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Visualizing the model's predictions alongside the original data\n",
    "plt.scatter(X_test, y_test, color='green', label='Testing data')\n",
    "plt.plot(X_test, y_pred, color='red', label='Model Prediction')\n",
    "plt.title(\"Model Predictions vs. Testing Data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting more results\n",
    "\n",
    "After fitting the model to your training data, you can access the model's slope (coefficient) and intercept directly via the .coef_ and .intercept_ attributes. To evaluate the performance of your model, particularly how well it generalizes to unseen data, you can use the .score() method to compute the $R^{2}$ score on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below assunmes `model` is your LinearRegression instance and you've already fit \n",
    "# it to your training data. Later we will change the name of our model instance, so watch this\n",
    "\n",
    "# Extracting the coefficient and intercept\n",
    "coefficient = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "# Calculating the R^2 score\n",
    "r2_score = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Coefficient (Slope): {coefficient[0]}\")\n",
    "print(f\"Intercept: {intercept}\")\n",
    "print(f\"R^2 Score: {r2_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning about data shape\n",
    "\n",
    "### Example\n",
    "When working with a pandas DataFrame and selecting predictor variables for a regression model, you may need to reshape your data to fit the expected input format for Scikit-Learn models. Typically, Scikit-Learn expects the features (X) to be a two-dimensional array (matrix) of shape (n_samples, n_features) and the target (y) to be a one-dimensional array of shape (n_samples,). Here's how you can use the .reshape() method and other techniques to prepare your data correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'feature1': np.random.rand(100),\n",
    "    'target': np.random.rand(100)\n",
    "})\n",
    "\n",
    "# Selecting the predictor and target variables\n",
    "#X = df[['feature1']]  # This keeps X as a DataFrame, which is already 2D.\n",
    "#y = df['target']\n",
    "\n",
    "# Alternatively, if you select the column as a Series, reshape is required:\n",
    "X = df['feature1'].values.reshape(-1, 1)  # Reshape to 2D array\n",
    "y = df['target'].values  # y can stay as 1D array\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1: Single Predictor Variable\n",
    "If your DataFrame has a single predictor variable, selecting this column will result in a pandas Series. You need to reshape this Series into a two-dimensional array.\n",
    "\n",
    "Suppose you have a DataFrame df with a column 'feature1' as your predictor and 'target' as your target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'feature1': np.random.rand(100),\n",
    "    'target': np.random.rand(100)\n",
    "})\n",
    "\n",
    "# Selecting the predictor and target variables\n",
    "X = df[['feature1']]  # This keeps X as a DataFrame, which is already 2D.\n",
    "y = df['target']\n",
    "\n",
    "# Alternatively, if you select the column as a Series, reshape is required:\n",
    "#X = df['feature1'].values.reshape(-1, 1)  # Reshape to 2D array\n",
    "#y = df['target'].values  # y can stay as 1D array\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Using df[['feature1']] (note the double brackets) keeps X as a DataFrame, which is inherently two-dimensional. If you use df['feature1'].values or df['feature1'].to_numpy(), it returns a one-dimensional NumPy array, hence the need for .reshape(-1, 1) to convert it into a 2D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: Multiple Predictor Variables\n",
    "If you're selecting multiple predictor variables, pandas will keep the data in a two-dimensional structure, which is what Scikit-Learn expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(1953)\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 100\n",
    "feature1 = np.random.rand(n_samples) * 10  # Feature 1: Random values scaled up to 10\n",
    "feature2 = np.random.rand(n_samples) * 20  # Feature 2: Random values scaled up to 20\n",
    "feature3 = np.random.rand(n_samples) * 5   # Feature 3: Random values scaled up to 5\n",
    "\n",
    "# Create a target variable with a linear combination of features plus some noise\n",
    "target = 3 * feature1 + 2 * feature2 - 4 * feature3 + np.random.randn(n_samples) * 3\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'feature1': feature1,\n",
    "    'feature2': feature2,\n",
    "    'feature3': feature3,\n",
    "    'target': target\n",
    "})\n",
    "\n",
    "# the first 5 rows \n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'feature1', 'feature2', 'feature3' are your predictors\n",
    "X = df[['feature1', 'feature2', 'feature3']]\n",
    "y = df['target'].values\n",
    "\n",
    "# Splitting the data into training and testing sets remains the same\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, X is already in the correct shape because selecting multiple columns from a DataFrame results in another DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pro tip\n",
    "When you encounter an error related to the shape of your data, especially with Scikit-Learn, it often helps to check the dimensions of your arrays with .shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)  # Should be (n_samples, n_features)\n",
    "print(y.shape)  # Should be (n_samples,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Splitting data\n",
    "\n",
    "Splitting data into training and testing sets is a fundamental practice in machine learning to evaluate the performance of a model. It helps in understanding how well the model generalizes to unseen data. To demonstrate its value, let's create an example using Scikit-Learn and Python where we compare the performance of a model on the training set versus the testing set.\n",
    "\n",
    "We'll use a simple linear regression model with synthetic data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "#### Generating Synthetic Data\n",
    "First, we generate synthetic data that has a linear relationship, but with added noise to simulate real-world data imperfections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#step 1: Choose a model class. \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data\n",
    "X = np.random.rand(100, 1) * 10  # 100 data points in the range 0-10\n",
    "y = 2 * X.squeeze() + 1 + np.random.randn(100) * 2  # Linear relation with noise\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model\n",
    "Next, we train a linear regression model on the training set and evaluate its performance both on the training set and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: choose the model hyperparameters\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Arrange the data\n",
    "print(X.shape)  # Should be (n_samples, n_features)\n",
    "print(y.shape)  # Should be (n_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Performance evaluation\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training MSE: {train_mse:.2f}, Training R^2: {train_r2:.2f}\")\n",
    "print(f\"Testing MSE: {test_mse:.2f}, Testing R^2: {test_r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error (MSE) and $R^{2}$ score on the training and testing sets will give us an idea of how well the model performs. A significant difference in performance between the training and testing sets can indicate overfitting: the model performs well on the training data but fails to generalize to new, unseen data. Splitting the data into training and testing sets helps us detect this issue and take steps to address it, such as simplifying the model, using regularization techniques, or gathering more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot training data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training data')\n",
    "plt.plot(X_train, y_train_pred, color='red', label='Model Prediction')\n",
    "plt.title(\"Model Fit on Training Data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot testing data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, color='green', label='Testing data')\n",
    "plt.plot(X_test, y_test_pred, color='red', label='Model Prediction')\n",
    "plt.title(\"Model Prediction on Testing Data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An overfit example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use some synthetic data (fake). We are going to fit two models to it: one with a reasonable complexity for the data and another with excessive complexity. \n",
    "\n",
    "Overfitting occurs when a model learns the training data **too well**, capturing noise as if it were a true pattern, which negatively impacts its performance on new, unseen data.\n",
    "\n",
    "We'll use polynomial regression as our example. A simple linear regression model will serve as the reasonably complex model, and a high-degree polynomial regression model will serve as the overly complex model. We'll use Scikit-Learn for model fitting and matplotlib for visualization.\n",
    "\n",
    "Don't get lost in the model... this is just to demonstrate overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = 0.5 * X.squeeze() ** 2 + np.random.randn(100) * 1.5 + 2\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very simple: Linear regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Very complex: High-degree polynomial regression model\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree=15), LinearRegression())\n",
    "poly_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_train_linear = linear_model.predict(X_train)\n",
    "y_pred_test_linear = linear_model.predict(X_test)\n",
    "\n",
    "y_pred_train_poly = poly_model.predict(X_train)\n",
    "y_pred_test_poly = poly_model.predict(X_test)\n",
    "\n",
    "# MSE calculations\n",
    "mse_train_linear = mean_squared_error(y_train, y_pred_train_linear)\n",
    "mse_test_linear = mean_squared_error(y_test, y_pred_test_linear)\n",
    "\n",
    "mse_train_poly = mean_squared_error(y_train, y_pred_train_poly)\n",
    "mse_test_poly = mean_squared_error(y_test, y_pred_test_poly)\n",
    "\n",
    "# Display MSE\n",
    "print(f\"Linear Regression - Training MSE: {mse_train_linear:.2f}, Testing MSE: {mse_test_linear:.2f}\")\n",
    "print(f\"Polynomial Regression - Training MSE: {mse_train_poly:.2f}, Testing MSE: {mse_test_poly:.2f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Linear model\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train, y_train, color='lightgray', label='Training data')\n",
    "plt.scatter(X_test, y_test, color='gold', label='Testing data')\n",
    "plt.plot(np.sort(X_train.squeeze()), linear_model.predict(np.sort(X_train, axis=0)), color='red', label='Linear Model')\n",
    "plt.title(\"Linear Regression Fit\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "\n",
    "# Polynomial model\n",
    "plt.subplot(1, 2, 2)\n",
    "X_fit = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "plt.scatter(X_train, y_train, color='lightgray', label='Training data')\n",
    "plt.scatter(X_test, y_test, color='gold', label='Testing data')\n",
    "plt.plot(X_fit, poly_model.predict(X_fit), color='blue', label='Polynomial Model')\n",
    "plt.title(\"Polynomial Regression Fit\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
