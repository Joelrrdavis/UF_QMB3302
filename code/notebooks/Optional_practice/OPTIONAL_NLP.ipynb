{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010cfaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "example = [\n",
    "    'This instrument measures the planes altitude',\n",
    "    'This instrument measures the planes latitude'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3018d",
   "metadata": {},
   "source": [
    "### Tokenization with Out-of-Vocabulary Handling\n",
    "\n",
    "Tokenization converts a text corpus into a sequence of words (tokens), enabling the model to work with numerical data. We also introduce an \"out-of-vocabulary\" (OOV) token to handle any words that the model hasn't seen during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e025bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index: {'<OOV>': 1, 'this': 2, 'instrument': 3, 'measures': 4, 'the': 5, 'planes': 6, 'altitude': 7, 'latitude': 8}\n",
      "Sequences: [[2, 3, 4, 5, 6, 7], [2, 3, 4, 5, 6, 8]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define the example corpus\n",
    "example = [\n",
    "    'This instrument measures the planes altitude',\n",
    "    'This instrument measures the planes latitude'\n",
    "]\n",
    "\n",
    "# Updated tokenizer with out-of-vocabulary (OOV) handling\n",
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(example)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(example)\n",
    "\n",
    "print(\"Word Index:\", word_index)\n",
    "print(\"Sequences:\", sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1e20e",
   "metadata": {},
   "source": [
    "### Out of vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d441138b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1, 'instrument': 2, 'measures': 3, 'the': 4, 'planes': 5, 'is': 6, 'altitude': 7, 'latitude': 8, 'nothing': 9, 'wrong': 10, 'everybody': 11, 'onboard': 12, 'doing': 13, 'ok': 14, 'and': 15, 'enjoying': 16, 'dinner': 17}\n",
      "[[1, 2, 3, 4, 5, 7], [1, 2, 3, 4, 5, 8], [9, 6, 10, 11, 12, 6, 13, 14, 15, 16, 17]]\n"
     ]
    }
   ],
   "source": [
    "# new corpus\n",
    "plane_status = [\n",
    "    'This instrument measures the planes altitude',\n",
    "    'This instrument measures the planes latitude',\n",
    "    'Nothing is wrong everybody onboard is doing ok and enjoying dinner'\n",
    "]\n",
    "# tokenizer\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(plane_status)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(plane_status)\n",
    "print(word_index)\n",
    "print(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c63e7577",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "  'Nothing works! We were wrong about the altitude! ok what do we do now?',\n",
    "  'What does this button on planes do? Are alarm sounds ok?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b67c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1, 'instrument': 2, 'measures': 3, 'the': 4, 'planes': 5, 'is': 6, 'altitude': 7, 'latitude': 8, 'nothing': 9, 'wrong': 10, 'everybody': 11, 'onboard': 12, 'doing': 13, 'ok': 14, 'and': 15, 'enjoying': 16, 'dinner': 17}\n",
      "[[9, 10, 4, 7, 14], [1, 5, 14]]\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "print(word_index)\n",
    "print(test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d79c02",
   "metadata": {},
   "source": [
    "translate back to:\n",
    "\n",
    "\"Nothing wrong, the altitude ok\"\n",
    "\n",
    "and \n",
    "\n",
    "\"this planes ok\"\n",
    "\n",
    "Our AI is not very I right now... it is a good thing AI doesn't fly planes, am I right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9132843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index: {'<OOV>': 1, 'this': 2, 'instrument': 3, 'measures': 4, 'the': 5, 'planes': 6, 'altitude': 7, 'latitude': 8}\n",
      "Sequences: [[2, 3, 4, 5, 6, 7], [2, 3, 4, 5, 6, 8]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer with oov_token\n",
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(example)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(example)\n",
    "print(\"Word Index:\", word_index)\n",
    "print(\"Sequences:\", sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819de153",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "### Let's load in some new text for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c4e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88179324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#review titles from amazon for my black shirt(s)\n",
    "review1= \"Wash by hand and line dry ONLY\"\n",
    "review2= \"You can't go wrong with these premium polo shirts. Really nice texture and cool in the sun.\"\n",
    "review3= \"There are a few out there that don't snag easily but this is not one of those\"\n",
    "review4= \"Great Shirt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b061ea53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'there': 2, 'wash': 3, 'by': 4, 'hand': 5, 'line': 6, 'dry': 7, 'only': 8, 'you': 9, \"can't\": 10, 'go': 11, 'wrong': 12, 'with': 13, 'these': 14, 'premium': 15, 'polo': 16, 'shirts': 17, 'really': 18, 'nice': 19, 'texture': 20, 'cool': 21, 'in': 22, 'the': 23, 'sun': 24, 'are': 25, 'a': 26, 'few': 27, 'out': 28, 'that': 29, \"don't\": 30, 'snag': 31, 'easily': 32, 'but': 33, 'this': 34, 'is': 35, 'not': 36, 'one': 37, 'of': 38, 'those': 39, 'great': 40, 'shirt': 41}\n",
      "[[3, 4, 5, 1, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 21, 22, 23, 24], [2, 25, 26, 27, 28, 2, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41]]\n"
     ]
    }
   ],
   "source": [
    "# new corpus\n",
    "shirt_header = [review1, review2, review3, review4]\n",
    "    \n",
    "# tokenizer\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(shirt_header)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(shirt_header)\n",
    "print(word_index)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad2b3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  3  4  5  1  6  7  8]\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20  1 21 22 23 24]\n",
      " [ 2 25 26 27 28  2 29 30 31 32 33 34 35 36 37 38 39]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 40 41]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3bb71",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed4156",
   "metadata": {},
   "source": [
    "### Remove HTML tags\n",
    "\n",
    "In the code below we clean up HTML content by removing the HTML tags and leaving only the plain text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de67592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_scraping_job = \"<span>Wash by hand and line dry ONLY.</span>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0adbbe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span>Wash by hand and line dry ONLY.</span>\n",
      "Wash by hand and line dry ONLY.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup =BeautifulSoup(bad_scraping_job)\n",
    "clean_text = soup.get_text()\n",
    "print(bad_scraping_job)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e963296",
   "metadata": {},
   "source": [
    "### Remove stop words using a list\n",
    "\n",
    "Stopwords are common words (such as \"the\", \"is\", \"in\") that are often removed in natural language processing tasks to focus on more meaningful words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b02d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopword_list=['um', 'ah', 'er', 'eh', 'hmm', 'a', 'the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66439d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how about turkey sandwich with cheese \n"
     ]
    }
   ],
   "source": [
    "example = 'um how about a er hmm turkey sandwich with ah cheese'\n",
    "\n",
    "words = example.split()\n",
    "filtered_example= ''\n",
    "for word in words:\n",
    "    if word not in my_stopword_list:\n",
    "        filtered_example = filtered_example + word + \" \"\n",
    "print(filtered_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8afad33-e55b-4d2b-b102-33c249372621",
   "metadata": {},
   "source": [
    "### Stopword Removal using NLTK\n",
    "\n",
    "Stopwords are common words (such as \"the\", \"is\", \"in\") that are often removed in natural language processing tasks to focus on more meaningful words. In the above example we provided a stop word list, but they already exist and we can just use those if we want to. Below we use the NLTK library to remove these stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e924c295-4748-46c5-ae42-aa9a96584db9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# note this package needs to be installed\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Download stopwords\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# note this package needs to be installed\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Use stopwords from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "example = 'um how about a er hmm turkey sandwich with ah cheese'\n",
    "words = example.split()\n",
    "\n",
    "filtered_example = ' '.join([word for word in words if word.lower() not in stop_words])\n",
    "print(filtered_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883c684-cc1c-4d7b-a9ed-3ab52a57ce59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
