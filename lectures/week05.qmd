---
title: Week 5
subtitle: QMB3302 Foundations of Analytics and AI
format:
  metropolis-beamer-revealjs:
    slide-number: c
    embed-resources: true
    # Syntax highlighting theme (pick one: pygments, tango, zenburn, kate, breeze, nord, github, dracula, monokai, etc.)
    highlight-style: ../materials/assets/highlight_accessible.theme
    # Nice-to-have code UX for slides
    code-line-numbers: true      # add line numbers to all code blocks
    code-overflow: wrap          # wrap long lines (good for projectors)
    code-copy: true              # copy-to-clipboard button
    code-block-bg: true          # subtle background behind code blocks
    code-block-border-left: "#E69F00"  # UF-amber accent; pick your brand color
author:
  - name: Joel Davis
    orcid: 0000-0000-0000-0000
    email: joel.davis@warrington.ufl.edu
    affiliations: University of Florida
date: last-modified
bibliography: ../materials/assets/shared_references_courses.bib
---

# I. Defining the Field: AI, Machine Learning, and Deep Learning  


## Artificial Intelligence is the broad field, while Machine Learning and Deep Learning are subsets

- **Artificial Intelligence (AI):** Systems performing tasks requiring human-like intelligence (reasoning, planning, perception, problem-solving).  
- **Machine Learning (ML):** Subfield of AI where systems improve through data-driven learning instead of explicit programming.  
- **Deep Learning (DL):** Subset of ML using deep neural networks to learn hierarchical representations.  



::: notes

### Slide:: Artificial Intelligence is the broad field, while Machine Learning and Deep Learning are subsets

### Detailed Notes  
- Frame this slide as a **taxonomy clarification**, not a technical deep dive. The primary goal is to correct imprecise language students already use.  
- Emphasize that confusion here leads to downstream misunderstandings about **capabilities, limitations, and expectations** of systems labeled “AI.”  
- Stress the **set inclusion logic**: Deep Learning ⊂ Machine Learning ⊂ Artificial Intelligence. This is about *scope*, not superiority.  
- Highlight that AI is a **problem domain** (intelligent behavior), whereas ML and DL are **methodological approaches** within that domain.  
- Use contrastive examples to anchor intuition:  
  - A rule-based scheduling system can be AI without ML.  
  - A linear regression model is ML but not deep learning.  
  - A large neural network trained on images or text is DL (and therefore also ML and AI).  
- Pedagogically, slow down here. Students often nod along while still collapsing all three terms mentally into “AI = neural networks.”  
- Set expectations for the course arc: early material spans AI broadly, the middle focuses on ML foundations, and later sections emphasize DL where scale and representation dominate.

### Deeper Dive  
Artificial Intelligence historically predates Machine Learning and Deep Learning by decades. AI originally referred to **any computational system capable of intelligent behavior**, including symbolic reasoning systems, search algorithms, planning systems, and expert systems built from explicit human knowledge. These systems operate through *logic and rules*, not learning.

Machine Learning emerged as a response to the brittleness of symbolic AI. Rather than encoding intelligence explicitly, ML systems infer patterns from data. Formally, ML focuses on algorithms that improve performance on a task \(T\) with experience \(E\), as measured by a performance metric \(P\). This reframing shifts intelligence from **knowledge engineering** to **statistical inference**.

Deep Learning is a specific family of machine learning methods characterized by:
- Multi-layer neural network architectures  
- End-to-end representation learning  
- Optimization via gradient-based methods  

What distinguishes DL is not simply neural networks, but **depth plus scale**. Deep architectures learn *hierarchical representations*, where early layers encode low-level patterns and later layers encode increasingly abstract concepts. This representation learning replaces much of the manual feature engineering required in classical ML.

Importantly, these categories differ along multiple dimensions:
- **Assumptions:** Symbolic AI assumes rules can be articulated; ML assumes patterns can be learned; DL assumes representations emerge with sufficient data and capacity.  
- **Data dependence:** DL is far more data- and compute-intensive than earlier ML methods.  
- **Interpretability:** Traditional AI and some ML models are interpretable by design, while DL models are often opaque.  

Modern AI practice is dominated by deep learning not because it is philosophically superior, but because **data availability, GPUs, and optimization techniques** made it viable at scale. However, many real-world AI systems still combine symbolic logic, classical ML, and DL components. Understanding these distinctions is essential for making informed design, evaluation, and governance decisions later in the course.
:::



## The relationship between AI, ML, and DL can be shown as a set of nested categories {.layout-two-content}

:::: {.columns}
::: {.column width="50%"}


- AI includes both symbolic systems (like expert systems) and data-driven systems (like ML).  
- ML focuses on algorithms that adapt from data.  
- DL dominates recent breakthroughs due to scalability with large data and GPUs.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/1_qmb_nested_ai.svg"
       alt="**Figure:** AI ⊃ ML ⊃ DL as a nested Venn diagram">
  <figcaption>**Figure:** AI ⊃ ML ⊃ DL as a nested Venn diagram</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: The relationship between AI, ML, and DL can be shown as a set of nested categories

### Detailed Notes  
- Use this slide to **visually reinforce** the conceptual taxonomy introduced previously. The goal is alignment between language and mental models.  
- Walk students through the diagram deliberately from **outside to inside**, emphasizing that each inner category inherits the properties of the outer one.  
- Stress that the nesting reflects **methodological containment**, not chronological replacement. AI did not “become” ML, and ML did not “become” DL.  
- Point out that symbolic systems (expert systems, rule engines) still live comfortably inside AI even though they sit outside ML.  
- Emphasize why DL dominates recent breakthroughs: not because it is universally better, but because it **scales effectively** with data and compute.  
- Pedagogically, connect the figure to expectations: when students read headlines about “AI,” they are usually seeing DL applications, but those are only one region of the broader AI landscape.  
- Use this slide as a checkpoint: if students cannot correctly place a system inside this diagram, they likely do not yet understand what kind of “AI” they are discussing.

### Deeper Dive  
The nested representation AI ⊃ ML ⊃ DL captures both **historical evolution** and **methodological specialization**. Artificial Intelligence encompasses any computational approach aimed at producing intelligent behavior, regardless of whether learning is involved. This includes search algorithms, planning systems, constraint solvers, and symbolic reasoning engines.

Machine Learning narrows the scope by requiring **adaptation through data**. An ML system is defined less by its architecture and more by its learning process: parameters are adjusted to minimize error or maximize reward based on observed examples. Classical ML methods—such as linear regression, decision trees, and support vector machines—fall entirely within this region.

Deep Learning is a further specialization characterized by **high-capacity function approximators** with multiple layers. The depth of these models enables hierarchical composition of features, which becomes especially powerful when paired with:
- Large labeled or self-supervised datasets  
- Specialized hardware (GPUs, TPUs)  
- Advances in optimization and regularization  

The dominance of DL in recent decades reflects a shift from **algorithmic cleverness to representational capacity**. Rather than designing task-specific features or rules, practitioners increasingly rely on models whose internal representations emerge from scale. This has proven especially effective in perceptual domains (vision, speech) and language, where hand-engineering features is difficult.

However, the nesting diagram also highlights an important limitation: **not all AI problems require or benefit from deep learning**. Many business and operational problems are well-served by simpler ML models or even rule-based logic. Understanding where a problem sits within this hierarchy helps avoid unnecessary complexity and supports better system design decisions.
:::



## Different eras, AI (rules-based): 1980s expert systems with hand-coded rules. 

<figure>
  <img src="../materials/assets/images/checkers.png"
       alt="**Figure:** [@CheckersComputerBecomes2007]">
  <figcaption>**Figure:** [@CheckersComputerBecomes2007]</figcaption>
</figure>

::: {.notes}
### Slide:: Different eras, AI (rules-based): 1980s expert systems with hand-coded rules

### Detailed Notes  
- Introduce this slide as a **historical anchor**, not a critique. Emphasize that early AI was ambitious and intellectually rigorous, even if limited by today’s standards.  
- Use the checkers example to illustrate how intelligence was achieved through **explicit human knowledge**, not learning. The “intelligence” lives in the rules, not the data.  
- Walk students through the core idea: programmers anticipated situations and encoded *if–then* logic for every relevant case.  
- Emphasize why games like checkers were attractive early AI testbeds:  
  - Fully observable environment.  
  - Finite rules and states.  
  - Clear objective (win/lose).  
- Stress the central limitation: these systems **do not improve with experience** unless a human rewrites the rules.  
- Pedagogically, contrast this era with modern intuitions about AI. Many students assume learning is intrinsic to AI; this slide shows that learning is historically optional, not essential.  
- Set up the transition: rule-based success in constrained domains ultimately exposed scalability limits when rules became too numerous or environments too noisy.

### Deeper Dive  
Rule-based AI systems dominated from the 1960s through the 1980s and were grounded in **symbolic reasoning**. Intelligence was modeled as logical manipulation of symbols governed by explicit rules. Expert systems, in particular, attempted to encode human expertise into large collections of condition–action statements.

In games like checkers, early AI systems relied on:
- Enumerated legal moves defined by the game rules.  
- Evaluation functions crafted by experts to score board positions.  
- Search algorithms (e.g., minimax with pruning) to explore possible futures.  

Crucially, the system’s competence was bounded by the **quality and completeness of human knowledge** embedded in it. Performance improvements came from refining heuristics or adding rules, not from data-driven learning.

This paradigm revealed two fundamental limitations:
1. **Knowledge acquisition bottleneck:** Extracting expert reasoning and translating it into rules was slow, expensive, and error-prone.  
2. **Combinatorial explosion:** As problem complexity increased, the number of rules and states grew exponentially, making systems brittle and hard to maintain.

The checkers example illustrates both the power and fragility of rule-based AI. In constrained, well-defined environments, explicit logic can achieve strong performance. But in real-world settings with ambiguity, noise, and incomplete information, rule-based systems struggle to scale. These limitations directly motivated the shift toward data-driven machine learning approaches in later decades.
:::


## Different eras. Machine Learning: Spam filters, recommendation systems. 


<figure>
  <img src="../materials/assets/images/spam_filter2.svg"
       alt="**Figure:** Example of spam">
  <figcaption>**Figure:** Example of spam</figcaption>
</figure>


::: {.notes}
### Slide:: Different eras. Machine Learning: Spam filters, recommendation systems

### Detailed Notes  
- Present this slide as the **conceptual pivot** from hand-coded intelligence to data-driven learning. Emphasize what fundamentally changed: *the source of intelligence*.  
- Use spam filtering as the canonical example because it is intuitive and familiar: humans struggle to articulate perfect rules for spam, but they can easily label examples.  
- Contrast explicitly with the prior slide: instead of writing rules like “IF subject contains X THEN spam,” we let the system **infer patterns statistically** from labeled data.  
- Emphasize that learning here is still **task-specific**: models are trained for a particular objective (spam vs. not spam, recommendation relevance), not general intelligence.  
- Highlight the role of feedback loops: more data → better model → better predictions → more data.  
- Pedagogically, stress that this era made AI practical in messy, real-world domains where rules break down.  
- Set up the next transition: although ML worked well, it still depended heavily on **manual feature engineering**, which became a new bottleneck.

### Deeper Dive  
The emergence of machine learning in the 1990s and 2000s marked a shift from symbolic reasoning to **statistical pattern recognition**. Rather than encoding explicit rules, practitioners defined input representations, collected labeled data, and trained models to minimize prediction error.

Spam filters exemplify this paradigm. Early ML-based spam systems typically relied on:
- Feature representations such as word counts, n-grams, or metadata.  
- Supervised learning algorithms (e.g., Naive Bayes, logistic regression, support vector machines).  
- Continuous retraining as spam tactics evolved.

Recommendation systems followed a similar logic but often used **implicit feedback** (clicks, views, purchases) rather than explicit labels. Techniques such as collaborative filtering leveraged patterns across users and items, enabling personalization at scale.

Despite their success, classical ML systems introduced a new dependency: **feature engineering**. Model performance was constrained by how well humans transformed raw data into meaningful predictors. For example, text models required careful preprocessing and feature selection, and recommendation systems depended on domain-specific similarity metrics.

This era demonstrated that learning from data could outperform hand-coded logic in complex environments, but it also revealed the limits of shallow models and manually designed representations. These constraints motivated the rise of deep learning, which aimed to automate feature learning itself.
:::



::: {.notes}
### Slide:: Different eras. Deep Learning: Image recognition with CNNs, language models like GPT

### Detailed Notes  
- Frame this slide as the **modern inflection point** where representation learning overtakes manual feature engineering. Emphasize what changed relative to classical ML.  
- Use the image recognition example to make probability-based outputs concrete: the model is not “seeing,” it is estimating likelihoods over learned categories.  
- Highlight CNNs as a structural innovation: spatial hierarchies and shared weights enable learning directly from raw pixels without handcrafted features.  
- Introduce language models (e.g., GPT) as a parallel breakthrough in text, where representations emerge from massive corpora via self-supervised learning.  
- Stress that deep learning systems are **data-hungry and compute-intensive**, but this cost buys generalization and transfer across tasks.  
- Pedagogically, caution against anthropomorphism. These models do not “understand” images or language in a human sense; they learn statistical structure at scale.  
- Bridge forward: this era sets up later discussions about scale, overfitting, instability, and the need for regularization and careful training.

### Deeper Dive  
Deep learning’s resurgence around 2010 was driven by the convergence of three forces: large datasets, powerful hardware (GPUs), and improved optimization techniques. Convolutional Neural Networks (CNNs) demonstrated that hierarchical architectures could learn progressively abstract features—from edges to textures to object parts—directly from raw pixel data.

In contrast to classical ML pipelines, deep learning systems perform **end-to-end learning**. Rather than separating feature extraction and prediction into distinct stages, the entire system is optimized jointly. This dramatically reduces reliance on human-designed features and allows models to adapt representations to the task.

Large language models follow the same principle in the textual domain. Trained using self-supervised objectives (e.g., predicting the next token), these models learn dense vector representations that encode syntax, semantics, and contextual relationships. With sufficient scale, they exhibit emergent capabilities such as zero-shot and few-shot learning.

However, deep learning introduces new challenges:
- **Opacity:** Learned representations are difficult to interpret.  
- **Instability:** Training can be sensitive to initialization, learning rates, and data quality.  
- **Resource intensity:** Performance gains depend heavily on compute and data availability.  

The key conceptual shift is that intelligence is no longer explicitly engineered or narrowly inferred; it **emerges from representation learning at scale**. This shift underpins modern AI systems but also raises questions about robustness, fairness, and controllability that become increasingly important as models grow larger.
:::


## Rules-based systems rely on explicit logic, while data-driven systems learn from examples

- **Rules-based systems:**  
  - Hand-crafted if–then logic.  
  - Good for narrow, stable tasks.  
  - Poor at handling complexity or adapting.  
- **Data-driven systems:**  
  - Learn directly from data.  
  - More flexible and generalizable.  
  - Require large, high-quality datasets.  

::: {.notes}
### Slide:: Rules-based systems rely on explicit logic, while data-driven systems learn from examples

### Detailed Notes  
- Use this slide to **sharpen the contrast** between two fundamentally different sources of intelligence: human-authored logic versus learned patterns.  
- Emphasize that this is not a value judgment. Rules-based systems are not “bad,” and data-driven systems are not “magic”; they solve different classes of problems.  
- Walk through the rules-based side first, stressing the burden it places on designers: every exception, edge case, and contingency must be anticipated in advance.  
- Then pivot to data-driven systems, highlighting the key shift: instead of specifying *how* to decide, we provide examples and let the model infer decision boundaries.  
- Use concrete comparisons to anchor intuition:  
  - Tax rules or eligibility checks → rules-based.  
  - Image classification or spam detection → data-driven.  
- Pedagogically, underline the tradeoff: data-driven systems gain flexibility and generalization, but at the cost of **data dependence and reduced transparency**.  
- Set up later material by foreshadowing that learning from data introduces new risks: overfitting, bias, and sensitivity to data quality.

### Deeper Dive  
Rules-based systems implement intelligence through **explicit symbolic representations**. Decisions are made by evaluating logical conditions, often expressed as if–then rules or decision trees authored by humans. These systems excel when:
- The domain is stable and well understood.  
- Rules can be clearly articulated and rarely change.  
- Accountability and interpretability are paramount.

However, as environments grow more complex or noisy, rule sets grow combinatorially. Maintenance becomes difficult, and unanticipated situations lead to brittle behavior.

Data-driven systems invert this paradigm. Instead of encoding logic directly, they infer relationships from observed examples. Formally, a model learns a function \( f(X) \) that maps inputs to outputs by minimizing some notion of error across a dataset. The decision logic is **implicit**, distributed across learned parameters rather than explicit rules.

This shift enables:
- Adaptation as new data arrives.  
- Robustness to variability and ambiguity.  
- Generalization to previously unseen cases.

At the same time, it introduces dependencies absent in rule-based systems. Model behavior is only as good as the data used to train it. Biases, gaps, or shifts in data distributions directly affect outcomes. Moreover, the learned logic is often opaque, complicating explanation and governance.

Understanding this distinction is foundational. Many modern AI systems blend both approaches: learned models generate predictions, while rule-based logic constrains, audits, or overrides decisions. The effectiveness of such hybrid systems depends on recognizing the strengths and limitations of each paradigm.
:::




## Deep Learning overtook classical ML by automating feature learning at scale

- Traditional ML relied heavily on manual feature engineering.  
- DL learns hierarchical representations directly from raw data.  
- With sufficient data and compute, DL often outperforms classical ML in perception and language tasks.  
- This shift explains major advances in computer vision, speech, and NLP.  


::: {.notes}
### Slide:: Deep Learning overtook classical ML by automating feature learning at scale

### Detailed Notes  
- Present this slide as a **mechanism-level explanation** for deep learning’s success, not as a blanket performance claim.  
- Begin by reminding students how classical ML pipelines worked: raw data → **human-designed features** → relatively simple models. Emphasize that feature engineering was often the dominant source of performance.  
- Clarify the key shift: deep learning **learns representations automatically**, rather than requiring humans to specify which patterns matter in advance.  
- Use concrete contrasts to anchor intuition:  
  - Vision: hand-crafted edge detectors → CNN-learned spatial hierarchies.  
  - NLP: bag-of-words and TF–IDF → learned embeddings and contextual representations.  
- Stress that this advantage appears most clearly in **high-dimensional, unstructured data** (images, audio, text), where manual feature design is difficult or brittle.  
- Pedagogically, emphasize that “automation” here does *not* eliminate human involvement; it moves human effort upstream into data curation, architecture design, and training strategy.  
- Set up later sections by foreshadowing the tradeoffs: feature learning increases power, but also data requirements, opacity, and training instability.

### Deeper Dive  
Classical machine learning separates **representation construction** from **prediction**. Practitioners transform raw inputs into engineered features based on domain knowledge, then train relatively shallow models on those features. Model performance is therefore bounded by how well humans anticipate useful structure in the data.

Deep learning collapses this separation through **end-to-end optimization**. Multi-layer neural networks learn a sequence of representations jointly with the predictive task. Early layers capture low-level regularities (edges, phonemes, tokens), intermediate layers encode compositional structure, and later layers represent task-relevant abstractions. These representations emerge through gradient-based learning rather than explicit design.

This shift has several important implications:
- **Scalability:** Learned representations improve systematically with more data and larger models.  
- **Transferability:** Representations trained on one task can be reused or fine-tuned for others.  
- **Reduced manual bias:** Feature choices are less constrained by human intuition, for better or worse.

However, feature learning is not universally superior. In structured, low-dimensional, or small-sample settings, classical ML with thoughtful feature engineering often remains competitive or preferable. Deep learning’s advantage is domain-dependent, not absolute.

The core insight is that deep learning changed *where intelligence lives* in the pipeline: not in hand-crafted inputs, but in learned internal representations. This reallocation of effort explains its breakthroughs—and also motivates the need for regularization, validation, and careful evaluation at scale.
:::



# II. Categories of Machine Learning  



## Supervised learning uses labeled data to predict outcomes

- **Definition:** The model learns from labeled examples (X → y).  
- **Goal:** Predict an output given an input.  
- **Types:**  
  - **Regression:** Predict a continuous value.  
    - Example: House price prediction from features.  
  - **Classification:** Predict a category.  
    - Example: Email spam detection (spam vs. not spam).  

::: {.notes}
### Slide:: Supervised learning uses labeled data to predict outcomes

### Detailed Notes  
- Introduce supervised learning as the **workhorse paradigm** of machine learning, especially in business, social science, and applied analytics.  
- Emphasize that the defining feature is **labeled data**: for every input, we know the correct output during training.  
- Make the mapping idea explicit: the model’s job is to learn a function that maps inputs \(X\) to outputs \(y\), guided by examples.  
- Walk students slowly through the regression vs. classification split, since confusion here causes repeated downstream errors.  
- Reinforce that regression and classification differ in **output type**, not in sophistication or importance.  
- Use familiar examples to ground intuition:  
  - Continuous outcomes feel like “how much” questions.  
  - Categorical outcomes feel like “which one” questions.  
- Pedagogically, stress that most evaluation metrics, validation strategies, and modeling decisions later in the course depend on whether the task is supervised and which subtype it belongs to.

### Deeper Dive  
Supervised learning formalizes prediction as a problem of **function approximation under guidance**. Given a dataset of paired observations \((X_i, y_i)\), the learning algorithm selects a function \( \hat{f} \) from a hypothesis class to minimize prediction error on observed labels, with the goal of generalizing to unseen data.

Regression and classification share the same structural goal—predicting outputs from inputs—but differ in the **nature of the response variable**. Regression models target continuous outcomes and typically optimize squared or absolute error. Classification models target discrete labels and often optimize probabilistic or margin-based objectives.

A critical implication of supervision is that **labels encode the objective**. The model learns whatever signal the labels represent, whether or not that signal is well-defined, unbiased, or stable. As a result:
- Label quality directly bounds achievable performance.  
- Label noise introduces irreducible error.  
- Misaligned labels produce models that optimize the wrong objective.

Supervised learning is powerful because it translates vague goals (“predict churn,” “detect fraud”) into concrete optimization problems. At the same time, it is constrained by the availability, cost, and reliability of labeled data. These constraints motivate alternative paradigms—unsupervised, reinforcement, and self-supervised learning—which relax or reinterpret the role of labels in different ways.
:::



## Unsupervised learning finds hidden structure without labeled data {.layout-two-content}

:::: {.columns}
::: {.column width="50%"}

- **Definition:** The model discovers structure in unlabeled data.  
- **Goals:** Group similar items, reduce complexity, find patterns.  
- **Examples:**  
  - **Clustering:** Customer segmentation in marketing.  
  - **Dimensionality reduction:** PCA for visualizing high-dimensional data.  
- **Key idea:** Patterns must be inferred — no “correct” answers are given.  
:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/clustering_simple_light.svg"
       alt="**Figure:** Example of clustering with distinct groups. There are 3 seperate groups of colored circles, representing differet groups. Each set of colored circles is seperated from the others by a line.">
  <figcaption>**Figure:** Example of clustering with distinct groups</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Unsupervised learning finds hidden structure without labeled data

### Detailed Notes  
- Introduce unsupervised learning as a **different objective**, not a weaker version of supervised learning. The goal is understanding structure, not prediction accuracy.  
- Emphasize the absence of labels as the defining feature: the algorithm receives only inputs \(X\), with no guidance about what the “right” answer should be.  
- Stress that because there are no labels, **evaluation is inherently subjective and contextual**. What counts as a “good” result depends on the problem and the user’s goals.  
- Walk through clustering as the most intuitive example: grouping similar items without knowing categories in advance.  
- Use dimensionality reduction to highlight a different goal: simplifying complex data while preserving important structure.  
- Pedagogically, warn students not to ask “Is this clustering correct?” in the same way they would for supervised tasks. The better question is “Is this clustering useful?”  
- Connect forward: unsupervised learning often serves as an **exploratory or preprocessing step** rather than a final decision system.

### Deeper Dive  
Unsupervised learning seeks to uncover **latent structure** in data distributions. Without labels, the learning problem shifts from minimizing prediction error to identifying regularities such as clusters, manifolds, or low-dimensional representations that summarize the data effectively.

Clustering algorithms, such as k-means or hierarchical clustering, impose inductive biases about what structure should look like—typically compact, well-separated groups. The resulting clusters are not discovered facts about the world, but **model-dependent interpretations** of similarity under a chosen distance metric and objective function.

Dimensionality reduction techniques, such as Principal Component Analysis (PCA), pursue a different notion of structure. PCA identifies directions of maximum variance and projects data into a lower-dimensional space that preserves as much variability as possible. This is especially valuable for visualization, denoising, and compression in high-dimensional settings.

A key implication of unsupervised learning is the absence of a natural error metric. Without labels:
- Multiple solutions may be equally valid.  
- Results are sensitive to modeling assumptions and preprocessing choices.  
- Validation often relies on domain knowledge, stability analysis, or downstream utility.

Unsupervised learning is therefore best understood as a **tool for discovery and representation**, not as a direct decision-making mechanism. Its outputs often inform later supervised models, guide feature engineering, or support human interpretation rather than replacing it.
:::


## Reinforcement learning teaches agents to act through trial and error {.layout-two-content}

:::: {.columns}
::: {.column width="50%"}

- **Definition:** An agent learns by interacting with an environment and receiving rewards or penalties.  
- **Core elements:**  
  - **Agent:** decision-maker.  
  - **Environment:** system it interacts with.  
  - **Actions:** choices taken by the agent.  
  - **Rewards:** feedback signal, immediate or delayed.    

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/agent_diagram.png"
       alt="**Figure:** Reinforcement learning loop of agent, environment, actions, and rewards">
  <figcaption>**Figure:** Reinforcement learning loop of agent, environment, actions, and rewards</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Reinforcement learning teaches agents to act through trial and error

### Detailed Notes  
- Introduce reinforcement learning (RL) as a **fundamentally different learning setup**, not just another variation on supervised or unsupervised learning.  
- Emphasize that the key shift is from learning mappings to learning **sequences of decisions over time**.  
- Walk through the agent–environment loop explicitly: the agent acts, the environment responds, and the agent updates behavior based on reward signals.  
- Stress that rewards may be **delayed**, which makes credit assignment difficult and distinguishes RL from supervised learning.  
- Introduce the term *policy* informally as the agent’s decision strategy, without formal notation at this stage.  
- Use high-profile examples (games, robotics) to convey intuition, but avoid overgeneralization.  
- Clarify that many real-world applications (e.g., recommendations or ads) use **bandit or RL-inspired approaches**, which simplify the full RL framework.  
- Pedagogically, position RL as powerful but complex, and therefore used selectively where sequential decisions truly matter.

### Deeper Dive  
Reinforcement learning formalizes decision-making as an optimization problem over **long-term cumulative reward**. At each time step, an agent observes a state of the environment, selects an action according to a policy, and receives a reward signal. The objective is not to maximize immediate reward, but the expected sum of future rewards.

This setup introduces several challenges absent in supervised learning:
- **Exploration vs. exploitation:** The agent must balance trying new actions to learn their value against exploiting known high-reward actions.  
- **Delayed feedback:** Actions may influence rewards far in the future, complicating learning.  
- **Non-stationarity:** The data distribution depends on the agent’s own behavior, not a fixed dataset.

Classic successes such as game-playing agents and robotic control rely on full reinforcement learning, often modeled as Markov Decision Processes (MDPs). These settings require learning value functions or policies over large state spaces.

In contrast, many industrial systems use simplified RL formulations, such as **multi-armed bandits** or **contextual bandits**, where:
- The state is minimal or ignored.  
- Rewards are immediate.  
- Planning horizons are short.

These approaches retain the reward-driven adaptation of RL while avoiding its full complexity.

Reinforcement learning is best viewed as the paradigm for **sequential decision-making under uncertainty**. Its power comes from optimizing behavior over time, but that power brings computational cost, instability, and sensitivity to design choices—factors that limit where RL is deployed in practice.
:::


## Emerging paradigms like self-supervised and few-shot learning are reshaping the field

- **Self-supervised learning:**  
  - Uses raw data with automatically constructed training signals.  
  - Example: Predicting missing or next words in a sentence.  
  - Foundation of modern representation learning in models like BERT and GPT.  
- **Few-shot learning:**  
  - Models adapt to new tasks using only a small number of labeled examples.  
  - Enabled by large pretrained models and in-context learning.  


::: {.notes}
### Slide:: Emerging paradigms like self-supervised and few-shot learning are reshaping the field

### Detailed Notes  
- Present this slide as a **forward-looking pivot** that explains why modern AI systems feel qualitatively different from earlier ML models.  
- Emphasize that both paradigms relax traditional constraints: self-supervised learning reduces dependence on labeled data, and few-shot learning reduces task-specific retraining.  
- For self-supervised learning, stress the key idea: the model learns by solving **artificial tasks derived from raw data itself**, not from human-provided labels.  
- Use language prediction as the anchor example because it is intuitive and central to modern models.  
- When introducing few-shot learning, be explicit that this is **not a new training algorithm**, but a capability that emerges from large pretrained models.  
- Pedagogically, caution students against assuming these paradigms eliminate the need for data or supervision; they **restructure where supervision enters the pipeline**.  
- Set expectations: these ideas underpin much of what students encounter today under the label “foundation models” or “generative AI.”

### Deeper Dive  
Self-supervised learning reframes representation learning by replacing external labels with **intrinsic prediction tasks**. In language models, this includes objectives such as masked token prediction or next-token prediction. In vision, it may involve predicting missing patches, colorization, or relative spatial positions.

Crucially, these objectives are not ends in themselves. They function as **surrogate tasks** that force models to learn internal representations capturing semantic, syntactic, or structural regularities in data. Once learned, these representations transfer effectively to downstream tasks with minimal additional supervision.

Few-shot learning emerges when such pretrained representations are sufficiently rich that models can adapt to new tasks using only a handful of examples. In modern large language models, this often occurs through **in-context learning**, where examples are provided at inference time rather than through parameter updates.

Several clarifications are important:
- Few-shot learning is enabled by **scale and pretraining**, not by abandoning supervision entirely.  
- Performance depends strongly on how tasks are framed and examples are presented.  
- Few-shot capabilities are uneven: models may excel in some domains and fail in others.

Together, self-supervised and few-shot learning signal a shift from task-specific models to **general-purpose representations**. This shift reduces marginal labeling costs and accelerates deployment, but it also introduces new challenges related to controllability, evaluation, and reliability—topics that recur throughout modern AI practice.
:::


## Different categories of machine learning are best suited to different problems

- **Supervised regression:** Predict sales, prices, or demand.  
- **Unsupervised clustering:** Group shoppers by behavior or similarity.  
- **Reinforcement learning / bandits:** Optimize decisions over time using feedback (e.g., ad selection).  
- Ask: *“Which learning paradigm fits this problem?”*  

::: {.notes}
### Slide:: Different categories of machine learning are best suited to different problems

### Detailed Notes  
- Frame this slide as a **decision-making lens**, not a summary slide. The goal is to shift students from memorizing definitions to selecting approaches strategically.  
- Emphasize that there is no universally “best” ML paradigm; suitability depends on the structure of the problem and the data available.  
- Walk through each example briefly, reinforcing *why* the paradigm fits:  
  - Regression for numeric outcomes where accuracy is measured by error.  
  - Clustering for exploratory grouping when labels do not exist.  
  - Reinforcement learning or bandits for repeated decisions with feedback over time.  
- Stress that misalignment is common: people often try to force supervised learning onto problems that lack reliable labels, or apply RL when simpler methods suffice.  
- Encourage students to ask the highlighted question explicitly before modeling: *What signal do I have, and what decision am I trying to support?*  
- Pedagogically, position this as a habit of mind they should carry into projects, exams, and real-world analytics work.

### Deeper Dive  
Choosing a machine learning paradigm is fundamentally about **matching the learning signal to the problem structure**. Supervised learning requires labeled outcomes and is appropriate when the objective is prediction under known targets. Its strength lies in measurable accuracy and clear evaluation metrics.

Unsupervised learning, by contrast, is exploratory. It is most useful when the goal is understanding structure, heterogeneity, or latent groupings rather than predicting a specific outcome. Because there is no ground truth, success is judged by interpretability, stability, or downstream usefulness.

Reinforcement learning and bandit methods address a different class of problems: **sequential decision-making with feedback**. These paradigms are appropriate when actions influence future data and rewards, and when learning must occur online rather than from a fixed dataset. In practice, simplified forms such as contextual bandits are far more common than full RL.

A recurring failure mode in applied ML is **paradigm mismatch**—using a powerful method simply because it is fashionable, rather than because it fits the problem. Effective practitioners begin by diagnosing:
- Whether labels exist and are reliable.  
- Whether decisions are one-shot or sequential.  
- Whether the goal is prediction, discovery, or optimization.

This diagnostic step often matters more than the choice of algorithm within a paradigm. Matching the problem to the right learning framework is the foundation of sound machine learning practice.
:::



# III. Core Concepts in the Machine Learning Pipeline  


## A learning task begins by defining inputs and outputs


- **Problem framing:** Identify what the input (X) and output (y) are.  
- **Examples:**  
  - Customer attributes → Churn (yes/no).  
  - House features → Price.  
- Not every problem is suitable for ML. Best used for *predicting patterns from data*.  

::: {.notes}
### Slide:: A learning task begins by defining inputs and outputs

### Detailed Notes  
- Frame this slide as the **most important conceptual checkpoint** in the entire pipeline. Emphasize that most downstream failures trace back to poor problem framing.  
- Stress that machine learning always begins with a clear definition of **what information is available (X)** and **what outcome is to be predicted (y)**.  
- Walk through the examples slowly to reinforce the mapping idea: the same raw data can support different tasks depending on how outputs are defined.  
- Emphasize that defining inputs and outputs is not a technical step; it is a **conceptual and managerial decision**.  
- Explicitly caution students that not all interesting questions are ML questions. If there is no stable pattern linking X to y, ML will fail regardless of algorithm choice.  
- Pedagogically, encourage students to pause here in projects and exams and ask: *What exactly am I predicting, and why should it be predictable from the data I have?*  
- Set up the rest of the pipeline: once X and y are defined, everything else—data collection, modeling, evaluation—follows logically.

### Deeper Dive  
Problem framing formalizes learning as a function approximation task: given inputs \(X\), estimate a function \(f\) such that \(y \approx f(X)\). This formulation implicitly assumes that:
- Relevant information about \(y\) is encoded in \(X\).  
- The relationship between \(X\) and \(y\) is sufficiently stable to be learned from historical data.  

Many real-world failures arise when these assumptions do not hold. Outcomes may be driven by unobserved variables, strategic behavior, or random shocks that are not captured in the input space. In such cases, increasing model complexity does not solve the problem.

Problem framing also determines whether learning is supervised, unsupervised, or reinforcement-based. Choosing \(y\) commits the analyst to a particular learning paradigm and evaluation strategy. For example, defining churn as a binary outcome enables classification but obscures timing; defining time-to-churn enables survival modeling instead.

Finally, framing is inseparable from intent. Predictive tasks answer “what is likely to happen,” not “what should be done” or “why it happens.” Confusing these questions leads to misuse of ML outputs. A well-framed learning task respects the boundary between prediction, explanation, and decision-making—setting the foundation for a robust and responsible pipeline.
:::


## Machine learning datasets consist of features and labels

- **Features (X):** measurable predictors, such as age, income, location.  
- **Labels (y):** the target outcome, such as churn, price, or diagnosis.  
- Typically organized in tabular form: **rows = instances, columns = variables**.  

::: {.notes}
### Slide:: Machine learning datasets consist of features and labels

### Detailed Notes  
- Use this slide to **normalize the mental model of data** that students will carry throughout the course. Everything that follows—models, metrics, diagnostics—operates on this structure.  
- Emphasize that features are not just columns in a table; they are **measurements chosen to represent aspects of the world** believed to be predictive.  
- Stress that labels encode the learning objective. Whatever signal is embedded in \(y\) is exactly what the model will optimize for—no more, no less.  
- Walk through the tabular structure carefully, since students often confuse rows and columns conceptually.  
- Use a concrete example (e.g., customers): each row is one customer, each column is one attribute measured consistently across customers.  
- Pedagogically, caution that not all data naturally fits this structure; forcing complex data into tables is itself a modeling decision.  
- Set expectations: later topics (encoding, scaling, feature engineering) are about **making this table suitable for learning**, not changing its fundamental structure.

### Deeper Dive  
At an abstract level, a supervised learning dataset is a collection of input–output pairs \((X_i, y_i)\), where each \(X_i\) is a vector of feature values and \(y_i\) is the associated label. In tabular data, this corresponds to a design matrix with observations as rows and variables as columns.

Several subtle but important distinctions matter:
- **Features vs. raw data:** Features are processed representations, not necessarily raw measurements. Choices about aggregation, encoding, and transformation shape what the model can learn.  
- **Labels as proxies:** Labels often approximate the true concept of interest (e.g., churn, risk, quality). Any mismatch between the label and the underlying goal constrains model usefulness.  
- **Implicit assumptions:** Tabular organization assumes independence across rows and consistent meaning across columns—assumptions that may be violated in time-series or relational data.

Understanding dataset structure also clarifies failure modes. Leakage, bias, and overfitting often arise not from algorithms, but from how features and labels are constructed and aligned. A model trained on poorly defined features or noisy labels will faithfully learn the wrong relationships.

This slide reinforces a core principle: machine learning does not operate on “data” in the abstract, but on **structured representations** designed to make learning possible. Mastery begins with understanding—and questioning—that structure.
:::



## Splitting data into train, validation, and test sets prevents biased evaluation

:::: {.columns}
::: {.column width="50%"}

- **Purpose:** Estimate how well a model generalizes to unseen data.  
- **Typical split:** 70–80% training, 20–30% testing (with validation via holdout or cross-validation).  
- **Validation set:** used to tune hyperparameters and modeling choices.  
- **Analogy:**  
  - Training = studying for a test.  
  - Validation = practice questions.  
  - Test = the real exam.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/holdoutdata_light.svg"
       alt="**Figure:** Illustration of data split into training, validation, and testing sets">
  <figcaption>**Figure:** Illustration of data split into training, validation, and testing sets</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Splitting data into train, validation, and test sets prevents biased evaluation

### Detailed Notes  
- Frame this slide as a **self-protection mechanism**: the purpose of data splitting is to stop us from fooling ourselves about model performance.  
- Emphasize that models almost always look better on the data they were trained on; without a holdout, apparent success is meaningless.  
- Walk through the roles of each split carefully:  
  - Training data teaches the model.  
  - Validation data guides choices (hyperparameters, features, complexity).  
  - Test data is untouched until the very end and used exactly once.  
- Reinforce the analogy explicitly, since it resonates strongly with students’ lived experience.  
- Pedagogically, warn against common mistakes: peeking at test results early, repeatedly tuning to the test set, or treating validation accuracy as final performance.  
- Set expectations for later material: cross-validation generalizes this idea and becomes essential when data is limited.

### Deeper Dive  
Data splitting operationalizes the concept of **generalization**—performance on unseen data drawn from the same distribution as future observations. Without a proper split, models are evaluated in-sample, leading to systematically optimistic estimates of accuracy.

The validation set plays a subtle but critical role. Any decision informed by data—feature selection, model choice, hyperparameter tuning—creates dependency between the model and that data. Once such dependency exists, the data can no longer be used for unbiased evaluation.

The test set is therefore conceptually sacred. It represents a proxy for future deployment conditions and must remain isolated until all modeling decisions are finalized. Violating this separation—even unintentionally—introduces evaluation bias that cannot be corrected after the fact.

Cross-validation extends this logic by rotating validation roles across subsets of the data, improving stability of performance estimates while preserving the principle of separation. Regardless of technique, the core idea remains the same: **evaluation must be insulated from training decisions**.

This practice is not merely academic. In real-world systems, failure to respect data splits leads to overconfident models that degrade rapidly in production. Proper splitting is one of the simplest yet most powerful safeguards in the machine learning pipeline.
:::


## Overfitting occurs when a model fits noise instead of generalizing

- **Overfitting:** Model performs extremely well on training data but poorly on new data.  
- **Generalization:** Model captures patterns that apply beyond training.  
- **Examples:**  
  - Overfit: Decision tree that perfectly classifies training data but fails on test data.  
  - Generalized: Linear regression that captures broader trends.  


::: {.notes}
### Slide:: Overfitting occurs when a model fits noise instead of generalizing

### Detailed Notes  
- Frame overfitting as the **central failure mode** of machine learning, especially as models become more flexible.  
- Emphasize that good training performance alone is meaningless; what matters is performance on unseen data.  
- Clarify that overfitting does not require literal memorization—it often arises from models that are too flexible relative to the amount of data.  
- Walk through the examples deliberately:  
  - A deep decision tree can carve up the training data perfectly by reacting to noise.  
  - A simpler linear model may miss small details but capture the dominant pattern.  
- Pedagogically, stress that overfitting is not a moral failure or coding mistake; it is a **structural consequence of model flexibility**.  
- Reinforce that detecting overfitting requires proper data splits and honest evaluation.  
- Set up later content: regularization, validation, and bias–variance tradeoffs exist largely to manage this problem.

### Deeper Dive  
Overfitting arises when a model’s capacity exceeds what the available data can support. In such cases, the model captures random fluctuations, measurement error, or dataset-specific quirks rather than the underlying data-generating process.

Formally, overfitting reflects a mismatch between **empirical risk** (training error) and **expected risk** (error on new data). Highly flexible models can drive empirical risk toward zero while expected risk remains high.

Generalization, by contrast, occurs when a model identifies stable, repeatable structure—relationships that persist across samples drawn from the same distribution. This often requires sacrificing perfect in-sample fit in exchange for robustness.

Several factors influence overfitting:
- Model complexity (depth, number of parameters).  
- Sample size relative to dimensionality.  
- Noise in features or labels.  
- Data leakage or improper evaluation.

Importantly, overfitting is not solved b
:::



## An end-to-end ML pipeline connects problem definition to deployment and maintenance


:::: {.columns}
::: {.column width="50%"}


1. Problem definition.  
2. Data collection & preprocessing.  
3. Feature engineering and selection.  
4. Model training.  
5. Validation & evaluation.  
6. Deployment into real-world systems.  
7. Monitoring & updating as data evolves.  


:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/datascience_cycle.svg"
       alt="**Figure:** End-to-end ML pipeline flow">
  <figcaption>**Figure:** End-to-end ML pipeline flow</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: An end-to-end ML pipeline connects problem definition to deployment and maintenance

### Detailed Notes  
- Use this slide to **break the “model-centric” mindset** students often bring into ML courses. Emphasize that models are only one component of a much larger system.  
- Walk through the pipeline sequentially, stressing that earlier decisions constrain everything that follows. Poor problem framing or data collection cannot be fixed later by better algorithms.  
- Highlight that feature engineering and preprocessing typically consume more time and effort than model training in real projects.  
- Emphasize validation and evaluation as gates, not formalities. These steps determine whether a model is ready to be trusted.  
- Make deployment concrete: models must integrate with existing systems, users, and workflows. Accuracy alone is insufficient.  
- Stress that monitoring is not optional. Data distributions shift, behaviors change, and models degrade without maintenance.  
- Pedagogically, frame the pipeline as a **loop**, not a checklist. Real-world ML systems cycle continuously through these stages.

### Deeper Dive  
An end-to-end ML pipeline formalizes machine learning as a **socio-technical system**, not a standalone algorithm. Each stage addresses a distinct failure mode: framing errors, data quality issues, representation mismatch, overfitting, or deployment misalignment.

Crucially, pipelines are **path-dependent**. Choices made early—such as how labels are defined or which data sources are used—propagate forward and shape what is possible downstream. This is why ML failures are often systemic rather than algorithmic.

Deployment introduces new constraints absent in offline experimentation:
- Latency and scalability requirements.  
- Integration with legacy systems.  
- Human interaction and trust.  

Once deployed, models encounter **non-stationarity**: the statistical properties of incoming data change over time due to evolving behavior, market conditions, or feedback effects. Monitoring detects these shifts, while retraining or redesign restores performance.

The pipeline perspective reframes success: a model that performs well in isolation but fails in production is not a successful ML system. Robust machine learning requires continuous alignment between problem definition, data, modeling, and real-world use—an alignment maintained through disciplined pipeline design.
:::


## Success in ML depends on the pipeline, not just the model


- ML success depends on the **entire process**, not just algorithms.  
- Beginners often overemphasize model tweaking relative to other steps.  
- Just as important—often more so—are:  
  - Problem framing.  
  - Data preparation.  
  - Careful evaluation.  
  - Ongoing monitoring.  


::: {.notes}
### Slide:: Success in ML depends on the pipeline, not just the model

### Detailed Notes  
- Treat this slide as a **capstone mindset slide** for the pipeline section. The goal is to reset student intuitions about where value actually comes from.  
- Emphasize that algorithm choice is usually a **second-order decision** once the pipeline is well designed.  
- Explicitly call out a common novice mistake: repeatedly swapping models while ignoring flawed problem definitions or weak data.  
- Walk through the listed components and stress their leverage:  
  - Problem framing determines whether the task is learnable.  
  - Data preparation determines signal-to-noise ratio.  
  - Evaluation determines whether performance claims are real.  
  - Monitoring determines whether value persists after deployment.  
- Pedagogically, encourage students to see modeling as *one tool* in a broader decision system, not the focal point.  
- Reinforce that professional ML work is often more about discipline and process than about clever algorithms.

### Deeper Dive  
Empirical studies and industry experience consistently show that marginal gains from algorithmic sophistication are often dwarfed by gains from better data, clearer objectives, and more rigorous evaluation. Once a reasonable model class is chosen, improvements in performance frequently plateau unless upstream issues are addressed.

The pipeline perspective also explains why many ML projects fail despite technically correct models. Failures often arise from:
- Misaligned objectives encoded in labels.  
- Data leakage or biased evaluation.  
- Deployment environments that differ from training assumptions.  
- Lack of monitoring as data distributions shift.

From a systems perspective, a model is best understood as a **component embedded in a workflow**, interacting with data sources, users, and downstream decisions. Optimizing the component in isolation rarely optimizes the system.

This insight has practical implications for how ML work is organized and evaluated. Teams that invest early in problem framing, data governance, and evaluation infrastructure consistently outperform those that focus narrowly on model selection. The pipeline mindset is therefore not just a technical preference—it is a strategic one.
:::


# IV. Introduction to Data Preprocessing  


## Data preprocessing is essential because real-world data is messy

- Models perform only as well as their inputs.  
- “Garbage in, garbage out”: poor-quality data leads to unreliable models.  
- Preprocessing ensures consistency, interpretability, and readiness for modeling.  

::: {.notes}

### Slide:: Data preprocessing is essential because real-world data is messy

### Detailed Notes  
- Introduce this slide as a **reset of expectations**: machine learning does not start with elegant datasets, but with imperfect, human-generated data.  
- Emphasize that preprocessing is not a cosmetic step; it directly determines what the model can and cannot learn.  
- Reinforce the idea that models are fundamentally *input–output machines*. If the inputs are flawed, no algorithm can compensate reliably.  
- Use the phrase “garbage in, garbage out” deliberately, but clarify that “garbage” often means subtle issues like inconsistencies, missingness, or bias—not obvious errors.  
- Pedagogically, warn students that preprocessing is often where most project time is spent, even though it receives less attention than modeling.  
- Set expectations for the upcoming section: preprocessing is about **making data learnable**, not just “clean.”

### Deeper Dive  
Data preprocessing addresses the mismatch between how data is generated in the real world and how learning algorithms expect data to be structured. Real-world data reflects human behavior, operational constraints, and measurement processes, all of which introduce noise, inconsistency, and bias.

From a modeling perspective, preprocessing serves several functions:
- **Stability:** Ensuring numerical scales and formats do not distort learning.  
- **Validity:** Making sure variables represent what they claim to measure.  
- **Comparability:** Aligning units, categories, and representations across observations.  

Importantly, preprocessing decisions embed assumptions about what variation matters and what variation should be suppressed. Removing outliers, imputing missing values, or standardizing features all shape the learning objective implicitly.

Poor preprocessing can introduce hidden failure modes: leakage, spurious correlations, or models that perform well in development but fail in deployment. Conversely, thoughtful preprocessing often yields larger gains than switching algorithms.

This slide establishes a core principle: preprocessing is not preparation for modeling—it is **part of modeling**. Understanding and justifying preprocessing choices is essential for building reliable and interpretable machine learning systems.
:::



## Raw data often contains missing values, inconsistencies, and outliers 

:::: {.columns}
::: {.column width="50%"}

- **Missing values:**  
  - Example: Age column with blanks.  
  - Problem: Most models cannot directly handle missing values without preprocessing.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/messy_data.png"
       alt="**Figure:** Examples of messy data entries">
  <figcaption>**Figure:** Examples of messy data entries</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Raw data often contains missing values, inconsistencies, and outliers

### Detailed Notes  
- Use this slide to **make data problems concrete**. Students often underestimate how messy real datasets are until they encounter them directly.  
- Start with missing values, since they are the most common and immediately disruptive issue. Emphasize that many algorithms expect complete numerical inputs.  
- Explain that missingness is not just a technical nuisance; it often reflects **systematic processes** (nonresponse, sensor failure, data entry practices).  
- Briefly preview that missing values, inconsistencies, and outliers are related problems that require different treatments, even though they often co-occur.  
- Pedagogically, normalize this messiness. The presence of missing data does not mean the dataset is “bad,” but it does mean preprocessing is unavoidable.  
- Set expectations: subsequent slides will treat each issue separately and introduce tools for handling them responsibly.

### Deeper Dive  
Missing values arise when observations are not recorded, lost, or deemed inapplicable. From a statistical perspective, missingness can occur under different mechanisms—completely at random, at random conditional on observed data, or not at random—each with different implications for modeling.

In practice, most machine learning algorithms require complete feature vectors. As a result, missing values must be addressed explicitly through strategies such as deletion, imputation, or modeling missingness itself. Each choice embeds assumptions about why data is missing and how it relates to the outcome.

Missing values rarely appear in isolation. They often coexist with inconsistent encodings (e.g., mixed formats) and outliers, compounding their impact. Naively ignoring missingness can distort learned relationships, bias estimates, or silently remove large portions of data.

This slide introduces a recurring theme in preprocessing: data issues are not merely technical defects, but reflections of how data was generated. Effective preprocessing requires understanding those generative processes rather than blindly applying fixes.
:::



## Raw data often contains missing values, inconsistencies, and outliers 

:::: {.columns}
::: {.column width="50%"}


- **Inconsistent formats:**  
  - Example: Dates as strings vs. timestamps; “Male/Female” vs. “M/F.”  
  - Problem: Models and encoders treat different representations as different values.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/messy_data.png"
       alt="**Figure:** Examples of messy data entries">
  <figcaption>**Figure:** Examples of messy data entries</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Raw data often contains missing values, inconsistencies, and outliers

### Detailed Notes  
- Use this slide to highlight that inconsistency is often **more dangerous than missingness**, because it can silently corrupt models without obvious errors.  
- Emphasize that inconsistent formats usually arise from **multiple data sources, human entry, or evolving data standards**.  
- Walk through the examples concretely:  
  - Dates stored as strings versus timestamps behave very differently in sorting, filtering, and modeling.  
  - Categorical values like gender or region may appear equivalent to humans but are distinct to algorithms.  
- Stress that models do not infer equivalence unless explicitly told to do so through preprocessing.  
- Pedagogically, warn students that inconsistent formats often pass basic validation checks and therefore require **deliberate inspection**.  
- Set expectations: resolving inconsistencies is a prerequisite for encoding, scaling, and feature engineering.

### Deeper Dive  
Inconsistent formats reflect a breakdown between **semantic meaning** and **syntactic representation**. While humans easily recognize that “Male,” “M,” and “male” refer to the same concept, machine learning systems treat them as unrelated symbols unless preprocessing enforces equivalence.

These inconsistencies introduce several risks:
- Artificial category explosion in categorical variables.  
- Incorrect ordering or distance calculations in numeric or temporal variables.  
- Hidden data leakage when formats encode unintended information (e.g., missing dates represented as special strings).

From a modeling perspective, inconsistent formats increase dimensionality and noise without adding signal. They degrade model performance while remaining difficult to diagnose through aggregate metrics.

Resolving inconsistencies requires **standardization decisions**: choosing canonical representations, enforcing schemas, and validating data types. These decisions are not neutral—they embed assumptions about meaning, hierarchy, and comparability.

This slide reinforces a broader lesson: preprocessing is not merely about fixing errors, but about **aligning representations with intended semantics** so that learning algorithms operate on meaningful structure rather than accidental artifacts.
:::



## Raw data often contains missing values, inconsistencies, and outliers 

:::: {.columns}
::: {.column width="50%"}

- **Outliers:**  
  - Example: Income recorded as \$1,000,000,000.  
  - Problem: Can distort scales, influence parameter estimates, and skew distance-based methods.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/messy_data.png"
       alt="**Figure:** Examples of messy data entries">
  <figcaption>**Figure:** Examples of messy data entries</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Raw data often contains missing values, inconsistencies, and outliers

### Detailed Notes  
- Use this slide to emphasize that outliers are not just “weird data points,” but **high-leverage influences** on many models.  
- Walk through the example carefully: an income of \$1,000,000,000 is implausible in most datasets and likely reflects an error, unit mismatch, or rare edge case.  
- Stress that outliers can dominate learning even when they represent a tiny fraction of the data.  
- Highlight which methods are especially vulnerable:  
  - Linear regression (coefficients pulled by extreme values).  
  - Distance-based models (KNN, clustering).  
- Pedagogically, caution against automatic deletion. The question is not “Is this extreme?” but “Is this *meaningful* or *erroneous* for the task?”  
- Set expectations: handling outliers is about judgment and context, not applying a universal rule.

### Deeper Dive  
Outliers are observations that lie far from the bulk of the data distribution. They may arise from data entry errors, measurement failures, unit mismatches, or genuinely rare but valid events. From a modeling standpoint, these cases behave very differently depending on the algorithm.

In parametric models like linear regression, outliers can exert **disproportionate influence** on estimated parameters, especially when combined with high leverage in the predictor space. In unsupervised settings, outliers can distort centroids, inflate variance estimates, or create artificial clusters.

Importantly, outliers are not inherently “bad.” In some domains—fraud detection, risk management, anomaly detection—they are precisely the cases of interest. Blindly removing them can erase the very signal a model is meant to detect.

Effective outlier handling therefore requires:
- Diagnosing whether extreme values are errors or meaningful extremes.  
- Understanding model sensitivity to such values.  
- Choosing mitigation strategies aligned with the modeling goal (e.g., transformation, robust methods, capping, or separate modeling).

This slide reinforces a central preprocessing principle: **extremes demand interpretation, not reflexive correction**. How outliers are handled can change both model behavior and the story the data tells.
:::


## Tools for handling missing values, encoding categories, and scaling features

- **Missing values:**  
  - Drop rows or columns with excessive missingness (when justified).  
  - Impute using mean, median, mode, or domain-informed rules.  
  - Add a “missingness indicator” feature when absence may be informative.  


::: {.notes}
### Slide:: Tools for handling missing values, encoding categories, and scaling features

### Detailed Notes  
- Present this slide as the **first concrete toolkit** students encounter after diagnosing data problems. The emphasis is on options, not prescriptions.  
- Stress that missing-value handling is always a **design decision** informed by data volume, missingness patterns, and domain context.  
- Walk through the strategies in increasing order of sophistication:  
  - Dropping data is simple but potentially costly.  
  - Statistical imputation preserves sample size but adds assumptions.  
  - Missingness indicators acknowledge that “missing” can itself be informative.  
- Emphasize that there is no universally correct choice; different strategies answer different modeling questions.  
- Pedagogically, warn students against reflexively imputing means without thinking about what missingness represents.  
- Set expectations: later preprocessing steps (encoding and scaling) assume missingness has already been handled consistently.

### Deeper Dive  
Handling missing values is fundamentally about managing **information loss and bias**. Dropping rows or columns reduces complexity but can systematically exclude certain cases, especially when missingness is not random. This can distort both estimation and generalization.

Imputation replaces missing values with plausible substitutes. Simple approaches (mean, median, mode) are easy to implement but implicitly assume that missingness is unrelated to the outcome once observed variables are controlled for. More sophisticated approaches—such as model-based or domain-specific imputations—attempt to preserve structure but introduce additional assumptions.

Missingness indicators introduce an explicit feature signaling absence. This strategy is particularly powerful when missingness is **informative**—for example, when failure to report income or skipping a survey question correlates with behavior or risk. In such cases, the fact that a value is missing may be more predictive than the value itself.

The broader lesson is that missing data is not just a technical nuisance; it reflects how data is generated and collected. Effective preprocessing treats missingness as a modeling signal to be understood and incorporated, rather than automatically erased.
:::



## Tools for handling missing values, encoding categories, and scaling features

- **Categorical variables:**  
  - **One-hot encoding:** Convert unordered categories into binary indicator columns.  
  - **Ordinal encoding:** Map ordered categories to integers reflecting rank.  
 

::: {.notes}
### Slide:: Tools for handling missing values, encoding categories, and scaling features

### Detailed Notes  
- Introduce this slide as addressing a **fundamental incompatibility**: most ML models operate on numbers, but real-world data is often categorical.  
- Emphasize that encoding is not just a formatting step; it determines how the model interprets category relationships.  
- Walk through one-hot encoding first, stressing that it is appropriate when categories have **no natural order**.  
- Then introduce ordinal encoding, making clear that it should only be used when categories have a **meaningful ranking**.  
- Pedagogically, warn students against assigning numbers to categories “just to make them numeric.” That shortcut often introduces false structure.  
- Reinforce that encoding choices interact with model choice: linear models, trees, and distance-based methods respond differently to encodings.  
- Set expectations: poor encoding can silently bias results even when models appear to perform well.

### Deeper Dive  
Categorical encoding translates qualitative distinctions into quantitative representations suitable for learning algorithms. The challenge is to preserve **semantic meaning** without introducing artificial structure.

One-hot encoding represents each category as a separate binary feature. This encoding imposes no ordering and treats all categories as equidistant. It works well for linear models and neural networks but can increase dimensionality substantially when categories are numerous.

Ordinal encoding assigns integer values to categories with an inherent order (e.g., low, medium, high). While compact, this encoding implicitly assumes monotonicity: higher codes correspond to “more” of something. It should only be used when that assumption is justified.

Misuse of ordinal encoding is a common source of modeling error. Assigning arbitrary integers to unordered categories causes models to infer relationships that do not exist, especially in linear and distance-based methods.

Encoding is therefore a representational choice, not a technical afterthought. Effective preprocessing aligns numeric representations with the conceptual structure of the underlying variable, ensuring that models learn meaningful patterns rather than artifacts of encoding decisions.
:::



## Tools for handling missing values, encoding categories, and scaling features

- **Scaling and normalization:**  
  - **Standardization:** Mean = 0, standard deviation = 1.  
  - **Normalization:** Rescale features to a fixed range (e.g., 0–1).  
  - Essential for scale-sensitive models (e.g., KNN, neural networks).  
  

::: {.notes}
### Slide:: Tools for handling missing values, encoding categories, and scaling features

### Detailed Notes  
- Frame this slide as addressing a **numerical alignment problem**: many ML algorithms assume features are comparable in scale, even when the raw data is not.  
- Emphasize that scaling does not change the *information* in the data, but it changes how that information is weighted during learning.  
- Walk through standardization first, stressing that centering and scaling to unit variance makes coefficients and gradients more stable.  
- Then contrast normalization, explaining that it constrains values to a fixed range and is especially useful when features have hard bounds.  
- Highlight that scale sensitivity is **model-dependent**:  
  - Distance-based models (KNN, clustering) are directly affected.  
  - Gradient-based models (neural networks) train more reliably when features are scaled.  
- Pedagogically, caution students not to scale blindly; some models (e.g., tree-based methods) are largely scale-invariant.  
- Set expectations: scaling choices should be consistent across training, validation, and test data.

### Deeper Dive  
Scaling and normalization address numerical properties of the feature space that affect optimization and geometry. In many models, distances, dot products, or gradients implicitly assume that features are on comparable scales. When this assumption is violated, learning can become unstable or biased toward large-magnitude variables.

Standardization transforms features to have zero mean and unit variance. This is particularly important for gradient-based optimization, where unscaled features can lead to slow convergence or poorly conditioned loss landscapes. It also aids interpretability in linear models by making coefficients comparable across predictors.

Normalization rescales features to a fixed interval, often [0,1]. This is useful when features have known bounds or when models are sensitive to absolute magnitudes rather than variance. However, normalization can be sensitive to outliers, which may compress the majority of observations into a narrow range.

A critical operational detail is **fit–apply discipline**: scaling parameters must be learned from the training data and then applied unchanged to validation and test sets. Violating this rule introduces data leakage and inflates performance estimates.

The broader lesson is that scaling is not cosmetic. It shapes the geometry of the learning problem and can determine whether a model trains effectively at all. Thoughtful scaling is therefore a prerequisite for reliable
:::


# V. Feature Engineering

## Data leakage occurs when preprocessing uses information from validation or test data

<figure>
  <img src="../materials/assets/images/apply_preprocessing_light.svg"
       alt="**Figure:** Always fit preprocessing steps on the **training set only**.">
  <figcaption>**Figure:** Always fit preprocessing steps on the **training set only**.</figcaption>
</figure>

::: {.notes}
### Slide:: Data leakage occurs when preprocessing uses information from validation or test data

### Detailed Notes  
- Treat this slide as a **hard stop warning**. Emphasize that leakage invalidates evaluation results, regardless of how sophisticated the model is.  
- Define leakage concretely: any time information from validation or test data influences model training or preprocessing decisions, performance estimates become optimistic.  
- Walk students through the correct workflow explicitly:  
  - Fit preprocessing steps **only** on the training data.  
  - Apply the learned transformations unchanged to validation and test data.  
- Use the figure to reinforce the “fit once, apply everywhere else” rule visually.  
- Pedagogically, call out common leakage mistakes:  
  - Scaling using the full dataset.  
  - Imputing missing values using global statistics.  
  - Performing feature selection before splitting data.  
- Stress that leakage often happens unintentionally and is therefore especially dangerous.  
- Set expectations: disciplined pipelines and tooling exist largely to prevent this exact failure mode.

### Deeper Dive  
Data leakage occurs when the separation between training and evaluation data is violated. This violation can be direct—such as computing normalization statistics on the full dataset—or indirect, such as selecting features based on correlations computed using all data before splitting.

From a statistical perspective, leakage contaminates the test set, making it no longer an unbiased proxy for future data. The resulting performance estimates systematically overstate generalization and cannot be corrected after the fact.

Leakage is particularly insidious because it often improves metrics without triggering errors or warnings. Models appear to perform well, leading practitioners to deploy systems that fail in production.

Preventing leakage requires strict adherence to **procedural discipline**:
- Split data early.  
- Fit all data-dependent transformations on training data only.  
- Encapsulate preprocessing and modeling steps into unified pipelines.  

This slide reinforces a central theme of the course: reliable machine learning depends as much on **process integrity** as on algorithms. Leakage undermines that integrity and erodes trust in any reported result.
:::


## Feature engineering transforms raw data into more informative inputs

- Raw datasets rarely contain features in the most useful form.  
- **Feature engineering** = transforming raw variables into predictors that expose structure.  
- Goal: improve performance by embedding domain knowledge into representations.  
- Example: From “date,” derive “day of week,” “is holiday,” or “month.”  

::: {.notes}
### Slide:: Feature engineering transforms raw data into more informative inputs

### Detailed Notes  
- Frame this slide as the point where **human understanding and machine learning intersect most directly**. Feature engineering is where insight matters.  
- Emphasize that raw variables are often poor representations of the underlying phenomena we care about. The model only sees what we choose to show it.  
- Walk through the definition carefully: feature engineering is not adding new data, but **re-expressing existing data** to make patterns easier to learn.  
- Use the date example deliberately, since it is intuitive and widely applicable. A raw timestamp hides periodic structure that models cannot infer easily on their own.  
- Stress that feature engineering can dramatically improve performance even with simple models.  
- Pedagogically, caution that feature engineering is hypothesis-driven: every transformation encodes an assumption about what matters.  
- Set expectations: later slides will show common transformations, but the deeper skill is knowing *when* and *why* to apply them.

### Deeper Dive  
Feature engineering addresses the mismatch between how data is recorded and how predictive structure manifests. Many raw variables are proxies—timestamps, IDs, logs—that obscure the relationships models need to learn.

By transforming features, practitioners introduce **inductive bias**: structured assumptions that guide learning toward meaningful patterns. For example, extracting day-of-week from a date injects the assumption that behavior may follow weekly cycles, an assumption grounded in domain knowledge.

Feature engineering can:
- Linearize nonlinear relationships.  
- Reveal periodicity or thresholds.  
- Reduce noise by aggregation.  
- Improve interpretability of downstream models.

Importantly, feature engineering shifts effort from algorithm selection to representation design. In many applied settings, a well-engineered feature set paired with a simple model outperforms a complex model trained on raw inputs.

The tradeoff is that feature engineering is subjective and context-dependent. Poorly chosen transformations can mislead models or encode spurious correlations. As such, feature engineering is best understood as a disciplined form of hypothesis testing, where transformations are proposed, evaluated, and refined within a robust validation framework.
:::


## Binning: Convert continuous variables into categories. Example: Age → groups (0–18, 19–35, 36–60, 60+).

:::: {.columns} 
::: {.column width="50%"} 
<figure>
  <img src="../materials/assets/images/age_hist.png"
       alt="">
  <figcaption></figcaption>
</figure>

::: 
::: {.column width="50%"} 

<figure>
  <img src="../materials/assets/images/age_binned_counts.png"
       alt="">
  <figcaption></figcaption>
</figure>
 
::: 
::::

::: {.notes}
### Slide:: Binning: Convert continuous variables into categories (e.g., Age → groups)

### Detailed Notes  
- Introduce binning as a **deliberate loss of precision** in exchange for interpretability or robustness. Emphasize that this is a choice, not a requirement.  
- Explain that binning is most useful when **thresholds matter more than exact values**, such as legal age cutoffs, eligibility rules, or life-stage groupings.  
- Walk students through the visual comparison:  
  - The raw histogram shows fine-grained variation.  
  - The binned version highlights broader structure and simplifies interpretation.  
- Stress that binning can make models easier to explain and sometimes more stable, especially with noisy or sparse data.  
- Pedagogically, caution that binning throws away information and should therefore be justified, not automatic.  
- Set expectations: binning competes with other feature transformations (polynomials, interactions) and should be evaluated empirically.

### Deeper Dive  
Binning discretizes a continuous variable into intervals, replacing numeric precision with categorical structure. This transformation imposes a **piecewise-constant assumption**: values within the same bin are treated as equivalent with respect to the outcome.

Binning can be advantageous when:
- The true relationship exhibits sharp changes at known thresholds.  
- Measurement error makes fine-grained values unreliable.  
- Interpretability is prioritized over marginal predictive gains.

However, binning introduces several tradeoffs:
- **Information loss:** Differences within bins are ignored.  
- **Boundary sensitivity:** Small changes near cutpoints can flip categories.  
- **Arbitrariness:** Bin definitions embed subjective assumptions.

From a modeling perspective, binning can be seen as a coarse approximation to nonlinear effects. Alternatives such as polynomial terms, splines, or tree-based models often capture smooth nonlinear structure more faithfully while retaining continuity.

The key lesson is not that binning is good or bad, but that it encodes a specific hypothesis about how a variable matters. As with all feature engineering choices, binning should be motivated by domain knowledge and validated through careful evaluation.
:::



## Log-Scaling reduces skewness of variables like income or housing prices. {.layout-two-content}

:::: {.columns} 
::: {.column width="50%"} 
<figure>
  <img src="../materials/assets/images/income_hist.png"
       alt="**Figure:** Example of raw skewed data">
  <figcaption>**Figure:** Example of raw skewed data</figcaption>
</figure>

::: 
::: {.column width="50%"} 
<figure>
  <img src="../materials/assets/images/income_log_hist.png"
       alt="**Figure:** Example of log transformation on skewed data">
  <figcaption>**Figure:** Example of log transformation on skewed data</figcaption>
</figure>

::: 
::::

::: {.notes}
### Slide:: Log-scaling reduces skewness in variables like income or housing prices

### Detailed Notes  
- Introduce log-scaling as a **shape-changing transformation**, not a cosmetic adjustment. The goal is to make patterns easier for models to learn.  
- Use income and housing prices deliberately: students intuitively recognize these as right-skewed variables with extreme values.  
- Walk through the visual comparison explicitly:  
  - The raw distribution is dominated by a long right tail.  
  - The log-transformed distribution is more compact and symmetric.  
- Emphasize that log-scaling compresses large values while preserving rank order.  
- Pedagogically, stress that log-scaling often improves both **model performance and interpretability**, especially in linear models.  
- Clarify that log-scaling changes the meaning of coefficients and predictions, which must be interpreted on the transformed scale.  
- Set expectations: log transforms are common but not automatic; they should be justified by data shape and domain meaning.

### Deeper Dive  
Log-scaling is a monotonic transformation that reduces right skewness by applying a nonlinear compression to large values. For variables like income, prices, or counts, this reflects a realistic assumption: relative differences often matter more than absolute differences.

From a modeling perspective, log transformations can:
- Stabilize variance (reducing heteroscedasticity).  
- Linearize multiplicative relationships.  
- Reduce the influence of extreme values without discarding them.  

In linear regression, log-transforming the response or predictors often yields residuals that better satisfy homoscedasticity and linearity assumptions, improving both estimation and inference. In other models, log-scaling improves numerical stability and gradient behavior.

Interpretation shifts after transformation. For example, coefficients in a log-linear model correspond to **percentage changes**, not unit changes. This can align more naturally with economic reasoning, where proportional effects are often more meaningful.

However, log-scaling is not universally appropriate. It requires strictly positive values and embeds assumptions about diminishing marginal effects. As with all feature engineering choices, it should be motivated by both data diagnostics and substantive understanding of the domain.

The broader lesson is that transformations like log-scaling are tools for aligning data geometry with model assumptions—not tricks to inflate performance metrics.
:::


## Interactions combine features to capture joint effects.

:::: {.columns}
::: {.column width="50%"}

 - Example: BMI = weight / (height²).  
 - Example: Age × Exercise Frequency.  

:::

::: {.column width="50%"}

<figure>
  <img src="../materials/assets/images/Interaction_scatter.png"
       alt="**Figure:** Example of interaction features">
  <figcaption>**Figure:** Example of interaction features</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Interactions combine features to capture joint effects

### Detailed Notes  
- Introduce interactions as a way to model situations where **the effect of one variable depends on another**.  
- Emphasize that interactions move beyond additive thinking: predictors no longer contribute independently.  
- Use the examples deliberately to show two common forms:  
  - Domain-defined composite features (BMI).  
  - Explicit interaction terms (Age × Exercise Frequency).  
- Clarify that BMI is a **constructed feature** reflecting known physiology, while Age × Exercise is a learned hypothesis about joint influence.  
- Pedagogically, stress that interactions are powerful but increase complexity and risk overfitting if added indiscriminately.  
- Encourage students to ask: *Is there a plausible reason these variables should interact?*  
- Set expectations: interactions often improve models when supported by theory or domain insight, not by brute-force inclusion.

### Deeper Dive  
Interaction features allow models to represent **non-additive relationships**. In an additive model, the effect of each predictor is assumed constant regardless of other variables. Interactions relax this assumption by allowing effects to vary conditionally.

Mathematically, a simple interaction takes the form:
\[
y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 (X_1 \times X_2) + \epsilon
\]
Here, the marginal effect of \(X_1\) depends on the value of \(X_2\), and vice versa.

Not all feature combinations are interactions in this strict sense. Composite features like BMI encode **known nonlinear structure** derived from domain knowledge, whereas interaction terms allow the model to estimate joint effects directly from data.

Interactions can dramatically improve model expressiveness, but they also:
- Increase dimensionality.  
- Complicate interpretation.  
- Amplify variance in small datasets.

As a result, interaction terms should be motivated by theory, exploratory analysis, or prior evidence. When used thoughtfully, they allow relatively simple models to capture rich behavior without resorting to black-box methods.

The broader takeaway is that interactions are a way of **injecting structural hypotheses** into the feature space—making models more realistic when additive assumptions are too restrictive.
:::



## Feature selection chooses the most useful predictors

- More features ≠ always better.  
- Too many features can increase variance, reduce interpretability, and slow training.  
- **Feature selection** = identifying a subset of predictors that preserves signal while reducing noise.  
- Approaches include:  
  - **Filter methods:** correlation, variance threshold.  
  - **Wrapper methods:** forward/backward selection.  
  - **Embedded methods:** regularization (e.g., Lasso), tree-based importance.  

::: {.notes}
### Slide:: Feature selection chooses the most useful predictors

### Detailed Notes  
- Frame this slide as the **counterbalance to feature engineering**: after creating features, we must decide which ones to keep.  
- Emphasize that more features do not automatically improve models; beyond a point, they often hurt generalization.  
- Stress that feature selection is about **signal-to-noise tradeoffs**, not just computational convenience.  
- Walk through the three families of methods conceptually:  
  - Filter methods as fast, model-agnostic screens.  
  - Wrapper methods as performance-driven but computationally expensive.  
  - Embedded methods as integrated, model-based selection.  
- Pedagogically, caution that feature selection itself can cause overfitting if performed incorrectly (e.g., before data splitting).  
- Reinforce that feature selection should always be evaluated within a proper validation framework.  
- Set expectations: later regularization methods formalize feature selection through penalty-based control of complexity.

### Deeper Dive  
Feature selection addresses the curse of dimensionality by reducing the effective size of the hypothesis space. As the number of predictors grows, models become more flexible, variance increases, and spurious correlations become more likely—especially when sample sizes are limited.

Filter methods operate independently of any specific model. They assess each feature’s relationship with the target using simple criteria such as correlation or variance. While computationally efficient, they ignore interactions and conditional relationships.

Wrapper methods evaluate subsets of features by training and testing models repeatedly. Forward and backward selection fall into this category. These methods can capture joint effects but are computationally expensive and prone to overfitting if not carefully validated.

Embedded methods perform feature selection as part of model training. Regularization techniques like Lasso shrink some coefficients to zero, while tree-based models rank features by their contribution to reducing impurity or error. These approaches balance efficiency and adaptivity but inherit the assumptions of the underlying model.

The central insight is that feature selection is not about finding the “true” set of predictors, but about identifying a representation that supports **stable, generalizable learning**. In modern practice, feature selection and regularization are often complementary tools for controlling complexity rather than competing alternatives.
:::




## A small set of well-designed features often explains most predictive power

- Often, a minority of features explains a majority of predictive performance.  
- Good feature design can outperform sophisticated algorithms applied to poor data.  
- Emphasizes the importance of *thoughtful* feature engineering.  

::: {.notes}
### Slide:: A small set of well-designed features often explains most predictive power

### Detailed Notes  
- Present this slide as a **synthesis insight** rather than a new technique. The goal is to reshape how students think about leverage in ML projects.  
- Emphasize that predictive power is rarely evenly distributed across features; a few variables often carry most of the signal.  
- Use this to counter the instinct to “throw everything into the model” and hope complexity does the work.  
- Reinforce that feature design determines what the model *can possibly learn*—algorithms only optimize within that space.  
- Pedagogically, stress that this is why experienced practitioners spend disproportionate time understanding data and variables.  
- Encourage students to reflect on feature usefulness qualitatively, not just through automated selection methods.  
- Position this idea as a bridge to regularization and model simplicity in later sections.

### Deeper Dive  
Empirically, many predictive tasks exhibit a highly skewed distribution of feature importance. A small subset of variables captures most of the explainable variance, while the remainder contribute marginally or add noise. This phenomenon arises from redundancy, weak signal strength, and correlations among predictors.

Well-designed features often encode:
- Aggregation over noisy raw inputs.  
- Domain-specific structure or constraints.  
- Transformations that linearize relationships or stabilize variance.  

In such cases, even simple models can perform strongly because the representation itself carries the predictive burden. Conversely, complex models trained on poorly engineered features are forced to infer structure that is weak, noisy, or misaligned with the task.

This observation explains why feature engineering is sometimes described as the “art” of machine learning. It requires judgment, domain insight, and iterative refinement rather than purely mechanical application of algorithms.

The deeper lesson is that **model complexity cannot compensate for representational failure**. Investing effort in a small number of high-quality features often yields greater returns than increasing algorithmic sophistication, especially in applied and resource-constrained settings.
:::



# Evaluating Model Performance (Preview)  


## Evaluation is about generalization, not just training accuracy

- Training performance alone is not sufficient.  
- We need metrics that reflect **generalization to unseen data**.  
- Different tasks require different evaluation metrics.  

::: {.notes}

### Slide: Evaluation is about generalization, not just training accuracy

### Detailed Notes  
- Introduce this slide as a **conceptual reset**: evaluation is not about how good the model looks during training, but how it behaves when it matters.  
- Emphasize that strong training performance is expected—even trivial—for flexible models; it is not evidence of success.  
- Reinforce the definition of generalization: performance on **new, unseen data drawn from the same process**.  
- Use this moment to connect back to overfitting and data splits, making evaluation the natural consequence of those earlier ideas.  
- Stress that evaluation metrics must align with the task and decision context; there is no universal “best” metric.  
- Pedagogically, warn students against treating evaluation as a checkbox at the end of modeling. It is a core design decision.  
- Frame this section as a preview: the next slides will unpack *how* to evaluate properly, not just *why*.

### Deeper Dive  
Model evaluation formalizes the distinction between **in-sample fit** and **out-of-sample performance**. Training metrics measure how well a model fits observed data, while evaluation metrics aim to estimate expected performance on future observations.

This distinction matters because learning algorithms optimize training objectives directly. Without external evaluation, there is no mechanism to detect overfitting or misalignment between the training objective and real-world goals.

Generalization-focused evaluation depends on three components:
- Proper data separation (train, validation, test).  
- Metrics that reflect the task’s loss structure.  
- Assumptions about data stationarity between training and deployment.

Different tasks require different evaluation lenses. Classification emphasizes tradeoffs between types of errors, while regression focuses on magnitude and variability of prediction errors. Using an inappropriate metric can make a weak model appear strong—or a strong model appear weak.

The deeper message is that evaluation is not an afterthought or a reporting exercise. It is the **bridge between modeling and decision-making**, translating statistical performance into operational confidence.
:::


## Classification models are judged with metrics like accuracy, precision, recall, and F1-score 

:::: {.columns}
::: {.column width="50%"}


- No single metric tells the full story.  
- Metrics reflect different types of errors and tradeoffs.  
- Choosing the right metric depends on context and consequences.  
 

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/confusion_matrix_light.svg"
       alt="**Figure:** Confusion matrix showing TP, FP, TN, FN">
  <figcaption>**Figure:** Confusion matrix showing TP, FP, TN, FN</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide: Classification models are evaluated using metrics like accuracy, precision, recall, and F1-score

### Detailed Notes  
- Introduce this slide as the **entry point into evaluation nuance** for classification problems. The goal is to break the idea that “accuracy” is sufficient.  
- Emphasize that classification errors are asymmetric: different mistakes have different real-world costs.  
- Use the confusion matrix visually to anchor all metrics in the same underlying counts (TP, FP, TN, FN).  
- Stress that accuracy is intuitive but often misleading, especially when classes are imbalanced.  
- Walk students through the intuition behind precision and recall without formulas:  
  - Precision asks, “When we predict positive, how often are we right?”  
  - Recall asks, “Of all true positives, how many did we catch?”  
- Introduce F1-score as a compromise metric that penalizes extreme tradeoffs.  
- Pedagogically, emphasize that metric choice is a **decision-making choice**, not a technical default.

### Deeper Dive  
Classification metrics arise from the confusion matrix, which decomposes predictions into true positives, false positives, true negatives, and false negatives. Each metric emphasizes different aspects of this error structure.

Accuracy measures overall correctness, but it implicitly weights all errors equally. In imbalanced datasets, a model can achieve high accuracy by predicting the majority class while failing at the task of interest.

Precision and recall decompose performance along different dimensions. Precision focuses on the reliability of positive predictions, making it appropriate when false positives are costly. Recall focuses on coverage of true positives, making it critical when missing positives is unacceptable.

The F1-score combines precision and recall using their harmonic mean. This formulation penalizes extreme imbalance between the two, ensuring that neither can be optimized at the complete expense of the other. However, it still encodes a specific tradeoff and may not reflect domain-specific costs.

The deeper lesson is that classification metrics are not neutral summaries; they encode value judgments about which errors matter most. Effective evaluation requires aligning metric choice with the real-world consequences of decisions driven by the model.

:::


## Accuracy: Proportion of correct predictions.

:::: {.columns}
::: {.column width="50%"}

- Measures overall correctness across all classes.  
- Simple and intuitive.  
- Can be misleading when classes are imbalanced.  


**Formula.**

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

**Use.** Works when classes are balanced; can mislead on imbalanced data.
:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/confusion_matrix_light.svg"
       alt="**Figure:** Confusion matrix showing TP, FP, TN, FN">
  <figcaption>**Figure:** Confusion matrix showing TP, FP, TN, FN</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide: Accuracy: Proportion of correct predictions

### Detailed Notes  
- Introduce accuracy as the **default metric students already know**, then immediately problematize it.  
- Emphasize that accuracy answers a very narrow question: *What fraction of predictions were correct overall?*  
- Walk through the formula briefly to anchor it in the confusion matrix, but do not dwell on algebra.  
- Stress that accuracy weights false positives and false negatives equally, regardless of context.  
- Use an intuitive imbalance example (e.g., rare disease, fraud detection) to show how accuracy can appear high while the model is useless.  
- Pedagogically, frame accuracy as a **starting point**, not a decision metric.  
- Set up the motivation for precision and recall as ways to look inside accuracy and understand *which* mistakes are being made.

### Deeper Dive  
Accuracy is defined as the proportion of correct predictions among all predictions:
\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\]
This formulation implicitly assumes that all errors are equally costly and that class frequencies are roughly balanced.

In balanced datasets, accuracy can be a reasonable summary of performance. However, when one class dominates, accuracy becomes insensitive to failures on the minority class. A classifier that always predicts the majority class can achieve high accuracy while providing no useful discrimination.

From a statistical perspective, accuracy collapses the confusion matrix into a single scalar, discarding information about the distribution of errors. This loss of detail makes it unsuitable for high-stakes or asymmetric decision contexts.

Accuracy remains useful as a diagnostic baseline and for sanity checks, but it should rarely be the sole metric guiding model selection. Its primary pedagogical value lies in motivating richer metrics that expose the structure of classification errors rather than obscuring them.
:::


## Precision: Of predicted positives, the fraction that are truly positive. 

:::: {.columns}
::: {.column width="50%"}

**Formula.**


- Focuses on the reliability of positive predictions.  
- Penalizes false positives.  
- Most useful when false alarms are costly.  


$$
\text{Precision} = \frac{TP}{TP + FP}
$$

**Use.** Prioritize when false positives are costly (e.g., flagging legitimate emails as spam).

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/confusion_matrix_light.svg"
       alt="**Figure:** Confusion matrix showing TP, FP, TN, FN">
  <figcaption>**Figure:** Confusion matrix showing TP, FP, TN, FN</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Precision: Of predicted positives, the fraction that are truly positive

### Detailed Notes  
- Introduce precision as a metric that answers a **conditional question**: given that the model predicts “positive,” how trustworthy is that prediction?  
- Emphasize that precision ignores true negatives entirely; it zooms in on the quality of positive predictions only.  
- Walk through the formula conceptually, tying it directly to false positives rather than to algebra.  
- Use the spam example deliberately: users care more about avoiding false alarms than catching every spam message.  
- Pedagogically, caution students not to interpret high precision as overall model quality—it says nothing about missed positives.  
- Reinforce that precision is most appropriate when **acting on a positive prediction is costly or disruptive**.  
- Set up recall as the complementary metric that answers a different question about missed cases.

### Deeper Dive  
Precision is defined as:
\[
\text{Precision} = \frac{TP}{TP + FP}
\]
It measures the proportion of predicted positives that are actually correct. From a decision-theoretic perspective, precision reflects the **cost of false positives** relative to true positives.

High precision means that when the model raises a flag, it is usually correct. This is critical in applications where false alarms are expensive, annoying, or harmful—such as spam filtering, fraud alerts, or medical follow-ups.

However, precision alone is incomplete. A model can achieve perfect precision by predicting “positive” only once, provided that single prediction is correct. Such a model would be useless in most settings because it fails to identify the majority of true cases.

Precision therefore captures one axis of performance, not overall effectiveness. Its value emerges only when interpreted alongside recall, which measures how many true positives are being missed. Together, these metrics expose the tradeoff between caution and coverage that underlies classification decisions.
:::


## Recall: Of actual positives, the fraction correctly identified. {.layout-two-content}

:::: {.columns}

::: {.column width="50%"}
- Focuses on coverage of true positive cases.  
- Penalizes false negatives.  
- Most useful when missing positives is costly.  


**Formula.**

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

**Use.** Prioritize when missing positives is costly (e.g., fraud or disease screening).

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/confusion_matrix_light.svg"
       alt="**Figure:** Confusion matrix showing TP, FP, TN, FN">
  <figcaption>**Figure:** Confusion matrix showing TP, FP, TN, FN</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Recall: Of actual positives, the fraction correctly identified

### Detailed Notes  
- Introduce recall as the **mirror image of precision**, answering a different conditional question.  
- Emphasize that recall asks: *Of all true positives that exist, how many did the model successfully capture?*  
- Walk through the formula conceptually by focusing on false negatives rather than algebra.  
- Use high-stakes examples where missing a positive is unacceptable, such as disease screening or fraud detection.  
- Pedagogically, caution that high recall often comes at the expense of precision; catching everything usually means more false alarms.  
- Reinforce that recall ignores true negatives entirely and therefore does not describe overall accuracy.  
- Set up the tension: precision and recall pull in opposite directions, motivating combined metrics.

### Deeper Dive  
Recall is defined as:
\[
\text{Recall} = \frac{TP}{TP + FN}
\]
It measures the proportion of actual positive cases that are correctly identified by the model. From a risk perspective, recall reflects tolerance for **false negatives**.

High recall is critical in domains where failing to detect a positive case carries significant cost. In medical screening, missing a disease may delay treatment. In fraud detection, missed fraud represents direct loss. In such contexts, it is often acceptable to tolerate false positives if it ensures high coverage.

However, recall alone is insufficient. A model that predicts every case as positive achieves perfect recall but provides no discrimination. This illustrates that recall captures **sensitivity**, not selectivity.

The practical value of recall lies in understanding what fraction of the relevant population the model reaches. When interpreted alongside precision, it reveals whether a model is conservative (high precision, low recall) or aggressive (high recall, low precision). Balancing these perspectives is essential for responsible classification system design.
:::


## F1-score: Harmonic mean of precision and recall. 

:::: {.columns}
::: {.column width="50%"}

- Combines precision and recall into a single metric.  
- Penalizes extreme imbalance between the two.  
- Useful when classes are imbalanced and both error types matter.  

**Formula.**

$$
\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$

$$
\text{Equivalently: } \text{F1} = \frac{2TP}{2TP + FP + FN}
$$

**Use.** Balances precision and recall; useful with class imbalance.
:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/confusion_matrix_light.svg"
       alt="**Figure:** Confusion matrix showing TP, FP, TN, FN">
  <figcaption>**Figure:** Confusion matrix showing TP, FP, TN, FN</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: F1-score: Harmonic mean of precision and recall

### Detailed Notes  
- Introduce F1 as a **compromise metric**, not a universally superior one. Its purpose is to balance two competing objectives.  
- Emphasize that F1 is only high when **both precision and recall are high**; improving one at the expense of the other will lower the score.  
- Walk through the intuition of the harmonic mean: it punishes extreme values more than a simple average would.  
- Use this to reinforce the idea that F1 discourages models that game one metric while ignoring the other.  
- Pedagogically, caution students against treating F1 as “the best” metric by default. It encodes a specific value judgment.  
- Connect this slide back to the broader theme: metrics are tools that reflect priorities, not objective truths.

### Deeper Dive  
The F1-score is defined as:
\[
\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\]
This formulation uses the harmonic mean, which is dominated by the smaller of the two inputs. As a result, F1 declines sharply when either precision or recall is low.

An equivalent expression in terms of confusion matrix counts,
\[
\text{F1} = \frac{2TP}{2TP + FP + FN},
\]
highlights that F1 ignores true negatives entirely. This makes it well-suited for imbalanced classification problems where the negative class is large and uninformative.

However, F1 assumes that false positives and false negatives are **equally costly**. In domains where these costs differ substantially, alternative metrics or weighted variants (e.g., \(F_\beta\)) may be more appropriate.

The deeper lesson is that F1 compresses a multidimensional tradeoff into a single scalar. This can be convenient for model comparison, but it inevitably hides information. Responsible evaluation often requires examining precision, recall, and confusion matrices alongside F1 rather than relying on it alone.
:::


## Regression models are evaluated using metrics like MSE, RMSE, and R²

:::: {.columns}
::: {.column width="50%"}

- **MSE (Mean Squared Error):** Average squared prediction error; heavily penalizes large mistakes.  
- **RMSE (Root Mean Squared Error):** Square root of MSE; expressed in the same units as the target.  
- **R² (Coefficient of Determination):** Proportion of variance explained relative to a baseline model.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/regression_residuals.png"
       alt="**Figure:** Residual errors plotted on regression line">
  <figcaption>**Figure:** Residual errors plotted on regression line</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Regression models are evaluated using metrics like MSE, RMSE, and R²

### Detailed Notes  
- Introduce this slide as the **regression counterpart** to classification metrics: different task, different lenses.  
- Emphasize that regression evaluation focuses on **magnitude of error**, not correctness of labels.  
- Walk through the three metrics in increasing interpretability:  
  - MSE as the optimization workhorse that penalizes large errors strongly.  
  - RMSE as the same idea translated back into meaningful units.  
  - R² as a relative measure of explanatory power.  
- Use the residual plot to reinforce that regression error is continuous, not binary.  
- Pedagogically, warn students not to rely on a single metric. Each answers a different question about performance.  
- Set expectations: later slides will unpack these metrics individually and discuss when each is appropriate.

### Deeper Dive  
Regression metrics quantify how far predictions deviate from observed outcomes. Mean Squared Error (MSE) computes the average of squared residuals, which makes it sensitive to large deviations. This sensitivity is often desirable during optimization, but it can make MSE difficult to interpret directly.

Root Mean Squared Error (RMSE) addresses this by taking the square root of MSE, restoring the original units of the target variable. This makes RMSE especially useful for communication and domain interpretation, such as dollars, units sold, or degrees.

R² measures the proportion of variance in the outcome explained by the model relative to a baseline that predicts the mean for all observations. While intuitive, R² is a relative measure: a high value does not guarantee accurate predictions, and a low value does not imply uselessness in all contexts.

Together, these metrics provide complementary perspectives. MSE and RMSE focus on **prediction accuracy**, while R² focuses on **explanatory fit**. Responsible evaluation involves understanding what each metric reveals—and conceals—about model behavior.
:::



## The right evaluation metric depends on the decision context


Examples:  
- Fraud detection → prioritize recall (minimize missed fraud).  
- Spam filters → prioritize precision (avoid false alarms).  
- House price prediction → RMSE is useful (errors interpretable in dollars).  


::: {.notes}
### Slide:: The right evaluation metric depends on the decision context

### Detailed Notes  
- Present this slide as a **capstone principle** for evaluation: metrics are tools for decisions, not objective truths about models.  
- Emphasize that metric choice encodes priorities about which errors matter most.  
- Walk through each example explicitly, tying the metric to real-world consequences rather than abstract definitions.  
- Reinforce that optimizing the “wrong” metric can produce models that look good numerically but fail operationally.  
- Pedagogically, encourage students to ask two questions before choosing a metric:  
  - *What decisions will this model inform?*  
  - *Which mistakes are most costly in that setting?*  
- Stress that metric choice should be justified and documented, not assumed or copied from examples.  
- Use this slide to bridge from technical evaluation back to managerial judgment.

### Deeper Dive  
Evaluation metrics implicitly define a **loss function**, even when no explicit cost matrix is provided. Choosing recall prioritizes avoiding false negatives, while choosing precision prioritizes avoiding false positives. Regression metrics prioritize different notions of error magnitude and interpretability.

In many applications, the true costs of errors are asymmetric and context-dependent. For example, missing fraud may be far more costly than investigating a false alarm, while misclassifying legitimate email as spam can erode trust disproportionately.

No single metric captures all dimensions of performance. As a result, practitioners often examine multiple metrics and select operating points (e.g., decision thresholds) that balance competing objectives. This practice highlights that evaluation is not purely statistical—it is **normative and strategic**.

The deeper lesson is that evaluation metrics should be chosen to align model behavior with organizational goals and risk tolerance. A technically optimal model under the wrong metric is functionally suboptimal in practice.
:::



# Introduction to Supervised Learning


## Supervised learning maps inputs to outputs using labeled data

- **Definition:** Learn mapping from inputs (X) → output (y).  
- **Regression vs. Classification:**  
  - Regression → continuous outputs (numbers).  
  - Classification → categorical outputs (labels).  

::: {.notes}
### Slide:: Supervised learning maps inputs to outputs using labeled data

### Detailed Notes  
- Introduce supervised learning as the **archetypal machine learning paradigm**, the one students will encounter most often in practice.  
- Emphasize that supervised learning depends on **pairs of examples**: every input \(X\) comes with a corresponding output \(y\). This pairing is what guides the learning algorithm.  
- Clarify that the goal is to learn a function that generalizes: it should perform well not only on the training examples, but on unseen cases drawn from the same process.  
- Revisit the regression/classification distinction slowly and clearly:  
  - Regression predicts *how much* (a number).  
  - Classification predicts *which category* (a label).  
- Pedagogically, stress that this distinction affects everything downstream—from loss functions to metrics to modeling choices.  
- Encourage students to start every ML problem by determining: *Is my output numerical or categorical?* This decision shapes the entire pipeline.

### Deeper Dive  
Supervised learning formalizes prediction as estimating a function \(f\) such that:
\[
y \approx f(X)
\]
where \(X\) is a vector of features and \(y\) is an observed label. The learning algorithm selects \( \hat{f} \) from a hypothesis class by minimizing prediction error on labeled examples. This process is grounded in **empirical risk minimization**, where the model minimizes an estimate of expected loss using sample data.

The distinction between regression and classification is not superficial. It reflects differences in:
- **Loss functions** (squared error vs. cross-entropy).  
- **Output spaces** (continuous real values vs. discrete categories).  
- **Uncertainty representation** (confidence intervals vs. class probabilities).  
- **Evaluation metrics** (RMSE vs. precision/recall).  

Supervised learning assumes that labels are accurate, meaningful, and representative. In practice, labels often act as **proxies** for underlying concepts of interest (e.g., churn, satisfaction, fraud). The quality of these labels fundamentally constrains what the model can learn.

Finally, supervised learning does not reveal causal structure; it learns statistical associations. This distinction becomes important when interpreting coefficients or deploying models in dynamic environments. Supervised learning is powerful, but its strength derives from recognizing its assumptions and limits as much as from its flexibility.
:::




## Regression problems arise across business and scientific domains 

- **Business examples:**  
  - Predict housing prices from property features.  
  - Forecast sales based on advertising spend.  
  - Estimate customer lifetime value.  
- **Scientific examples:**  
  - Predict blood pressure from medical measurements.  
  - Model energy consumption from weather and usage patterns.  

::: {.notes}
Regression problems are everywhere. In business, regression models predict housing prices, forecast sales, or estimate customer lifetime value. In science, regression predicts outcomes like blood pressure or models energy consumption. These examples show how broadly regression applies.  
:::


## Key questions regression helps us answer

1. **Is there a relationship?**  
   - Do the predictors explain variation in \(y\) beyond noise?

2. **How strong is the relationship?**  
   - Effect sizes; variance explained (e.g., \(R^2\), RSE).

3. **Which predictors matter uniquely?**  
   - Conditional effects “holding others fixed”; avoid misattribution.

4. **How big are the effects (with uncertainty)?**  
   - Coefficients + **CIs/p-values**; practical vs. statistical significance.

continued...


::: {.notes}

::: {.notes}
### Slide:: Key questions regression helps us answer

### Detailed Notes  
- Frame this slide as shifting from **prediction alone** to **understanding relationships**—the inferential side of regression analysis.  
- Emphasize that regression is not only about forecasting values; it helps us articulate and test structured questions about how variables relate.  
- Walk through each question slowly to reinforce the conceptual progression:  
  1. **Existence:** Is there any signal at all, beyond noise? This guards against overinterpreting spurious patterns.  
  2. **Strength:** How large is the relationship, and how much variance does the model explain? This is where \(R^2\) and RSE become useful.  
  3. **Uniqueness:** Which predictors contribute **after adjusting for others**? This introduces the idea of conditional effects and multicollinearity.  
  4. **Magnitude + Uncertainty:** How big are the effects, and how confident are we? Students should understand that point estimates alone are insufficient without interval estimates or hypothesis tests.  
- Pedagogically, emphasize that these questions are the *foundation* for regression interpretation and will recur throughout the course.  
- Encourage students to see regression not as a black-box tool but as a structured framework for reasoning about data.

### Deeper Dive  
Regression formalizes questions about relationships by estimating parameters that describe how changes in predictors relate to changes in an outcome. Each question on this slide connects to a core statistical concept:

1. **Is there a relationship?**  
   This is the domain of global and individual hypothesis tests (e.g., F-tests, t-tests). The goal is to determine whether variation in predictors systematically correlates with variation in \(y\) beyond random noise.

2. **How strong is the relationship?**  
   Metrics such as \(R^2\) quantify the proportion of variance explained, while the Residual Standard Error (RSE) expresses typical prediction error in original units. Strength can refer to both **practical** and **statistical** magnitude.

3. **Which predictors matter uniquely?**  
   Multiple regression distinguishes between marginal and conditional effects. That is, an individual variable’s predictive value must be understood *holding other variables constant*. This is essential when predictors are correlated—otherwise effects may be misattributed.

4. **How big are the effects (with uncertainty)?**  
   Estimated coefficients represent average changes in the outcome per unit change in predictors. Confidence intervals and p-values quantify uncertainty around these estimates, helping distinguish meaningful effects from noise. This also sets the stage for discussions about statistical vs. practical significance.

Together, these questions illustrate that regression is both a predictive and inferential tool. It provides a principled way to quantify relationships, evaluate uncertainty, and avoid common analytical pitfalls such as spurious correlation, omitted variable bias, and overinterpretation of point estimates.
:::


:::


## Key questions regression helps us answer

continued...

5. **How accurate are predictions?**  
   - **Confidence intervals** for the mean vs. **prediction intervals** for individuals.

6. **Is a linear model appropriate?**  
   - Residual diagnostics; consider **polynomials/interactions** if needed.

7. **Are there interactions or synergies?**  
   - Do effects of one predictor depend on the level of another?


::: {.notes}

::: {.notes}
### Slide:: Key questions regression helps us answer (continued)

### Detailed Notes  
- Position this slide as the **second half** of the regression reasoning framework. The earlier slide focused on establishing and quantifying relationships; this slide turns to model adequacy and predictive uncertainty.  
- Walk through each question deliberately, explaining why it matters operationally:

5. **How accurate are predictions?**  
   - Emphasize the distinction between **confidence intervals (CIs)** and **prediction intervals (PIs)**.  
   - CIs quantify uncertainty around the *mean response*, while PIs quantify uncertainty around an *individual prediction*.  
   - Pedagogically, stress that PIs are always wider and more appropriate for most real-world prediction tasks.

6. **Is a linear model appropriate?**  
   - Reinforce that regression assumptions must be checked, not assumed.  
   - Residual diagnostics (plots, patterns, variance checks) indicate whether linearity holds or whether transformations or interaction terms may be necessary.  
   - This helps students see diagnostics as a tool for model improvement, not as an afterthought.

7. **Are there interactions or synergies?**  
   - Reconnect to earlier concepts about interaction terms.  
   - Encourage students to think in terms of **effect modification**: the effect of one predictor may depend on the level of another.  
   - Explain that testing for interactions is both a statistical and a substantive question—domain knowledge often motivates these terms.

- Pedagogically, encourage students to treat these seven questions as a *mental checklist* for interpreting any regression output they encounter.

### Deeper Dive  
Regression provides not only estimates of relationships but a framework for evaluating prediction accuracy and model form.

**5. Predictive accuracy and uncertainty**  
Confidence intervals estimate where the *mean* of \(y\) lies for a given set of predictors:
\[
\hat{y} \pm 2 \cdot SE(\hat{y})
\]
Prediction intervals incorporate both model uncertainty and irreducible noise:
\[
\hat{y} \pm 2 \cdot SE_{\text{pred}}
\]
PIs are therefore substantially wider. This distinction is critical: CIs answer inferential questions about the mean, while PIs answer operational questions about individual outcomes.

**6. Assessing linear model adequacy**  
Linear regression assumes linearity in parameters, homoscedastic residuals, and reasonably normal errors. Residual plots reveal violations such as curvature (nonlinearity), funnel shapes (heteroscedasticity), and clusters (omitted structure).  
When linear assumptions fail, options include polynomial terms, interactions, or entirely different model classes.

**7. Interactions and effect modification**  
Regression’s additive structure can miss important joint effects. Interaction terms allow slopes to vary across levels of other predictors:
\[
y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 (X_1 X_2) + \epsilon
\]
A significant \(\beta_3\) indicates synergy or moderation. These terms often arise naturally in business and social science contexts—marketing channels, demographic subgroups, environmental factors, etc.

Together, these questions complete the interpretive toolkit for regression. They move students from simply reading outputs to critically evaluating whether the model is appropriate, how confident we are, and how complex relationships might manifest in real data.
:::


:::


## The supervised learning workflow follows a simple and standard process

1. Collect a labeled dataset (features + target).  
2. Split into train and test sets.  
3. Train a model on training data.  
4. Evaluate model performance on test data.  
5. Use the trained model to predict new cases.  

::: {.notes}

### Slide:: The supervised learning workflow follows a simple and standard process

### Detailed Notes  
- Treat this slide as the **operational blueprint** for supervised learning. Students should internalize this sequence as the default workflow they follow in every project.  
- Emphasize that each step builds logically on the previous one; skipping or reordering steps often leads to subtle modeling failures.  
- Highlight the importance of starting with a **labeled dataset**: without labels, supervised learning cannot occur. Reinforce that label quality constrains model quality.  
- Stress the significance of the **train/test split** as an integrity step, not clerical overhead. Evaluation depends on keeping test data untouched.  
- When discussing model training, remind students that this is where optimization happens—but it is only one part of the workflow, not the whole story.  
- Clarify that evaluation on the test set is about **generalization**, not about achieving the best possible number.  
- Pedagogically, encourage students to see prediction on new cases as the *purpose* of supervised learning—everything leading up to that is infrastructure to make predictions reliable.

### Deeper Dive  
This workflow operationalizes the key principles of supervised learning in a reproducible sequence. The process begins with assembling a dataset where each observation includes both predictors \(X\) and an associated label \(y\). The quality, representativeness, and stability of these labels determine the learning signal available to any model.

The train/test split enforces the distinction between **empirical risk** (performance on data the model has seen) and **expected risk** (performance on new data). Without this separation, the model may appear to perform well simply by overfitting the training data.

Model training involves selecting a hypothesis class and optimizing its parameters to minimize loss on the training set. This step implicitly assumes that the training distribution approximates the future distribution—a key assumption that will be revisited when we discuss monitoring and drift.

Evaluation on the test set provides an unbiased estimate of generalization. It is crucial that no modeling decisions—feature engineering, hyperparameter tuning, or algorithm selection—are informed by test data.

Finally, prediction on new cases represents the **deployment phase** of supervised learning. The trained model is applied to previously unseen inputs to generate outputs that support decisions. This step introduces operational considerations such as latency, interpretability, and robustness.

Understanding this workflow is essential because it reveals that supervised learning is not about “running a model”—it is about orchestrating a pipeline that protects against overfitting, leakage, and misinterpretation. Mastery of the workflow forms the backbone of responsible and effective machine learning practice.
:::



## Regression can be seen as function approximation


- Goal: Learn the underlying relationship between inputs and outputs.  
- Conceptual model:  
  $$
  y = f(X) + \epsilon
  $$  
  - f(X): systematic relationship or signal.  
  - $\epsilon$: irreducible error/noise.  
- The task is to approximate f while generalizing to new data.  


::: {.notes}

### Slide:: Regression can be seen as function approximation

### Detailed Notes  
- Use this slide to position regression within a much broader conceptual frame: regression is not just about lines or coefficients—it is about **learning an unknown function** from data.  
- Emphasize that this framing applies to **all** regression models, from simple linear regression to deep neural networks.  
- Walk through the decomposition \(y = f(X) + \epsilon\) carefully:  
  - \(f(X)\) represents the systematic part of the relationship—the structure we hope to learn.  
  - \(\epsilon\) represents noise, randomness, measurement error, or unobserved factors we cannot model.  
- Stress that our job is not to recover the true \(f\) perfectly (which is usually impossible) but to find a good **approximation** that generalizes well.  
- Pedagogically, connect this back to overfitting: if a model is too flexible, it may approximate not only \(f\) but also the noise term \(\epsilon\), destroying generalization.

### Deeper Dive  
Viewing regression as function approximation highlights the core objective of supervised learning: estimate an unknown mapping from inputs to outputs. In statistical learning theory, we assume that outcomes follow:
\[
y = f(X) + \epsilon
\]
where \(f\) is the true function governing the relationship and \(\epsilon\) captures irreducible noise, including randomness and unmeasured influences.

This framing has several implications:

- **Irreducible error:** Even a perfect estimate of \(f\) cannot eliminate \(\epsilon\). This sets a theoretical lower bound on predictive accuracy.  
- **Bias–variance tradeoff:** Simpler models may underfit—failing to capture the complexity of \(f\)—while overly flexible models risk fitting the noise in \(\epsilon\).  
- **Model comparison:** Different models impose different assumptions about the form of \(f\): linearity, smoothness, interactions, monotonicity, or deep hierarchical structure.  
- **Generalization:** The measure of a good approximation is not training error but expected prediction error on new samples, capturing how well \(\hat{f}\) tracks the underlying structure rather than sample-specific noise.

This conceptual view unifies the material to come—from linear regression to regularization, tree-based models, and neural networks. Each method represents a different strategy for approximating \(f(X)\) while balancing flexibility and stability. Understanding regression at this level helps students transfer intuition across model classes rather than treating each algorithm as a disconnected technique.
:::
 


# Linear Regression  

## Ordinary Least Squares regression provides a simple and interpretable baseline

**Idea:** Fit a straight line (or hyperplane) that minimizes squared errors.  

- Simple regression equation:  

  $$
  y = \beta_0 + \beta_1 x + \epsilon
  $$  
  - $\beta_0$: intercept.  
  - $\beta_1$: slope.  
  - $\epsilon$: error term.  
- OLS chooses coefficients to minimize:  

  $$
  \text{SSE} = \sum (y_i - \hat{y}_i)^2
  $$  
  
::: {.notes}

### Slide:: Ordinary Least Squares regression provides a simple and interpretable baseline

### Detailed Notes  
- Use this slide to introduce OLS as the **baseline method** against which more complex models will be compared. Emphasize that its value lies not only in performance but in interpretability and diagnostic clarity.  
- Start with the intuition: OLS finds the line (or hyperplane) that best fits the data by minimizing the **sum of squared residuals**. This gives students a concrete mental image before diving deeper.  
- Stress that simplicity is a strength here: with only a few parameters, OLS models are easy to explain, communicate, and troubleshoot.  
- Walk through the components of the equation slowly:  
  - \(\beta_0\) as the baseline prediction when \(x = 0\).  
  - \(\beta_1\) as the average change in \(y\) per unit change in \(x\).  
  - \(\epsilon\) capturing noise and unobserved influences.  
- Pedagogically, emphasize that OLS is often competitive even against more complex models, especially when data is limited or relationships are roughly linear.  
- Set expectations: later slides will introduce diagnostic tools to assess when OLS works well and when more flexible models are needed.

### Deeper Dive  
Ordinary Least Squares (OLS) solves a fundamental optimization problem:
\[
\min_{\beta_0, \beta_1} \sum_{i=1}^n (y_i - \hat{y}_i)^2,
\quad\text{where }\hat{y}_i = \beta_0 + \beta_1 x_i.
\]
This objective penalizes larger errors disproportionately due to squaring, encouraging the model to fit overall trends rather than a small number of extreme deviations.

Several important statistical properties follow from this formulation:

- **Closed-form solution:** For simple and multiple linear regression, OLS yields explicit formulas for coefficients, making computation efficient and interpretation straightforward.  
- **Unbiasedness under assumptions:** Under the classical Gauss–Markov assumptions (linearity in parameters, independent errors, zero-mean errors, homoscedasticity, and no perfect multicollinearity), OLS provides the **Best Linear Unbiased Estimator (BLUE)**.  
- **Interpretability:** Coefficients have clear meanings: each slope describes the expected change in \(y\) for a one-unit change in \(x\), holding all else constant (in multiple regression).  
- **Global optimization:** The squared-error loss surface is convex, so OLS has a single global minimum—unlike many modern ML models, which have complex, multimodal loss landscapes.

Although OLS is powerful, its linear functional form imposes structural limitations. It cannot capture nonlinear relationships or interactions without explicit feature engineering. Squared-error loss also makes OLS sensitive to outliers. These limitations motivate extensions such as polynomial regression, regularization, and more flexible learners.

Understanding OLS thoroughly equips students with the conceptual foundation needed for the rest of the supervised learning toolkit. It serves as both a **benchmark** and an **interpretive anchor** for evaluating more advanced models.
:::






## Population vs. sample regression lines

:::: {.columns}
::: {.column width="60%"}
- **Population regression line:**  
  $$
  Y = \beta_0 + \beta_1 X + \epsilon
  $$  
  - True but unknown relationship in the data-generating process.  

- **Sample regression line:**  
  $$
  \hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X
  $$  
  - Estimated from our finite dataset.  

**Key idea:**  OLS gives the *best linear unbiased estimate* (BLUE).  

:::

::: {.column width="40%"}
<figure>
  <img src="../materials/assets/images/ols__pop_vs_sample.png"
       alt="**Figure:** True line in red, sample regression fits in blue across repeated samples">
  <figcaption>**Figure:** True line in red, sample regression fits in blue across repeated samples</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Population vs. sample regression lines

### Detailed Notes  
- Use this slide to ground students in **the difference between the world and the data we observe**. This distinction is essential for understanding estimation, uncertainty, and inference.  
- Emphasize that the *population regression line* represents the true underlying relationship—what we would recover with infinite data. Students often mistakenly assume the sample line *is* the true line.  
- Clarify that the *sample regression line* is our best attempt to estimate that relationship using the finite data available.  
- Walk through the figure:  
  - The red line represents the true (but unknown) population relationship.  
  - The many blue lines represent what OLS would estimate across repeated samples; each is different due to sampling variability.  
- Pedagogically, highlight the key idea: **any single sample gives a noisy estimate**, but on average, OLS recovers the true line if the assumptions hold.  
- Connect this back to uncertainty measures: standard errors and confidence intervals quantify variability across possible samples.  
- Reinforce that BLUE holds only under specific assumptions; later diagnostics will help assess whether those assumptions are reasonable.

### Deeper Dive  
The population regression function:
\[
Y = \beta_0 + \beta_1 X + \epsilon
\]
represents the true conditional expectation \(E[Y \mid X]\) under the classical linear model. The parameters \(\beta_0, \beta_1\) describe the true linear relationship between \(X\) and \(Y\), while \(\epsilon\) captures irreducible noise.

Given a finite sample, we estimate:
\[
\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X.
\]
Because samples vary, so do the estimates \(\hat{\beta}_0, \hat{\beta}_1\). The distribution of these estimates across repeated samples reflects **sampling variability**.

Under the Gauss–Markov assumptions:
- Linearity in parameters  
- Zero-mean errors  
- Homoscedasticity  
- No autocorrelation  
- Full column rank (no perfect multicollinearity)

OLS estimators are **Best Linear Unbiased Estimators (BLUE)**, meaning:
- **Unbiased:** \(E[\hat{\beta}] = \beta\).  
- **Linear:** estimators are linear functions of the observed \(Y\).  
- **Best:** they have minimum variance among all linear unbiased estimators.

This distinction reveals why inference is necessary: we never observe the population regression line, only its noisy estimates. Confidence intervals, hypothesis tests, and standard errors exist precisely because the sample line varies from one dataset to another.

Understanding population vs. sample lines lays the conceptual foundation for all inferential tools and motivates why diagnostics and assumptions matter in practice.
:::



## OLS finds the line that minimizes squared errors

:::: {.columns}
::: {.column width="50%"}


- Each prediction error = residual: \(e_i = y_i - \hat{y}_i\).  
- OLS chooses coefficients \((\hat{\beta}_0, \hat{\beta}_1)\) to minimize:  
  $$
  RSS = \sum_{i=1}^n e_i^2
  $$
- **Intuition:** Different slopes and intercepts produce different RSS values; OLS picks the combination that yields the smallest RSS.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/ols__residuals.png"
       alt="**Figure:** Scatterplot with residuals shown as vertical lines">
  <figcaption>**Figure:** Scatterplot with residuals shown as vertical lines</figcaption>
</figure>

:::
::::

::: {.notes}

### Slide:: OLS finds the line that minimizes squared errors

### Detailed Notes  
- Frame this slide as the **mechanical heart** of linear regression: everything OLS does can be traced back to minimizing the sum of squared residuals.  
- Start with the definition of a residual: the vertical distance between an observed point and the model’s predicted value. Emphasize that residuals capture what the model “got wrong.”  
- Use the figure to illustrate residuals visually—students often understand the geometry before the algebra.  
- Walk through the intuition slowly:  
  - Each possible line has its own set of residuals.  
  - Squaring them makes larger errors count more.  
  - Summing them gives a single score (RSS) for how well that line fits the data.  
  - OLS selects the slope and intercept with the **lowest RSS**.  
- Pedagogically, highlight that this is a **global optimization problem** with a closed-form solution—not trial and error.  
- Connect this idea forward: minimizing squared error is not arbitrary; it leads directly to desirable statistical properties (BLUE) and connects regression to other ML algorithms that optimize similar loss functions.

### Deeper Dive  
Ordinary Least Squares solves the optimization problem:
\[
\min_{\beta_0, \beta_1} \sum_{i=1}^n (y_i - \hat{y}_i)^2,
\quad \text{with } \hat{y}_i = \beta_0 + \beta_1 x_i.
\]

Several important insights flow from this:

1. **Squared error emphasizes large deviations.**  
   Squaring residuals causes large errors to dominate the objective. This makes OLS sensitive to outliers but also encourages the model to fit central trends tightly.

2. **Convexity ensures a unique solution.**  
   The squared-error loss surface is convex in \(\beta_0\) and \(\beta_1\), meaning there is a single, global minimum. This stands in contrast to many modern machine learning models with complex, non-convex landscapes.

3. **Closed-form solutions exist.**  
   In simple and multiple regression, OLS estimates can be computed directly using linear algebra:
   \[
   \hat{\beta} = (X^\top X)^{-1} X^\top y.
   \]
   This efficiency and mathematical transparency make OLS a foundational method.

4. **Connection to statistical assumptions.**  
   OLS minimizes the sum of squared residuals regardless of distributional assumptions, but under the Gauss–Markov conditions it also yields the **Best Linear Unbiased Estimator (BLUE)**. If errors are normally distributed, OLS is also the maximum-likelihood estimator.

Understanding OLS as squared-error minimization provides a conceptual bridge to broader machine learning. Many algorithms—ridge regression, lasso, neural networks—optimize variations of the same objective but add penalties, constraints, or nonlinear transformations. OLS is thus both a practical tool and a conceptual anchor for the rest of the course.
:::
 



## Linear regression relies on several core assumptions

- **Linearity:** Predictors relate linearly to the outcome (in expectation).  
- **Homoscedasticity:** Residuals have constant variance across levels of predictors.  
- **Normality of residuals:** Errors are approximately normal (important for inference).  
- **Independence:** Observations and errors are independent of one another.  
- **Full rank:** No perfect multicollinearity among predictors.  
- *Note:* Violations primarily affect inference; predictive accuracy may still be reasonable.  
 

::: {.notes}
### Slide:: Linear regression relies on several core assumptions

### Detailed Notes  
- Present this slide as a **diagnostic checklist**, not a barrier to using regression. These assumptions tell us when inference is trustworthy and when caution is required.  
- Start by clarifying that assumptions refer to the **data-generating process**, not the model’s algebraic form. Students often misinterpret these as constraints on observed data.  
- Walk through each assumption with intuition:  
  - **Linearity:** The average effect of predictors is straight-line; slight curvature can often be handled with transformations.  
  - **Homoscedasticity:** Residuals should spread evenly; funnel shapes indicate problems.  
  - **Normality:** Mainly affects p-values and confidence intervals, not prediction.  
  - **Independence:** Repeated measures, clustered data, or time series violate this; new methods are needed.  
  - **Full rank:** Predictors cannot be perfectly redundant; multicollinearity complicates interpretation.  
- Pedagogically, emphasize that violations are common but manageable. Diagnostics, transformations, and robust methods are designed to help.  
- Reinforce the note: prediction may still be strong even when assumptions fail—but inference, interpretation, and uncertainty estimates become unreliable.

### Deeper Dive  
Classical linear regression is built on assumptions that ensure desirable statistical properties for estimators and valid inference.

1. **Linearity in expectation**  
   Regression assumes \(E[Y \mid X] = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p\). This captures average effects, not exact fits. It allows for noise and individual variation while focusing on systematic structure.

2. **Homoscedasticity**  
   Constant error variance ensures efficient estimation. When variance increases or decreases with predictors (heteroscedasticity), OLS coefficients remain unbiased, but standard errors become unreliable. Robust SEs or transformations can address this.

3. **Normality of residuals**  
   Normality justifies t-tests and confidence intervals, especially in small samples. With large samples, the Central Limit Theorem reduces dependence on this assumption.

4. **Independence**  
   Independence prevents hidden structure in errors. Violations occur in time series, spatial data, hierarchical data, and repeated-measures designs. Remedies include clustered SEs, mixed models, and time-series methods.

5. **Full rank (no perfect multicollinearity)**  
   Perfectly correlated predictors make \((X^\top X)^{-1}\) non-invertible. Near-multicollinearity inflates variances of estimated coefficients, complicating interpretation but not prediction.

Understanding assumptions allows analysts to diagnose model issues, justify methodological choices, and select appropriate remedies. Assumptions are not rigid rules but **guiding conditions** that help determine how much trust to place in estimates and interpretations.
:::
 




## Coefficients and intercepts have clear interpretations

- **Intercept:** Predicted value of \(y\) when all features are 0 (within the range where this is meaningful).  
- **Coefficients:** Expected change in \(y\) for a one-unit change in a predictor, holding other predictors constant.  
- Example: Coefficient on “size” = 250 → each additional square foot increases predicted price by \$250.  


::: {.notes}
### Slide:: Coefficients and intercepts have clear interpretations

### Detailed Notes  
- Use this slide to anchor students in one of regression’s greatest strengths: **interpretability**. Linear models provide parameters that map directly to statements about expected changes in outcomes.  
- Begin with the intercept but emphasize caution: it represents the model’s predicted value when all predictors equal zero. This is often not a realistic or observed scenario.  
- Explain that coefficients quantify **marginal effects**: how the predicted outcome shifts for a one-unit increase in a predictor, holding all other predictors fixed.  
- Stress the phrase “holding others constant”—students frequently overlook that multiple regression isolates unique contributions by adjusting for correlated variables.  
- Use the house-price example to ground intuition in a tangible context.  
- Pedagogically, remind students that regression captures **association**, not causation. A large coefficient does not imply a variable is a causal driver.  
- Set expectations: interpretation becomes more complex with interactions, nonlinear terms, or transformations, which will be covered shortly.

### Deeper Dive  
In the linear regression model
\[
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \dots + \hat{\beta}_p x_p,
\]
coefficients describe the expected change in the response given a one-unit increase in a predictor, controlling for all other predictors. This lies at the heart of regression’s explanatory framework.

**Intercept (\(\hat{\beta}_0\)).**  
The intercept represents the model’s prediction when all predictors equal zero. In practice, its interpretability depends on whether 0 lies within the observed data range. When it does not, the intercept serves primarily as a mathematical anchor.

**Slope coefficients (\(\hat{\beta}_j\)).**  
Each slope reflects a **partial derivative** of the regression function:
\[
\frac{\partial \hat{y}}{\partial x_j} = \hat{\beta}_j.
\]
This expresses how the model expects the outcome to change as one predictor varies, assuming all others remain fixed. This conditional interpretation distinguishes multiple regression from simple bivariate comparisons.

**Interpretation caveats.**  
- Multicollinearity inflates uncertainty around coefficients and can make individual interpretations unstable.  
- Transformations (log, polynomial) change the meaning of coefficients; interpretation must adapt accordingly.  
- Regression estimates statistical associations. Causal interpretations require additional assumptions, designs, or experiments.

Clear coefficient interpretation is foundational for inference, diagnostics, and communication of model results. It also sets the stage for understanding more complex models where interpretability becomes less direct.
:::




## Regression inference asks: does this predictor really matter?

- Coefficients are estimates, not exact truths.  
- We need tools to measure **uncertainty**:  
  - Standard errors.  
  - Confidence intervals.  
  - Hypothesis tests.  
- These tools help distinguish real effects from random noise.  

::: {.notes}
### Slide:: Regression inference asks: does this predictor really matter?

### Detailed Notes  
- Use this slide to shift students from **estimation** to **inference**. Up to this point, they have learned how to compute coefficients; now they must consider how much trust to place in them.  
- Emphasize the opening point: coefficients are *estimates*, not truths. Sampling variability means the estimated effect will differ from sample to sample.  
- Explain why uncertainty quantification is essential: without it, we cannot tell whether an observed effect is likely to be real or simply a product of noise.  
- Walk through each inferential tool:  
  - **Standard errors** measure how much an estimate would vary if we repeatedly sampled new datasets.  
  - **Confidence intervals** provide a plausible range for the true effect; students often find these easier to interpret than p-values.  
  - **Hypothesis tests** formalize whether an effect is distinguishable from zero (or another reference value).  
- Pedagogically, draw a contrast with prediction: predicting well does not guarantee we understand the underlying relationships, and interpreting coefficients without uncertainty can lead to incorrect conclusions.  
- Reinforce that inference is especially important in business, economics, and social science settings where decisions depend on understanding **drivers**, not just predictions.

### Deeper Dive  
Regression inference is built on the idea that each coefficient \(\hat{\beta}_j\) is a **random variable** shaped by sampling variability. Even if the true effect \(\beta_j\) is zero, sampling fluctuations can produce nonzero estimates.

The core tools for inference are:

**1. Standard Errors (SEs).**  
The standard error of \(\hat{\beta}_j\) estimates the spread of its sampling distribution. Large SEs indicate high uncertainty, often due to small sample size or multicollinearity.

**2. Confidence Intervals (CIs).**  
A 95% CI has the form:
\[
\hat{\beta}_j \pm 2 \cdot SE(\hat{\beta}_j).
\]
It represents a range of values consistent with the observed data under repeated sampling. Importantly, the CI communicates both magnitude and uncertainty, making it a useful tool for substantive interpretation.

**3. Hypothesis Tests.**  
The standard null hypothesis is \(H_0: \beta_j = 0\).  
The t-statistic compares the estimate to its standard error:
\[
t = \frac{\hat{\beta}_j}{SE(\hat{\beta}_j)}.
\]
A large magnitude t-value implies the effect is unlikely due to random noise. P-values quantify this likelihood.

These inferential tools help distinguish **signal** from **noise**, **practical importance** from **statistical flukes**, and **robust effects** from those driven by random variability.

Finally, stress the conceptual distinction:  
- **Prediction asks:** “How well can we forecast outcomes?”  
- **Inference asks:** “Which variables matter, and how confident are we?”  

Both perspectives are valuable, but they answer fundamentally different questions.  
:::




## Hypothesis tests check if coefficients differ from zero

- Null hypothesis: $H_0: \beta_j = 0$.  
- Alternative: $H_a: \beta_j \neq 0$.  
- Test statistic:  
  $$
  t = \frac{\hat{\beta}_j}{SE(\hat{\beta}_j)}
  $$  
- Small p-value → strong evidence that predictor matters.  

::: {.notes}
### Slide:: Hypothesis tests check if coefficients differ from zero

### Detailed Notes  
- Introduce this slide as the **formal mechanism** for answering, “Does this predictor matter?” in a regression model.  
- Emphasize that hypothesis testing provides a structured way to evaluate whether an estimated coefficient is distinguishable from zero after accounting for sampling variability.  
- Walk through the hypotheses slowly:  
  - \(H_0: \beta_j = 0\) means the predictor has no linear association with the outcome, after controlling for other variables.  
  - \(H_a: \beta_j \neq 0\) means there is evidence of a nonzero effect.  
- Explain that the t-statistic measures how large the estimate is relative to its standard error—the “signal-to-noise” ratio.  
- Pedagogically, caution students that p-values reflect evidence **under a model**, not certainty or causal truth.  
- Reinforce that statistical significance does not imply practical significance; a tiny but statistically significant effect may have little business relevance.  

### Deeper Dive  
In multiple regression, each coefficient \(\hat{\beta}_j\) is an estimate of a true parameter \(\beta_j\). Hypothesis testing assesses whether the observed estimate is consistent with random noise or indicates a meaningful effect.

The t-statistic:
\[
t = \frac{\hat{\beta}_j}{SE(\hat{\beta}_j)}
\]
compares the size of the estimated effect to the uncertainty around it. Large absolute values of \(t\) indicate that the estimate is far from zero relative to its variability.

Under the classical assumptions of the linear model, and assuming the null hypothesis is true, the t-statistic follows a t-distribution with \(n - p - 1\) degrees of freedom. The corresponding p-value quantifies how surprising the observed \(t\)-value would be if the true effect were zero.

Key considerations:

- **Significance vs. magnitude:**  
  A statistically significant coefficient can be too small to matter in practice. Inference must be paired with context.

- **Multiple comparisons:**  
  Testing many coefficients inflates false-positive risk—important in high-dimensional settings.

- **Model dependence:**  
  Hypothesis tests evaluate a coefficient *conditional on the other predictors in the model*. Adding or removing predictors can change conclusions, especially with correlated variables.

- **Non-causality:**  
  Rejecting \(H_0\) does not imply a causal effect. It indicates evidence of association under the model’s assumptions.

Ultimately, hypothesis testing is a tool for quantifying uncertainty and distinguishing likely signal from noise—not a substitute for domain knowledge or thoughtful model design.
:::





## Confidence intervals show a plausible range for effects

:::: {.columns}
::: {.column width="50%"}


- 95% confidence interval:  
  $$
  \hat{\beta}_j \pm 2 \times SE(\hat{\beta}_j)
  $$
- Interpretation (frequentist): if we repeated the study many times, 95% of constructed intervals would contain the true effect.  
- Example: β(size) = 250, SE = 25 → CI = [200, 300].  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/infer__coef_ci_bars.png"
       alt="**Figure:** Bar with coefficient estimate and error band">
  <figcaption>**Figure:** Bar with coefficient estimate and error band</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Confidence intervals show a plausible range for effects

### Detailed Notes  
- Use this slide to emphasize that **point estimates alone are insufficient**; uncertainty is integral to any interpretation of regression results.  
- Frame confidence intervals (CIs) as a way to express the *precision* of an estimated coefficient. A narrow interval indicates high precision; a wide interval signals substantial uncertainty.  
- Clarify the interpretation carefully: a CI provides a **range of plausible effect sizes** given the observed data and model assumptions.  
- Point out that many stakeholders find CIs easier to understand than p-values because they communicate both magnitude and uncertainty directly.  
- Highlight that CIs support better decision-making by showing what effect sizes are consistent with the data—not merely whether the effect is nonzero.  
- Pedagogically, remind students that CIs depend on model assumptions; violations (e.g., heteroscedasticity) can make intervals too narrow or too wide.

### Deeper Dive  
A 95% confidence interval for a coefficient \(\beta_j\) takes the form:
\[
\hat{\beta}_j \pm 2 \cdot SE(\hat{\beta}_j),
\]
where the multiplier 2 is an approximation to the appropriate critical value from the t-distribution.

**Interpretation (frequentist):**  
If we were to repeat the sampling and model-fitting process many times, and compute a 95% interval each time, approximately 95% of those intervals would contain the true coefficient. Importantly, this does *not* mean there is a 95% probability the true parameter lies in a specific interval. Rather, the method has 95% long-run coverage.

**Why CIs matter:**
- They show **direction**, **magnitude**, and **uncertainty** simultaneously.  
- They allow assessment of **practical significance**, not just statistical significance.  
- They reveal instability: a coefficient that is large but highly uncertain signals that more data or a different model may be needed.

**Connection to hypothesis testing:**  
If a CI does not include zero, it corresponds to rejecting \(H_0: \beta_j = 0\) at approximately the 5% level. But the CI gives more context by showing *how large* the effect might plausibly be.

Finally, stress that CIs are only as good as the underlying model assumptions. Violations such as heteroscedasticity or autocorrelation affect standard errors and therefore the width of intervals—motivating later discussions of robust inference and diagnostics.
:::


# Regression Diagnostics

## Visualization of predictions and residuals helps diagnose fit

:::: {.columns}
::: {.column width="50%"}


- **Prediction plots:** Compare true vs. predicted values to assess overall fit.  
- **Residual plots:** Plot residuals vs. predictions (or vs. predictors); randomness suggests a good fit.  
- **Histogram or density of residuals:** Assesses approximate normality for inference.  
- Example: A funnel-shaped residual plot suggests heteroscedasticity.  

:::

::: {.column width="50%"}

<figure>
  <img src="../materials/assets/images/diag__pred_and_hist.png"
       alt="**Figure:** Scatter and residual plots for regression model">
  <figcaption>**Figure:** Scatter and residual plots for regression model</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Visualization of predictions and residuals helps diagnose fit

### Detailed Notes  
- Frame this slide as teaching students **how to look inside a model**, not just how to evaluate it with metrics. Diagnostics are the visual grammar of regression analysis.  
- Emphasize that metrics like RMSE and R² summarize performance, but **visuals reveal structure**—patterns, violations of assumptions, and opportunities for improvement.  
- Walk through each diagnostic:  
  - **Prediction plot:** True vs. predicted values should cluster around the 45° line. Systematic deviations signal bias or model misspecification.  
  - **Residual plot:** Residuals should look like random noise—no curves, no trends, no funnels. Patterns indicate nonlinearity, heteroscedasticity, or omitted variables.  
  - **Histogram (or density):** Helps assess whether residuals are approximately normal, which matters for inference but not strictly for prediction.  
- Use the funnel example to illustrate heteroscedasticity: residual spread increasing with fitted values. Explain why this matters for standard errors and inference.  
- Pedagogically, encourage students to treat diagnostic plots as *routine checks*, not optional extras.  
- Reinforce that good diagnostics often uncover ways to improve models—adding polynomial terms, transforming variables, or changing model classes.

### Deeper Dive  
Residual diagnostics stem from analyzing:
\[
e_i = y_i - \hat{y}_i,
\]
the parts of the outcome the model cannot explain. Patterns in residuals reveal mismatches between the model and the underlying data-generating process.

**Prediction plots** assess calibration. Deviations from the 45° line indicate systematic under- or over-prediction. Clustering or curvature in this plot often signals that linearity assumptions are violated.

**Residual vs. fitted plots** test three core assumptions simultaneously:
- **Linearity:** Curves or waves in the plot suggest the relationship is not linear.  
- **Homoscedasticity:** A funnel shape indicates non-constant variance.  
- **Independence:** Blocks, clusters, or runs may indicate correlated errors.

**Residual distributions** support inference by assessing approximate normality. Regression does not require normally distributed predictors or outcomes, but many inferential procedures rely on residual normality (or large-sample approximations to it).

Diagnostics also help detect:
- **Outliers:** Points with unusually large residuals.  
- **High leverage points:** Observations with extreme predictor values.  
- **Model misspecification:** Omitted variables or incorrect functional forms.

The deeper lesson is that diagnostics are not about policing assumptions—they are about **understanding how and why the model succeeds or fails**. Strong predictive modeling depends not just on training algorithms but on reading these visual cues and refining the model accordingly.
:::


## Diagnostics reveal hidden problems in regression models

- Assumptions often fail in practice.  
- Key issues:  
  - Outliers.  
  - High leverage points.  
  - Multicollinearity.  
  - Heteroscedasticity or nonlinearity.  
- Without diagnostics, results can be misleading.  

::: {.notes}
### Slide:: Diagnostics reveal hidden problems in regression models

### Detailed Notes  
- Frame this slide as showing students **why diagnostics matter**. Even if a model fits numerically, underlying issues may invalidate interpretations or undermine generalization.  
- Emphasize that regression assumptions do not fail dramatically—they fail **quietly**, producing misleading estimates and overconfident inference unless we check for problems.  
- Walk through each issue at a high level:
  - **Outliers:** Points with unusually large residuals can distort coefficients or inflate error estimates.  
  - **High leverage points:** Observations with extreme predictor values can disproportionately pull the regression line.  
  - **Multicollinearity:** Strong correlations among predictors inflate standard errors, making coefficients unstable or misleading.  
  - **Heteroscedasticity / Nonlinearity:** Violations of constant variance or linearity suggest that the functional form is misspecified.  
- Pedagogically, stress that these problems often occur **together** and may not be obvious from metrics alone.  
- Reinforce that diagnostics are not optional; they protect analysts from drawing incorrect conclusions and help guide model refinement.

### Deeper Dive  
Regression diagnostics reveal how well the model’s assumptions align with the observed data. When assumptions fail, the consequences vary:

**Outliers**  
Observations with large residuals can inflate RSS, distort slope estimates, and produce misleading inference. They often signal data entry errors, shifting regimes, or unmodeled structure.

**High leverage points**  
These are observations with extreme predictor values. Even if their residuals are small, they can anchor or rotate the regression line. Combined with large residuals, they become highly influential.

**Multicollinearity**  
High correlation among predictors undermines coefficient stability. Standard errors increase, confidence intervals widen, and sign flips may occur. While prediction may remain strong, interpretation becomes unreliable.

**Heteroscedasticity**  
Non-constant variance of residuals violates OLS efficiency. Standard errors become biased, which undermines hypothesis tests and confidence intervals. Robust SEs or transformations are often appropriate remedies.

**Nonlinearity**  
If the relationship between predictors and the outcome is nonlinear, linear models systematically misestimate effects. Residual plots reveal curvature that suggests adding polynomial terms, splines, or switching to more flexible models.

The overarching theme is that regression diagnostics provide a **map of model health**. They show where the model is aligned with the data and where it is failing. Without them, analysts risk false confidence, spurious conclusions, and decisions based on artifacts rather than signal.

Diagnostics shift us from “Does the model run?” to “Is the model valid, trustworthy, and appropriate for the question?”  
:::




## Outliers and leverage points can distort results

- **Outliers:** unusual response values → large residuals.  
- **High leverage:** unusual predictor values → strong influence on regression line.  
- Dangerous combination: high residual + high leverage.  
- Tools:  
  - Studentized residuals.  
  - Leverage statistic.  

Check Cook’s Distance as a combined influence measure; large values flag points with high leverage × large residual.


::: {.notes}
### Slide:: Outliers and leverage points can distort results

### Detailed Notes  
- Frame this slide as a **critical distinction** in regression diagnostics: not all unusual points are equally problematic, and understanding the difference clarifies how influence arises.  
- Emphasize that outliers and leverage points operate on **different axes**:  
  - Outliers are extreme in the **outcome space** (Y).  
  - Leverage points are extreme in the **predictor space** (X).  
- Explain that leverage alone does not necessarily distort the line; a high-leverage point that lies cleanly on the trend may not be harmful.  
- Conversely, an outlier with low leverage affects the line only modestly.  
- The dangerous cases are those with **both** unusual predictors and unusual responses—these points can “tilt” or “anchor” the regression line.  
- Use the listed tools to structure your teaching:  
  - **Studentized residuals** help identify outliers while accounting for estimated variance.  
  - **Leverage statistics** show whether a point sits far out in predictor space.  
  - **Cook’s Distance** combines the two, providing a holistic measure of influence on estimates.  
- Pedagogically, remind students that influential points are not automatically errors; they may represent meaningful subgroups or important phenomena.

### Deeper Dive  
Influential observations can meaningfully alter regression estimates, making influence diagnostics essential for trustworthy inference.

**Outliers**  
Outliers have residuals that are large relative to model predictions:
\[
e_i = y_i - \hat{y}_i.
\]
Studentized residuals standardize these deviations by their estimated variance, making it easier to identify unusually large errors in a statistically coherent way. Points with large studentized residuals may indicate data entry problems, unmodeled structure, or natural but rare events.

**High leverage points**  
Leverage reflects how far an observation’s predictor vector \(x_i\) lies from the mean of the predictors. The diagonal of the hat matrix \(H\):
\[
h_{ii}
\]
quantifies leverage. High-leverage points have the capacity to shift the fitted line significantly because predictions for those points rely heavily on their own observed values.

**Influential points and Cook’s Distance**  
Cook’s Distance combines leverage and residual size:
\[
D_i = \frac{( \hat{\beta} - \hat{\beta}_{(i)} )^\top X^\top X ( \hat{\beta} - \hat{\beta}_{(i)} )}{p \cdot \hat{\sigma}^2},
\]
where \(\hat{\beta}_{(i)}\) denotes estimates obtained by excluding observation \(i\). Large values of \(D_i\) indicate points that materially change the regression estimates.

Interpreting influence is nuanced:
- Not all influential points should be removed.  
- They may reveal heterogeneity, segmentation, or structural breaks.  
- Sometimes they justify model refinement rather than deletion.

The key lesson: outliers, leverage points, and influential points require **investigation**, not automatic elimination. Diagnostics illuminate how these points interact with the model and guide principled decisions about model specification and data quality.
:::




## Outliers Exampe:

:::: {.columns} 
::: {.column width="80%"} 
<figure>
  <img src="../materials/assets/images/diag__outlier_example.png"
       alt="**Figure:** Outlier example: typical 𝑥, large vertical error. Annotation shows external studentized residual 𝑟∗ and Cook’s 𝐷">
  <figcaption>**Figure:** Outlier example: typical 𝑥, large vertical error. Annotation shows external studentized residual 𝑟∗ and Cook’s 𝐷</figcaption>
</figure>

::: 
::: {.column width="20%"} 

::: 
::::





## High Leverage Example? 

:::: {.columns} 
::: {.column width="80%"} 

<figure>
  <img src="../materials/assets/images/diag__leverage_example.png"
       alt="**Figure:** High-leverage example: extreme 𝑥, small residual. Solid line = OLS (all points); dashed line = OLS without the leverage point. Annotation shows ℎ𝑖𝑖 and Cook’s 𝐷">
  <figcaption>**Figure:** High-leverage example: extreme 𝑥.  Solid line = OLS (all points); dashed line = OLS without the leverage point.</figcaption>
</figure>

::: 
::: {.column width="20%"} 

::: 
::::

::: {.notes}
### Slide:: Outliers Example

### Detailed Notes  
- Use this slide to make the concept of an **outlier in the response variable** concrete. Students often struggle to visualize the difference between an outlier and a high-leverage point.  
- Emphasize that the observation in the figure sits in a *typical region of the predictor space*—its \(x\)-value is not unusual. This means it is **not** a leverage point.  
- However, its \(y\)-value is far from the regression line, producing a **large residual**. This is what makes it an outlier in the response dimension.  
- Point out that because it has typical \(x\), this point does not dramatically shift the regression line’s slope; it mostly increases residual variance and affects inference (e.g., wider confidence intervals, lower significance).  
- Explain the annotations:  
  - **External studentized residual (\(r^\*\))** quantifies how extreme the residual is relative to what the model expects.  
  - **Cook’s Distance (D)** will be moderate here—not as high as for a point with both high residual and high leverage.  
- Pedagogically, reinforce the idea that outliers must be interpreted, not automatically removed. They may reflect data entry errors, unusual events, or important minority patterns.

### Deeper Dive  
This example illustrates the statistical nature of an outlier: a point whose response value \(y_i\) deviates substantially from what the model predicts based on its predictor value \(x_i\). In mathematical terms:
\[
e_i = y_i - \hat{y}_i
\]
is large in magnitude relative to other residuals.

**External studentized residual:**  
This measures how extreme a residual is after removing the observation from the model fit. Large absolute values (often > 2 or 3) indicate potential outliers with respect to \(Y\).

**Cook’s Distance:**  
Cook’s Distance combines information about residual size and leverage. Here, leverage is low because \(x_i\) is typical, so \(D_i\) will not be extreme even though the residual is large. This distinction reinforces that “outlier” and “influential point” are not synonyms.

Consequences of response outliers include:
- Inflation of standard errors  
- Reduced model fit  
- Increased sensitivity of inference  
- Detection of possible data issues or meaningful rare events

The key takeaway:  
Outliers in \(Y\) affect regression **through residual structure**, not through the geometry of predictor space. Proper diagnostics distinguish between these roles and help determine whether corrective action is necessary.
:::



## An outlier with high leverage is a bad combination!

:::: {.columns} 
::: {.column width="80%"} 
<figure>
  <img src="../materials/assets/images/diag_outlier_leverage_combo.png"
       alt="**Figure:** Outlier + high leverage: extreme 𝑥 and large residual. Both lines shown; annotations include 𝑟∗, ℎ𝑖𝑖, 𝐷. Note how one point can 'tilt' the line.">
  <figcaption>**Figure:** Outlier + high leverage: extreme 𝑥 and large residual. Note how one point can "tilt" the line.</figcaption>
</figure>


::: 
::: {.column width="20%"} 

::: 
::::

::: {.notes}
### Slide:: An outlier with high leverage is a bad combination!

### Detailed Notes  
- Frame this slide as the **worst-case scenario** in regression diagnostics. Students need to understand that not all unusual points are harmful, but this combination is uniquely problematic.  
- Emphasize that this point is extreme in **both dimensions**:  
  - It has an **unusual predictor value** (high leverage).  
  - It also has a **large residual** (outlier in the response).  
- Explain what this means practically: because the point sits far out in predictor space, the model must pass a line through or near it to minimize squared error. When the residual is also large, that line gets **tilted** dramatically.  
- Use the visual comparison (line with and without the point) to highlight how much a single observation can distort the entire model. This is an excellent moment to reinforce that regression is sensitive to influential observations.  
- Walk through the diagnostic markers in the figure:  
  - **\(r^\*\)** (studentized residual) flags the observation as an outlier in \(Y\).  
  - **\(h_{ii}\)** (leverage) identifies it as extreme in \(X\).  
  - **Cook’s Distance (D)** combines both to quantify overall influence. Large \(D\) signals a point that *materially changes* the regression fit.  
- Pedagogically, warn students: these points must always be investigated. They may represent data entry errors, meaningful subgroups, regime shifts, or structural breaks—but they cannot be ignored.

### Deeper Dive  
Influence in regression arises when an observation has both **the capacity to affect the fit** (high leverage) and **a strong incentive for the model to adjust toward it** (large residual). In mathematical terms:

- High leverage means the observation dramatically affects estimated coefficients because its predictor value \(x_i\) is far from the mean predictor vector.
- A large residual means the model predicts this point poorly:
  \[
  e_i = y_i - \hat{y}_i
  \]

Cook’s Distance brings these two ideas together:
\[
D_i \propto (e_i^2) \times h_{ii}
\]
A point with high \(e_i\) *or* high \(h_{ii}\) may warrant review.  
A point with both almost always does.

Consequences of such points:
- Regression line rotates or shifts to reduce the residual for that one point.  
- Other predictions worsen to accommodate this distortion.  
- Coefficients, standard errors, and even signs of effects can change dramatically.  
- Inference becomes unreliable, and prediction error can spike.

Importantly, influential points should not be removed automatically. The appropriate response depends on context:
- If it’s a data error → correct/remove it.  
- If it reflects a distinct population → consider subgroup models.  
- If it represents a real but rare scenario → model structure may need refinement.

The broader lesson:  
**One influential point can undo the integrity of an entire regression model. Diagnostics exist so we can detect and reason about these cases before making decisions or drawing conclusions.**
:::



## Multicollinearity makes coefficients unstable

:::: {.columns}
::: {.column width="50%"}


- When predictors are highly correlated → difficult to disentangle unique effects.  
- Coefficients become unstable and standard errors inflate.  
- Variance Inflation Factor (VIF):  
  $$
  VIF(\beta_j) = \frac{1}{1 - R^2_{X_j \mid X_{-j}}}
  $$
- Rule of thumb: VIF > 5 (or 10) may indicate problematic multicollinearity.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/diag__vif_heatmap.png"
       alt="**Figure:** X1–X3 show high pairwise correlations, signaling potential redundancy.">
  <figcaption>**Figure:** High pairwise correlations, signaling potential redundancy.</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Multicollinearity makes coefficients unstable

### Detailed Notes  
- Introduce this slide by emphasizing that multicollinearity is primarily an **interpretability problem**, not a predictive accuracy problem. Students often confuse the two.  
- Explain the intuition: when predictors move together, the model struggles to determine which variable is responsible for the variation in the outcome. This leads to unstable coefficient estimates.  
- Highlight that instability manifests as:  
  - Large swings in coefficients when adding/removing predictors.  
  - Inflated standard errors → wide confidence intervals → insignificant estimates even when variables matter.  
- Use the heatmap to visually show how strongly correlated features cluster together. This helps students connect visual EDA with formal diagnostics.  
- Explain VIF conceptually: it measures how much the variance of a coefficient is inflated because of collinearity with other predictors.  
- Pedagogically, stress that VIF thresholds (5 or 10) are **rules of thumb**, not strict cutoffs. Decision-making should incorporate domain knowledge and model goals.  
- Close by emphasizing mitigation strategies: dropping redundant variables, combining predictors, or using regularization methods like Ridge or Elastic Net.

### Deeper Dive  
Multicollinearity occurs when a predictor \(X_j\) can be expressed as a linear combination of other predictors. In extreme cases (perfect collinearity), OLS cannot compute a unique solution because \((X^\top X)\) becomes non-invertible.

In less extreme but still problematic cases—**near multicollinearity**—several consequences follow:

1. **Coefficient estimates become unstable.**  
   Small perturbations in the data or model specification can produce large changes in coefficient values.

2. **Standard errors inflate.**  
   The variance of \(\hat{\beta}_j\) is proportional to \(\frac{1}{1 - R^2_{X_j \mid X_{-j}}}\).  
   As \(R^2_{X_j \mid X_{-j}}\) approaches 1, variance skyrockets.

3. **Interpretation becomes unreliable.**  
   Coefficients lose meaningful direction or magnitude because the model cannot distinguish overlapping effects.

4. **Prediction remains mostly unaffected.**  
   Because collinear predictors contain similar information, predictions from their combination are often stable. This distinction is essential for applied work.

**Variance Inflation Factor (VIF)** formalizes near-collinearity:
\[
VIF(\beta_j) = \frac{1}{1 - R^2_{X_j \mid X_{-j}}}.
\]
High VIF indicates that predictor \(X_j\) is largely explained by other predictors—leading to inflated uncertainty about its unique effect.

**Mitigation strategies:**
- Remove or merge redundant predictors.  
- Engineer combined features (e.g., sums, differences, indices).  
- Use regularization (Ridge stabilizes collinear coefficients; Elastic Net combines Ridge + Lasso benefits).  
- Centering predictors can help interpret interactions but does not eliminate multicollinearity itself.

The core message:  
Multicollinearity rarely breaks prediction, but it **destroys interpretability** and **inflates inference uncertainty**. Recognizing and addressing it is essential for any regression model intended to support explanation or decision-making.
:::


## Residual Standard Error (RSE): error in original units


- RSE estimates the **typical deviation of observations from the regression line**.  
- Formula:  
  $$
  RSE = \sqrt{\frac{RSS}{n - p - 1}}
  $$
  - RSS = residual sum of squares.  
  - \(n\) = sample size, \(p\) = number of predictors.  
- Expressed in the same units as \(y\).  
- Interpretation: “Typical size of a residual” or “average prediction error.”  


::: {.notes}
### Slide:: Residual Standard Error (RSE): error in original units

### Detailed Notes  
- Present this slide as introducing a **highly interpretable metric**—one that many stakeholders find more intuitive than \(R^2\).  
- Emphasize that RSE represents the **typical size of a residual**, placing model error back on the scale of the original outcome variable.  
- Walk through the formula conceptually: RSS aggregates squared errors, the denominator adjusts for model complexity, and the square root returns the metric to the same units as \(y\).  
- Contrast RSE with \(R^2\):  
  - \(R^2\) is relative and scale-free.  
  - RSE is absolute and conveys real-world magnitude.  
- Pedagogically, caution students: smaller RSE does not mean the model is “good”—it must be interpreted relative to the scale and variability of the outcome.  
- Encourage students to use RSE when communicating model performance to non-technical audiences; it answers the practical question, “How far off are our predictions, on average?”

### Deeper Dive  
RSE formalizes the idea of average error magnitude by computing the square root of the residual variance:
\[
RSE = \sqrt{\frac{1}{n - p - 1} \sum_{i=1}^n (y_i - \hat{y}_i)^2}.
\]
This expression estimates the standard deviation of the error term \(\epsilon\) in the classical linear model. Several implications follow:

1. **Units matter.**  
   Because RSE is expressed in the same units as \(y\), it is directly interpretable. An RSE of 3 means residuals typically deviate from the regression line by roughly 3 outcome units (e.g., dollars, sales points, mm Hg).

2. **Model complexity penalty.**  
   Dividing by \(n - p - 1\) penalizes the use of additional predictors. Adding predictors almost always decreases RSS, but may not meaningfully reduce RSE after adjusting for degrees of freedom.

3. **Complement to \(R^2\).**  
   \(R^2\) captures relative model fit (variance explained), but can mask models with high absolute error if the outcome scale is large. RSE quantifies actual predictive error magnitude.

4. **Connection to inference.**  
   RSE plays a central role in estimating standard errors of coefficients:
   \[
   SE(\hat{\beta}) \propto RSE.
   \]
   Larger residual variation leads to greater uncertainty in effect estimates.

5. **Model assessment.**  
   Evaluating whether an RSE is “good” depends on comparing it to:  
   - The scale of \(y\).  
   - The standard deviation of \(y\).  
   - Business or scientific tolerances for error.

RSE thus serves as a bridge between statistical fit and practical interpretability, making it an essential part of the regression diagnostic toolkit.
:::




## Example: interpreting RSE

:::: {.columns}
::: {.column width="50%"}
- Advertising model: Sales ~ TV + Radio + Newspaper.  
- RSE = 1.69 (thousands of units).  
- Mean sales ≈ 14.0 (thousands).  
- Interpretation:  
  - Predictions are typically off by ~1,690 units.  
  - Error ≈ 12% of the mean → fairly good fit?  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/diag__rse_bar.png"
       alt="**Figure:** Bar chart showing mean sales vs. average RSE error">
  <figcaption>**Figure:** Bar chart showing mean sales vs. average RSE error</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Example: interpreting RSE

### Detailed Notes  
- Use this slide to emphasize the **context-dependent nature** of RSE. The number alone—1.69—means nothing until we interpret it relative to the outcome scale.  
- Walk students through the example step-by-step:  
  - An RSE of 1.69 (thousand units) means the model’s predictions typically deviate from actual sales by roughly **1,690 units**.  
  - When the average sales value is around **14,000**, this represents roughly **12% error**, which may be acceptable or even quite good depending on the domain.  
- Highlight the key teaching point: **RSE must always be interpreted in relation to the magnitude of the outcome variable**.  
- Pedagogically, caution students that interpreting RSE in isolation can be misleading. The same RSE can represent excellent performance in one context and poor performance in another.  
- Encourage students to compare RSE to:  
  - The mean of the outcome  
  - The standard deviation of the outcome  
  - Business or scientific tolerance for error  
- Use this slide to reinforce that diagnostics and model assessment require **domain awareness**, not just numeric computation.

### Deeper Dive  
This example illustrates how absolute error metrics like RSE derive their meaning from **comparative scale**. RSE estimates the standard deviation of the model’s residuals—the typical size of prediction errors.

Formally:
\[
RSE = \sqrt{\frac{RSS}{n - p - 1}}.
\]

Interpreting RSE requires answering:

1. **How large is the error relative to the magnitude of the outcome?**  
   An RSE of 1.69 is small when outcomes range around 14, but large when outcomes are around 3.

2. **How does the RSE compare to the variability of the outcome?**  
   If the outcome’s standard deviation is much larger than 1.69, the model captures substantial structure. If the standard deviation is smaller, the model is underperforming.

3. **Is the error acceptable for the decision context?**  
   In forecasting, budgeting, or inventory planning, a 12% error might be excellent. In medical dosing or engineering tolerances, it might be unacceptable.

This example reinforces a broader theme: **model evaluation is contextual, not absolute**. RSE is a useful metric because it lives in the same units as the prediction target, but its meaning emerges only when interpreted relative to domain-specific scales and expectations.

Understanding this relationship prepares students for more nuanced evaluation techniques later in the course, where multiple metrics must be balanced to form a complete picture of model performance.
:::



## RSE vs. R²: complementary measures

- **RSE:**  
  - Error in original units.  
  - Communicates typical prediction error.  
- **R²:**  
  - Proportion of variance explained (0–1).  
  - Scale-free; useful for comparing fit across different outcomes or models.  

Taken together: RSE provides an **absolute** sense of fit; R² provides a **relative** sense of fit.


::: {.notes}
### Slide:: RSE vs. R² — complementary measures

### Detailed Notes  
- Use this slide to show students that **no single metric captures model fit completely**. RSE and R² answer different questions, and both are essential.  
- Start with RSE: emphasize that it gives an **absolute measure** of error in the same units as the outcome. This makes it easy to communicate to stakeholders (“our predictions are typically off by X units”).  
- Then move to R²: emphasize that it provides a **relative measure**—how much better the model is compared to simply predicting the mean every time.  
- Stress that neither metric should be interpreted in isolation. A model can have:  
  - High R² but large RSE (good relative fit but poor absolute accuracy).  
  - Low R² but acceptably small RSE (if the outcome itself has low variance).  
- Pedagogically, reinforce that model evaluation requires contextual thinking: the same R² or RSE can mean different things depending on domain, outcome scale, and decision stakes.  
- Encourage students to report both metrics in all analyses; together they form a more complete picture of fit.

### Deeper Dive  
RSE and R² derive from the same underlying decomposition of variance but serve distinct interpretive purposes.

**Residual Standard Error (RSE)**  
\[
RSE = \sqrt{\frac{RSS}{n - p - 1}}
\]
estimates the standard deviation of the unexplained portion of the outcome. RSE answers: *How far off are predictions, in the units that matter?* It is sensitive to the scale of \(y\), which makes it practical but not directly comparable across datasets or transformed outcomes.

**R²: Coefficient of Determination**  
\[
R^2 = 1 - \frac{RSS}{TSS}
\]
measures the proportion of total variation in \(y\) explained by the model relative to a naive baseline model that predicts the sample mean. R² answers: *How much better is the model at explanation than doing nothing but predicting the average?*

Key conceptual distinctions:

- **Absolute vs. relative fit:** RSE quantifies actual error magnitude; R² quantifies improvement over a null model.  
- **Scale sensitivity:** RSE changes if \(y\) is rescaled; R² remains unchanged.  
- **Interpretability:** RSE is easier to communicate; R² is easier to compare across models or outcomes.  
- **Limitations:** High R² does not guarantee good predictions. Low R² does not imply a model is useless, especially when the outcome has inherently low variability.

Together, the two metrics enable both **practical** and **statistical** interpretation—one anchors performance in real-world units, the other situates performance in relation to baseline variability.
:::




## Regression assumes errors are independent (one error shouldn’t predict another)

- Linear regression assumption:  
  $$
  \text{Cov}(\epsilon_i, \epsilon_j) = 0 \quad \text{for } i \neq j
  $$
- Violations occur commonly in:  
  - **Time series data** (e.g., sales by month, stock returns).  
  - **Clustered or grouped data** (e.g., observations from the same person, family, or store).  
- If ignored → standard errors underestimated → overconfident inference (narrow CIs, small p-values).  

::: {.notes}
### Slide:: Regression assumes errors are independent

### Detailed Notes  
- This slide reinforces a key point: regression does not assume *data* are independent; it assumes **errors** are independent. Students often conflate these.  
- Explain the intuition: independence of errors means that once the model accounts for predictors, the leftover unexplained variation should not have structure.  
- Walk through the examples:  
  - **Time series:** yesterday’s residual often predicts today’s, because trends or autocorrelation remain unmodeled.  
  - **Clustered data:** individuals or stores behave similarly across records, inducing correlation among their residuals.  
- Explain why this matters: if residuals are correlated, the model’s estimated uncertainty is **too optimistic**. Coefficients may look “significant” simply because standard errors are biased downward.  
- Pedagogically, stress that correlated errors do *not* necessarily invalidate predictions but they **do** undermine inference.  
- Set up the next slides: diagnostics will show how to detect correlated errors, and later content will discuss remedies.

### Deeper Dive  
In the classical linear model, independence of errors implies:
\[
\text{Cov}(\epsilon_i, \epsilon_j) = 0 \quad \text{for } i \neq j.
\]
This ensures that the variance–covariance matrix of the error term is diagonal. Under this condition, OLS estimators achieve minimum variance among unbiased linear estimators.

When errors are correlated, several consequences arise:

1. **Standard errors are biased downward.**  
   If residuals move together, the model behaves as if it has more information than it actually does. This leads to narrow confidence intervals and inflated t-statistics.

2. **Inference becomes unreliable.**  
   Hypothesis tests (t-tests, F-tests) and confidence intervals rely on correct standard errors. Correlated residuals break these assumptions.

3. **Coefficient estimates remain unbiased (usually).**  
   As long as predictors are exogenous and other assumptions hold, OLS point estimates do not become biased in the presence of correlated errors. The main issue is incorrect uncertainty quantification.

4. **Common sources of correlated errors:**  
   - **Temporal correlation:** patterns unfolding over time.  
   - **Clustered correlation:** repeated measurements within entities \((\text{people, firms, stores})\).  
   - **Spatial correlation:** nearby observations influencing one another.

5. **Solutions:**  
   - Robust standard errors (cluster-robust, Newey–West).  
   - Mixed-effects models for grouped data.  
   - Time-series models for autoregressive structure.

The broader takeaway is that independence of errors is essential for **valid inference**, not necessarily for **accurate prediction**. Recognizing when errors are correlated—and applying appropriate remedies—ensures that uncertainty statements and hypothesis tests remain trustworthy.
:::




## Detecting correlated errors

:::: {.columns}
::: {.column width="50%"}
- Plot residuals against **time/order of data**.  
  - Random scatter → independence holds.  
  - Patterns/trends → correlation present.  
- Example patterns:  
  - “Runs” of positive/negative residuals.  
  - Slow-moving cycles in residuals.  
 
:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/diag__correlated_errors.png"
       alt="**Figure:** Residuals plotted against time; top = random, bottom = clear trend">
  <figcaption>**Figure:** Residuals plotted against time; top = random, bottom = clear trend</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Detecting correlated errors

### Detailed Notes  
- Explain that correlated errors are often **invisible** in standard residual-vs.-fitted plots; time or ordering must be explicitly introduced to uncover structure.  
- Emphasize that regression assumes the unexplained part of the outcome behaves like random noise. When residuals show patterns over time, this assumption is violated.  
- Walk through the plot:  
  - The top panel shows desirable behavior—residuals fluctuate randomly around zero.  
  - The bottom panel shows a clear trend or “wave” pattern, indicating autocorrelation.  
- Help students interpret common patterns:  
  - **Runs of positive or negative residuals:** indicates the model consistently under- or over-predicts for stretches of time.  
  - **Slow-moving cycles:** suggests seasonality, omitted time trends, or unmodeled dynamics.  
- Pedagogically, explain that ignoring correlated errors can lead to **overconfident inference**. Standard errors shrink, confidence intervals narrow, and p-values become misleading.  
- Reinforce that detecting these patterns is the first step; later slides will cover remedies such as robust errors and time-series modeling.

### Deeper Dive  
Autocorrelation in residuals implies that:
\[
\text{Cov}(\epsilon_t, \epsilon_{t+k}) \neq 0 \quad \text{for some lag } k.
\]
Such patterns arise when the regression model fails to capture relevant structure in the data-generating process.

**Typical sources of correlated residuals:**
- **Trends:** upward or downward patterns over time not accounted for by predictors.  
- **Seasonality or cycles:** periodic behavior not reflected in the model.  
- **Lagged effects:** the outcome depends on its own past values (autoregressive structure).  
- **Clustered or panel data:** repeated observations from the same individuals or entities.

**Diagnostic strategies:**
1. **Residual plot vs. time/order**  
   Visual inspection is the earliest and often most revealing diagnostic method. Runs, waves, or cycles signal problems immediately.

2. **Autocorrelation Function (ACF) or Partial ACF**  
   These tools quantify correlation across different lags.

3. **Durbin–Watson test**  
   A formal test for first-order autocorrelation in regression errors.

**Consequences of ignoring correlated errors:**
- Standard errors too small → inflated t-statistics  
- Underestimated uncertainty → misleading inference  
- Poor forecasts if temporal structure is unmodeled

**Remedies:**
- Introduce time-related predictors  
- Add lagged dependent variables  
- Use autoregressive error structures  
- Apply robust (e.g., Newey–West) standard errors  
- Switch to time-series models (ARIMA, ETS, etc.)

This slide reinforces a central theme: **the order of data matters**, and diagnostics must respect that order to accurately assess model validity.
:::


## Remedies for correlated errors

- **Time series models:**  
  - ARIMA, exponential smoothing.  
- **Robust standard errors:**  
  - Adjust inference when clustering exists (e.g., cluster-robust SEs).  
- **Design changes:**  
  - Collect data to minimize dependence (randomization).  

::: {.notes}
### Slide:: Remedies for correlated errors

### Detailed Notes  
- Use this slide to emphasize that correlated errors don’t automatically invalidate a model’s coefficient estimates, but they **do** undermine inference. Students often mistakenly assume coefficients become biased; clarify that the main issue is incorrect standard errors.  
- Walk through the remedies by category:
  - **Time series models:** When patterns over time remain after accounting for predictors, switching to ARIMA or exponential smoothing acknowledges the sequential dependence directly. Stress that these methods explicitly model autocorrelation, seasonality, and trends.
  - **Robust standard errors:** For clustered or grouped data, cluster-robust SEs protect inference even when residuals within groups are correlated. Reinforce that this does not change coefficient estimates—it changes **confidence intervals and p-values**.
  - **Design changes:** In experimental or quasi-experimental settings, emphasize that randomization is the most powerful cure for dependence. Independent assignment of treatments breaks correlation structures and simplifies downstream modeling.
- Pedagogically, highlight that the best remedy depends on the **source** of correlation. There is no universal fix.

### Deeper Dive  
Correlated errors indicate that the modeling framework has not fully captured the structure of the data. Different contexts call for different remedies:

**1. Time-series contexts**  
Autocorrelation in sequential data can be addressed by:
- Adding lagged predictors.
- Using ARIMA or ARIMAX models.
- Applying exponential smoothing or state space models.

These methods explicitly model the dynamics of the outcome or the error structure.

**2. Clustered or panel data**  
When observations are nested (e.g., repeated measures for individuals), robust standard errors (clustered SEs) adjust the variance–covariance matrix:
\[
\widehat{\text{Var}}(\hat{\beta}) = (X^\top X)^{-1} (X^\top \Omega X) (X^\top X)^{-1},
\]
where \(\Omega\) accounts for intra-cluster correlation.

**3. Experimental design improvements**  
If correlation arises from the data collection process, the best long-run remedy may be **better design**:
- Random assignment
- Rotating sampling
- Temporal blocking

These reduce structural dependencies that create correlated errors in the first place.

The broader insight: correcting correlated errors is about aligning the modeling approach with the **data-generating process**, either through better models or better data.
:::



## Linear regression is a valuable baseline for both learning and practice

- Interpretable, fast, and intuitive.  
- Serves as a benchmark against more complex models.  
- Provides diagnostic insight into data before moving to nonlinear methods.  

::: {.notes}
### Slide:: Linear regression is a valuable baseline for both learning and practice

### Detailed Notes  
- Present this slide as a reminder that linear regression is not simply a historical artifact—it remains foundational because of its clarity, interpretability, and strong diagnostics.  
- Emphasize that even when more complex models outperform it in prediction accuracy, linear regression provides:
  - A **benchmark** for comparison.
  - A **sanity check** on relationships.
  - A **transparent model** that helps stakeholders understand the underlying structure.  
- Stress the pedagogical value: students who master linear regression’s assumptions, diagnostics, and interpretation will find it easier to understand more complex models later.  
- Highlight that because linear regression is fast to train and easy to debug, it is often the first model practitioners try in applied settings.  
- Encourage students to view linear regression not as a “toy model” but as a powerful conceptual anchor.

### Deeper Dive  
Linear regression plays three essential roles in modern analytics and machine learning:

1. **Interpretive baseline**  
   Coefficients provide clear, quantitative statements about relationships. Even when deep models are used, linear regression often informs feature selection, relationship testing, and early exploratory modeling.

2. **Predictive baseline**  
   In many domains—especially those involving structured tabular data—linear models perform competitively with more complex approaches. They provide a strong starting point for assessing whether complexity is warranted.

3. **Diagnostic scaffold**  
   Regression’s rich suite of diagnostics (residuals, leverage, influence, heteroscedasticity checks, multicollinearity diagnostics) teaches analysts how to evaluate model fit thoughtfully. Many analogs in machine learning borrow from these ideas.

This makes linear regression uniquely positioned:  
- Simple enough to interpret.  
- Powerful enough to serve as a launching point.  
- Transparent enough to reveal modeling issues early.

Far from being outdated, linear regression remains a core tool in the analyst’s toolkit, grounding both predictive modeling and statistical reasoning.
:::



# Multiple Regression


## Multiple regression extends linear regression to many predictors

- Equation:  
  $$
  y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon
  $$  
- Allows us to estimate the effect of each predictor **holding others constant**.  
- Answers: which predictors add unique explanatory power?  

::: {.notes}
### Slide:: Multiple regression extends linear regression to many predictors

### Detailed Notes  
- Present this slide as the natural generalization of simple regression: instead of modeling \(y\) as a function of a single predictor, we now model it as a function of many.  
- Emphasize that the core idea does **not** change—OLS still minimizes squared errors—but interpretation becomes richer because each coefficient represents an effect **controlling for the others**.  
- Stress that this is essential for real-world data, where outcomes rarely depend on a single variable.  
- Walk students through the equation and highlight that each predictor has its own coefficient, allowing us to isolate its contribution.  
- Pedagogically, warn students: adding predictors naively can introduce multicollinearity, overfitting, and interpretation challenges—hence the importance of diagnostics and feature selection.

### Deeper Dive  
Multiple regression models:
\[
y = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p + \epsilon
\]
capture the **conditional expectation** \(E[y \mid X]\). Each coefficient \(\beta_j\) reflects the partial effect of \(x_j\) given the values of other predictors.

Key implications:
- **Interpretational clarity:** Multiple regression helps separate confounded relationships by adjusting for correlated predictors.  
- **Model flexibility:** More variables allow the model to capture more structure (if they contain signal).  
- **Risk of overfitting:** As \(p\) grows, variance increases; regularization or feature selection may become necessary.  

Multiple regression forms the backbone of empirical modeling in business, economics, social science, and many areas of applied ML.
:::


## Separate regressions can exaggerate or misattribute effects

- Predictors often correlate with each other.  
- Example: Advertising spend on TV and newspaper both rise with sales.  
- Simple regressions may credit newspaper for an effect actually driven by TV.  
- Multiple regression separates unique contributions.  

::: {.notes}
### Slide:: Separate regressions can exaggerate or misattribute effects

### Detailed Notes  
- Introduce this slide by highlighting a common beginner mistake: running simple regressions separately for each predictor and interpreting them at face value.  
- Emphasize that when predictors are correlated—as they often are—separate regressions confound their effects.  
- Walk through the advertising example: TV, radio, and newspaper spending often rise together. A simple regression of sales on newspaper spend may mistakenly attribute TV’s effect to newspapers.  
- Stress that **multiple regression corrects this** by estimating unique contributions while holding other predictors constant.  
- Pedagogically, use this slide to reinforce the value of conditioning—without it, models may tell misleading stories.

### Deeper Dive  
Separate regressions estimate:
\[
y = \beta_0 + \beta_j x_j + \epsilon
\]
for each predictor independently. When predictors are correlated, these estimates represent **marginal associations**, not unique effects.

In multiple regression, the coefficient:
\[
\beta_j = \text{effect of } x_j \text{ holding } X_{-j} \text{ fixed}
\]
isolates each predictor’s conditional impact. This helps solve problems such as:
- **Omitted variable bias:** Failing to include correlated predictors biases estimates.  
- **Attribution error:** Misassigning effects to variables that merely correlate with the true driver.  
- **Interpretation distortion:** Coefficients from separate models may be inflated due to shared variance.

This slide sets the stage for why multiple regression is conceptually and practically superior to independent simple regressions.
:::



## Coefficients in multiple regression have a conditional meaning

- $\beta_j$: expected change in $y$ for a one-unit increase in $x_j$, **holding all other predictors fixed**.  
- Example:  
  - TV coefficient = 0.046 → each extra $1,000 on TV ads increases sales by ~46 units, holding radio and newspaper constant.  
- Interpretation is always **conditional** on the other variables in the model.  

::: {.notes}
### Slide:: Coefficients in multiple regression have a conditional meaning

### Detailed Notes  
- Emphasize the key interpretive shift from simple to multiple regression: **coefficients are conditional effects**.  
- Clarify that \(\beta_j\) measures the expected change in \(y\) from a one-unit change in \(x_j\) **while holding all other predictors constant**.  
- Use the advertising example to make this concrete: the effect of TV spend is interpreted after accounting for radio and newspaper spend.  
- Stress that students must always interpret coefficients within the full model context; the same variable can change sign or magnitude depending on which other predictors are included.  
- Pedagogically, remind students that correlation among predictors affects interpretability—even when predictions remain stable.

### Deeper Dive  
In multiple regression, coefficients represent **partial derivatives** of the regression function:
\[
\frac{\partial E[y \mid X]}{\partial x_j} = \beta_j.
\]
This quantifies the local effect of \(x_j\) in the presence of other variables.

Conditional interpretation has several implications:

- **Effect isolation:** Coefficients reflect unique contributions, not simple correlations.  
- **Sensitivity to model specification:** Adding or removing predictors changes coefficients because partial relationships shift.  
- **Collinearity consequences:** When predictors are correlated, standard errors inflate and coefficients become unstable, complicating interpretation.  
- **Contrast with causal inference:** Even conditional coefficients represent associations, not causal effects, unless additional assumptions or designs support causal interpretation.

Understanding conditional meaning is essential for empirical modeling. It distinguishes rigorous regression analysis from superficial “plug and play” modeling and prepares students for advanced topics like interactions, regularization, and causal inference.
:::



## Beyond individual predictors: does the model matter?

- In multiple regression, we may ask:  
  - Are **any** predictors related to the response?  

Null hypothesis:  
  $$
  H_0 : \beta_1 = \beta_2 = \dots = \beta_p = 0
  $$  
- Alternative: at least one $\beta_j \neq 0$.  
- Tested using the **F-statistic**.  

::: {.notes}
### Slide:: Beyond individual predictors: does the model matter?

### Detailed Notes  
- Introduce this slide as shifting from **individual-level inference** (t-tests) to **model-level inference**.  
- Emphasize that t-tests answer whether *each* predictor contributes uniquely, while the F-test asks whether the **model as a whole** improves prediction relative to a baseline with no predictors.  
- Walk students through the logic: the null hypothesis says that *none* of the predictors matter—in other words, the model has no explanatory power beyond predicting the mean.  
- Make clear that the alternative hypothesis is broad: at least one predictor contributes, but the F-test does not tell us *which* one(s).  
- Pedagogically, highlight that this is an important early diagnostic: before interpreting individual coefficients, we first verify that the model itself is useful.

### Deeper Dive  
The global null hypothesis is:
\[
H_0 : \beta_1 = \beta_2 = \dots = \beta_p = 0.
\]
Under this hypothesis, the model collapses to:
\[
y = \beta_0 + \epsilon.
\]

A statistically significant F-test indicates that the set of predictors collectively explains more variance in \(y\) than would be expected by chance.

Key implications:
- The F-test is a **joint test** of all slopes.  
- It protects against false discoveries that arise from examining individual coefficients in isolation.  
- If the F-test is not significant, examining individual predictors is usually unwarranted.  

This slide introduces the idea that regression modeling includes not just evaluating predictors but evaluating the **entire specification** of the model.
:::



## The F-statistic compares explained vs. unexplained variance

- Formula:  
  $$
  F = \frac{(TSS - RSS)/p}{RSS / (n - p - 1)}
  $$  
  - **TSS:** Total sum of squares (total variation in \(y\)).  
  - **RSS:** Residual sum of squares (unexplained variation).  
  - Numerator = variance explained per predictor.  
  - Denominator = variance left per degree of freedom.  

- Large \(F\) → strong evidence that at least one predictor matters.  


::: {.notes}
### Slide:: The F-statistic compares explained vs. unexplained variance

### Detailed Notes  
- Use this slide to show students the **mechanics** behind the F-test. Rather than memorizing a formula, they should understand the ratio’s meaning.  
- Emphasize the components:
  - \(TSS\) = total variability in the outcome.  
  - \(RSS\) = variability left unexplained by the model.  
- Walk through the numerator: \((TSS - RSS)/p\) quantifies **explained variance per predictor**.  
- Walk through the denominator: \(RSS/(n - p - 1)\) quantifies **unexplained variance per degree of freedom**.  
- Pedagogically, frame the F-statistic as asking: *Did adding predictors reduce error enough to justify their complexity?*

### Deeper Dive  
The F-statistic is:
\[
F = \frac{(TSS - RSS)/p}{RSS/(n - p - 1)}.
\]

Interpretation:
- Large numerator → much of the variation in \(y\) is explained by the model.  
- Small denominator → remaining error is small relative to model complexity.  
- Large \(F\) → evidence that the model explains significant variation.  

Under the null hypothesis that all coefficients except the intercept are zero, this ratio follows an F-distribution with \(p\) and \(n - p - 1\) degrees of freedom.

Additional insights:
- If \(F \approx 1\): model fits no better than predicting the mean.  
- The F-statistic guards against overfitting by penalizing models for additional predictors.  
- It provides a **global significance test** before diving into individual coefficient tests.

This prepares students to understand model comparison, nested models, and adjusted \(R^2\).
:::




## Example: advertising data

- Fit: Sales ~ TV + Radio + Newspaper.  
- Output:  
  - Residual standard error (RSE) = 1.69  
  - \(R^2 = 0.897\)  
  - F-statistic = 570 (p < 0.0001).  
- Conclusion:  
  - The overall model explains much more variance than chance.  
  - At least one medium (TV/Radio) is strongly associated with sales.  

::: {.notes}
### Slide:: Example: advertising data

### Detailed Notes  
- Use this slide to **bring the F-test to life** with a real numerical example.  
- Walk students through the interpretation step-by-step:  
  - RSE of 1.69 means typical prediction error is modest relative to mean sales.  
  - \(R^2 = 0.897\) indicates that nearly 90% of the variability is explained by the model.  
  - F-statistic of 570 (with an extremely low p-value) signals overwhelming evidence that the model explains meaningful variation.  
- Emphasize that even if some individual predictors are weak, the **model as a whole** can still be important.  
- Pedagogically, use this to reinforce the distinction between global usefulness (F-test) and individual significance (t-tests).

### Deeper Dive  
This example reinforces several regression concepts simultaneously:

**1. Model-level significance**  
The very large F-statistic indicates that the set of predictors—TV, radio, and newspaper—jointly improves predictive accuracy relative to the null model.

**2. Distinguishing overall vs. individual effects**  
It is entirely possible for one variable (e.g., newspaper) to have a non-significant coefficient while the model overall is highly significant. This reflects overlapping or correlated contributions to variance explained.

**3. Complementary metrics**  
- **RSE** provides practical interpretability (error magnitude).  
- **\(R^2\)** provides relative fit.  
- **F-statistic** provides model-level evidence.  

Together, these metrics tell a consistent story: the model captures meaningful structure in the data.

This example helps students appreciate why regression output must be read holistically, not one line at a time.
:::





## Multiple testing caution in regression

- In multiple regression with many predictors:  
  - Running many t-tests → some small p-values appear **by chance**.  
  - Example: with 100 predictors, expect ~5 to be “significant” at 5% level even if none truly matter.  
- Consequences:  
  - False positives (spurious significance).  
  - Overstated confidence in weak predictors.  

- Solutions:  
  - Use **overall F-test** to check if model adds value.  
  - Prefer **cross-validation** to assess predictive usefulness.  
  - Adjust for multiple comparisons if doing many hypothesis tests.  


::: {.notes}
### Slide:: Multiple testing caution in regression

### Detailed Notes  
- Introduce this slide as a warning about a **statistical pitfall that grows with model size**. When many predictors are tested simultaneously, some will appear significant purely due to random variation.  
- Stress that this is not a software bug or a modeling mistake—it is a mathematical inevitability under repeated hypothesis testing.  
- Use the example explicitly: with 100 predictors tested at α = 0.05, we expect around 5 false positives even if no predictors truly matter.  
- Explain the consequences:  
  - Analysts may mistakenly interpret noise as meaningful signal.  
  - Overfitting becomes more likely as models include spurious predictors.  
  - Inference becomes unreliable when researchers chase low p-values.  
- Pedagogically, emphasize the role of *global* tests like the F-test, which evaluate collective signal rather than individual effects.  
- Reinforce that in predictive modeling, we often rely more on **cross-validation** and **model performance** rather than scanning dozens of individual p-values.

### Deeper Dive  
Multiple hypothesis testing inflates the probability of false positives. In regression settings with many predictors, the probability that at least one coefficient appears significant under pure noise grows rapidly:
\[
P(\text{at least one false positive}) = 1 - (1 - \alpha)^p.
\]

Consequences include:
- Spurious inference (“finding significance” where there is none).  
- Instability in coefficient estimates across samples.  
- Difficulty communicating results ethically and accurately.

Common remedies:
1. **Global model tests (F-test):** Checks whether *any* predictors collectively add explanatory power.  
2. **Cross-validation:** Prioritizes predictive performance and guards against including variables that do not generalize.  
3. **Multiple-comparison adjustments:** Bonferroni, Holm, Benjamini–Hochberg (FDR), etc., when inference across many coefficients is essential.  
4. **Regularization:** Methods like Lasso shrink or eliminate coefficients, helping mitigate spurious effects.

The broader lesson: individual p-values are fragile in high-dimensional settings; model-based reasoning and validation are more robust.
:::




## Point predictions aren’t enough

Two interval types for uncertainty: **(1) Confidence interval (mean response)** and **(2) Prediction interval (individual)**. 

- Confidence intervals: average effect, Quantifies uncertainty around the **mean response** at given \(X\).  
- Formula (approx.):  
  $$
  \hat{y} \pm 2 \times SE(\hat{y})
  $$  
- Example:  
  - Spending \$100k on TV ads predicts **11.2k sales**.  
  - 95% CI for mean sales = [11.0k, 11.4k]. 

## Point predictions aren’t enough- Prediction Interval

- Prediction intervals are about an individual outcome. Quantifies uncertainty around a **single new observation**.  
- Formula:  
  $$
  \hat{y} \pm 2 \times SE_{\text{pred}}
  $$  
- Always wider than a CI, since they include irreducible error (\(\epsilon\)).  

::: {.notes}
### Slide:: Point predictions aren’t enough

### Detailed Notes  
- Introduce this slide as expanding students’ understanding of prediction: a single predicted value is incomplete without a sense of **uncertainty**.  
- Clarify the difference between **confidence intervals (CIs)** and **prediction intervals (PIs)**:
  - CIs describe uncertainty around the **mean response** at a given value of \(X\).  
  - PIs describe uncertainty around a **new individual observation**, which includes both model uncertainty and irreducible error.  
- Use the advertising example to show how narrow CIs can be compared to PIs, reinforcing how much more variability individual outcomes contain.  
- Pedagogically, emphasize that these two intervals answer different questions and must not be used interchangeably.

### Deeper Dive  
Confidence intervals are constructed as:
\[
\hat{y} \pm 2 \cdot SE(\hat{y}),
\]
which reflects uncertainty in estimating the mean response \(E[y \mid X]\). These intervals shrink as sample size grows because averaging reduces noise.

Prediction intervals incorporate both the variance of the estimate and the inherent noise in individual outcomes:
\[
\hat{y} \pm 2 \cdot SE_{\text{pred}}.
\]
Because individual outcomes include irreducible error \(\epsilon\), PIs are always wider.

Key distinctions:
- **Purpose:** CIs support inference; PIs support forecasting.  
- **Width:** PIs are always wider because they include both estimation error and outcome variance.  
- **Interpretation:** CIs describe means; PIs describe individuals.

This slide reinforces that meaningful prediction requires uncertainty quantification, not just point estimates.
:::



## Confidence vs. prediction intervals: side by side

:::: {.columns}
::: {.column width="50%"}
- **Confidence interval (CI):**  
  - “Where the mean will fall.”  
  - Narrower; used for inference on group averages.  

- **Prediction interval (PI):**  
  - “Where an individual will fall.”  
  - Wider; used for forecasting single cases.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/infer__ci_pi.png"
       alt="**Figure:** Graph with regression line, narrow CI band, wider PI band">
  <figcaption>**Figure:** Graph with regression line, narrow CI band, wider PI band</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Confidence vs. prediction intervals: side by side

### Detailed Notes  
- Use this slide to reinforce visually the conceptual difference between CIs and PIs.  
- Highlight that the CI band hugs the regression line tightly, representing much less uncertainty about the **mean**.  
- In contrast, the PI band is wider because it must account for individual-level variation (irreducible noise).  
- Pedagogically, remind students that stakeholders often mistake point predictions for precise estimates—showing both intervals helps calibrate expectations.  
- Encourage students always to ask: “Are we predicting an average or an individual?” The choice determines the appropriate interval.

### Deeper Dive  
The plotted figure demonstrates two fundamental statistical truths:

1. **Narrower CIs:**  
   These reflect the precision of estimating the mean response. As sample size increases, \(SE(\hat{y})\) decreases, tightening the CI band.

2. **Wider PIs:**  
   These must accommodate irreducible error \(\epsilon\). Even perfect knowledge of the model cannot eliminate individual randomness, so prediction intervals remain wide.

Mathematically:
- CI variance: \( \text{Var}(\hat{y}) \)  
- PI variance: \( \text{Var}(\hat{y}) + \sigma^2 \)

Interpretation implications:
- CI answers: *If we repeated the study, where would the mean fall?*  
- PI answers: *Given this \(X\), what range of outcomes is plausible for a specific new observation?*

This slide deepens understanding of uncertainty and sets up later discussions on forecasting, decision-making under uncertainty, and real-world model deployment.
:::



# Interactions & Categorical Predictors


## Predictors may interact with each other

- Standard linear regression assumes **additive effects**:  
  $$
  y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
  $$  
  - Effect of $(X_1)$ does not depend on $(X_2)$.  
- But often one predictor **modifies** the effect of another.  
- Solution: include an **interaction term** $(X_1 \times X_2)$.  

::: {.notes}
### Slide:: Predictors may interact with each other

### Detailed Notes  
- Introduce this slide as the moment where regression moves beyond simple additive thinking.  
- Emphasize that additive models assume a predictor’s effect is **constant**, regardless of the level of another predictor.  
- Use intuitive examples—marketing channels, pricing + promotions, demographics modifying treatment effects—to show how this assumption often fails in real settings.  
- Explain why interactions matter: they capture **synergy** (effects amplify each other) or **moderation** (one variable weakens the effect of another).  
- Prepare students for more complex interpretation: once an interaction is included, the meaning of individual coefficients changes.

### Deeper Dive  
Additive models:
\[
y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
\]
assume partial derivatives \(\partial y/\partial X_1\) do not depend on \(X_2\).

Adding an interaction:
\[
y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3(X_1X_2)
\]
means:
\[
\frac{\partial y}{\partial X_1} = \beta_1 + \beta_3 X_2.
\]

Thus, the effect of \(X_1\) depends on the value of \(X_2\). This formulation generalizes linear regression to capture multiplicative or conditional relationships that cannot be expressed additively.
:::




## Example: interaction between two continuous predictors

:::: {.columns}
::: {.column width="60%"}
- Model:  
  $$
  y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 (X_1 \times X_2) + \epsilon
  $$  
- Interpretation:  
  - Effect of $(X_1)$ changes depending on $(X_2)$.  
  - Example: TV and Radio advertising. TV more effective when paired with Radio.  
    - $(\beta_3 > 0)$ indicates synergy.  

:::

::: {.column width="40%"}
<figure>
  <img src="../materials/assets/images/interact__tv_radio_surface.png"
       alt="**Figure:** 3D plot of sales vs. TV + Radio; curved surface contour lines shows synergy">
  <figcaption>**Figure:** 3D plot of sales vs. TV + Radio; curved surface contour lines shows synergy</figcaption>
</figure>

:::
::::


::: {.notes}

### Slide:: Example — interaction between two continuous predictors

### Detailed Notes  
- Use the 3D plot to visually anchor intuition: without interaction, the surface would be a plane; with interaction, curvature appears.  
- Emphasize that a positive \(\beta_3\) means **synergy**: the marginal effect of one variable grows with the level of the other.  
- Walk through the surface:  
  - At low radio spending, TV has modest impact.  
  - At high radio spending, TV’s slope steepens significantly.  
- Pedagogically, highlight that the presence of curvature in the fitted surface is the geometric signature of interaction.

### Deeper Dive  
In the model:
\[
y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3(X_1X_2) + \epsilon,
\]
the interaction coefficient \(\beta_3\) captures the rate at which the marginal effect of \(X_1\) changes with \(X_2\).

Key points:
- If \(\beta_3 = 0\): the model is additive, surfaces are planar.  
- If \(\beta_3 > 0\): increasing, synergistic slopes.  
- If \beta_3 < 0: diminishing or antagonistic effects.

Contour lines bend because they represent sets of equal predicted values; curvature indicates the joint influence of the predictors.  


Interactions reveal synergy. For example, TV advertising may boost awareness, while radio reinforces it. The joint effect is larger than the sum of separate effects. In regression, this is captured by the positive coefficient on TV×Radio.  

The 3D surface plot you generated shows how sales depend on TV spending (x-axis) and Radio spending (y-axis).

If there were no interaction: the surface would be a flat tilted plane. TV and Radio would just add their separate effects.

With a positive interaction (𝛽3>0): the surface is curved upward. That curve means:

When Radio spend is low, adding more TV raises sales, but only a little (gentle slope).

When Radio spend is high, adding more TV raises sales a lot more (steeper slope).

Vice versa for Radio: its effect is stronger when TV is higher.

TLDR; 
TV and Radio reinforce each other. Spending in both channels together is more powerful than spending the same total in just one channel.

The scatter dots show simulated “observed” data points with some random noise, while the smooth surface shows the underlying true model.

CURVED LINES: 
The curved lines on the “floor” of the 3D plot are contour lines.

Interpreting:

- Imagine shining a light straight down on the surface from above.
- The contours are the “shadows” of the surface height (sales values) projected onto the bottom plane.
- Each line connects points where the model predicts the same sales value. Like a topographic map connects points at the same elevation.

So:

- Closer contour lines → the surface is steeper there (sales change quickly).
- Farther apart contour lines → the surface is flatter (sales change more slowly).
- Because of the positive interaction, the contours bend upward instead of being straight lines. That bending is another visual cue that TV and Radio effects combine synergistically rather than add independently.

:::


## Example: interaction between category and number

:::: {.columns}
::: {.column width="50%"}
- Suppose predictors: **Income** (numeric) and **Student status** (Yes/No).  
- Model without interaction:  
  - Same slope, parallel lines for students and non-students.  
- Model with interaction:  
  - Slopes differ by group.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/interaction_two_continuous.png"
       alt="**Figure:** Two regression lines, parallel vs. differing slopes">
  <figcaption>**Figure:** Two regression lines, parallel vs. differing slopes</figcaption>
</figure>

:::
::::
::: {.notes}
### Slide:: Example — interaction between category and numeric predictor

### Detailed Notes  
- Use the twin-panel figure to show students that interactions with categorical predictors change **slopes**, not just intercepts.  
- Start with the left panel: parallel lines indicate *no interaction*; differences appear only in intercepts.  
- Move to the right panel: slopes differ by group, showing that the effect of income depends on student status.  
- Pedagogically, emphasize conditional interpretation: each slope represents a distinct relationship within each category.

### Deeper Dive  
For a binary variable \(D\) (0 = non-student, 1 = student):
\[
y = \beta_0 + \beta_1 X + \beta_2 D + \beta_3(X \cdot D).
\]

Interpretation:
- Non-students: \(y = \beta_0 + \beta_1X\).  
- Students:  
\[
y = (\beta_0 + \beta_2) + (\beta_1 + \beta_3)X.
\]

Thus:
- \(\beta_2\) = difference in intercepts.  
- \(\beta_3\) = difference in slopes.

This decomposition helps translate the visuals into regression logic.
:::


## The hierarchical principle

- **Rule of thumb:**  
  - If an interaction is included, keep the main effects too.  
- Example:  
  - Even if $(\beta_1)$ (TV) not significant, keep it if TV×Radio is included.  
- Reason:  
  - Dropping main effects changes the interpretation of the interaction.  

::: {.notes}
### Slide:: The hierarchical principle

### Detailed Notes  
- Emphasize that interaction terms cannot stand alone—they modify main effects, not replace them.  
- Explain that dropping main effects produces coefficients that have no coherent interpretation.  
- Use the TV × Radio example: removing main effects forces the model to attribute all variation to the interaction term, altering meaning.

### Deeper Dive  
Including an interaction term implies:
\[
E[y \mid X_1, X_2] \text{ depends on both } X_1 \text{ and } X_2 \text{ individually.}
\]
Dropping main effects constrains the model to:
\[
E[y \mid X_1, X_2] = \beta_3 X_1X_2,
\]
which is rarely realistic and mis-specifies the functional form.

Hierarchical (or “principled”) modeling ensures interpretability and numerical stability.
:::




## Regression handles categories with dummy variables


:::: {.columns} 
::: {.column width="50%"} 

- Many predictors are **categorical** (e.g., gender, region, product type).  
- Dummy variable = indicator (0 or 1) for category membership.  
- Example: Own a house (Yes/No).  
  - Own = 1, Rent = 0.  
  - Equation:  
    $$
    y = \beta_0 + \beta_1 \cdot Own + \epsilon
    $$  
::: 
::: {.column width="50%"} 
<figure>
  <img src="../materials/assets/images/dummy_variables1.png"
       alt="**Figure:** Table encoding ownerhip into 0,1 dummies.">
  <figcaption>**Figure:** Table encoding ownerhip into 0,1 dummies.</figcaption>
</figure>

::: 
::::


::: {.notes}
### Slide:: Regression handles categories with dummy variables

### Detailed Notes  
- Explain that regression requires numeric inputs; dummy coding converts categories into interpretable numeric indicators.  
- Use simple examples (binary homeownership) to show how a single dummy captures group-level shifts in the mean response.  
- Emphasize that dummy variables do **not** imply ordering.

### Deeper Dive  
A dummy variable \(D\):
- Takes value 1 for category of interest, 0 otherwise.  
- Produces model:
\[
y = \beta_0 + \beta_1 D + \epsilon.
\]

Interpretation:
- Baseline (\(D=0\)): expected value = \(\beta_0\).  
- Category (\(D=1\)): expected value = \(\beta_0 + \beta_1\).

Thus \(\beta_1\) represents a group difference.  
Dummy coding preserves interpretability and maps categorical distinctions into linear models.
:::




## Coefficients represent differences from a baseline

:::: {.columns}
::: {.column width="50%"}
- Intercept $(\beta_0)$ = predicted outcome for baseline category.  
- Dummy coefficient $(\beta_1)$ = difference between baseline and category.  
- Example:  
  - Non-owners: average balance = \$510.  
  - Owners: average balance = \$510 + $\beta_1$.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/dummy_variables2.png"
       alt="**Figure:** Bar chart comparing two groups">
  <figcaption>**Figure:** Bar chart comparing two groups</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Coefficients represent differences from a baseline

### Detailed Notes  
- Reinforce that dummy-coded models are **relative** models: effects always compare against a reference group.  
- Clarify that the baseline is arbitrary but changes interpretation.  
- Use the bar chart to show how \(\beta_1\) shifts levels.

### Deeper Dive  
In a dummy-coded regression:
\[
y = \beta_0 + \beta_1 D + \epsilon,
\]
the intercept is the baseline group’s mean.  
\(\beta_1\) measures the *difference* between the baseline and the category.

This concept generalizes: when multiple categories exist, coefficients always represent contrasts with the omitted category.
:::


## More categories → need multiple dummies

:::: {.columns}
::: {.column width="50%"}
- For k categories, create k–1 dummy variables.  
- Example: Region (East, South, West).  
  - East = baseline.  
  - South dummy = 1 if South, 0 otherwise.  
  - West dummy = 1 if West, 0 otherwise.  
- Coefficients measure differences from the baseline.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/dummy_variables3.png"
       alt="**Figure:** Table of region encoding into dummies">
  <figcaption>**Figure:** Table of region encoding into dummies</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: More categories → need multiple dummies

### Detailed Notes  
- Explain that with k categories, one must be left out to avoid perfect multicollinearity.  
- Walk students through the example and highlight that each dummy compares its category against the baseline.  
- Encourage thinking in terms of **group contrasts**, not absolute levels.

### Deeper Dive  
For regions East, South, West:
- East = baseline  
- Two dummies: South, West  

Model:
\[
y = \beta_0 + \beta_1 \text{South} + \beta_2 \text{West} + \epsilon.
\]

Interpretations:
- South vs East: \(\beta_1\).  
- West vs East: \(\beta_2\).

Dummies create a full set of contrasts without redundancy.
:::



## The dummy variable trap


:::: {.columns} 
::: {.column width="50%"} 
- For $k$ categories:  
  - Use **$k-1$ dummies**, not $k$.  
- If all $k$ are included → perfect multicollinearity.  
  - Model matrix becomes non-invertible.  
  - OLS cannot estimate coefficients.  
- Example: Region (East, South, West).  
  - Include **only 2 dummies**; baseline = East.  
::: 
::: {.column width="50%"} 
<figure>
  <img src="../materials/assets/images/dummy_variable_trap.png"
       alt="**Figure:** Table of region encoding into dummies">
  <figcaption>**Figure:** Table of region encoding into dummies</figcaption>
</figure>

::: 
::::


::: {.notes}
### Slide:: The dummy variable trap

### Detailed Notes  
- Teach this slide as a **structural modeling rule**, not a suggestion. Students often remember the rule (“use k–1 dummies”) but not *why* it’s required.  
- Begin by emphasizing the purpose of dummy variables: to encode categorical information numerically **without implying an order**.  
- Then pivot to the trap: when all \(k\) category indicators are included *along with* an intercept, they form a **perfectly collinear set**.  
- The key instructional point:  
  - The intercept already represents the baseline category.  
  - Adding a dummy for *every* category means the model is trying to estimate redundant parameters for the same group-level differences.  
- Use the Region example (East, South, West) to illustrate:  
  - If East = baseline, we only need “South” and “West” dummies. Including “East” as a dummy forces a linear dependency.  
- Pedagogically emphasize: this is not about model performance—it is about **mathematical solvability**. OLS literally cannot compute the coefficients because the design matrix becomes singular.  
- Close by reinforcing the practice: drop one category, call it the baseline, and interpret all other dummy coefficients relative to that category.

---

### Deeper Dive  
The dummy variable trap arises from **perfect multicollinearity**, a situation where one or more predictors can be written as an exact linear combination of the others. To understand why this breaks OLS, we need to examine the structure of the **design matrix** and the algebra underlying regression estimation.

#### 1. Algebraic Source of the Problem  
For a categorical variable with \(k\) categories, suppose we define dummy variables:
\[
D_1, D_2, \dots, D_k,
\]
where each \(D_j\) is 1 if an observation belongs to category \(j\), otherwise 0.

By construction:
\[
D_1 + D_2 + \dots + D_k = 1 \quad \text{for every observation.}
\]

If the model also includes an intercept column (a column of ones), then:
\[
\text{Intercept} = D_1 + D_2 + \dots + D_k.
\]

This creates **exact linear dependence** among columns of the design matrix \(X\).

OLS estimates coefficients via:
\[
\hat{\beta} = (X^\top X)^{-1} X^\top y.
\]

But if the columns of \(X\) are linearly dependent, \(X^\top X\) becomes **non-invertible** (singular). This means:
- The inverse does not exist.  
- Coefficient estimates are not uniquely defined.  
- Numerical routines either fail or return arbitrary parameterizations.

This is why the trap is mathematically fatal—not just inconvenient.

---

#### 2. Why Dropping One Dummy Resolves the Issue  
By removing one dummy variable (say, \(D_1\)), the model becomes:
\[
y = \beta_0 + \beta_2 D_2 + \dots + \beta_k D_k + \epsilon.
\]

Now:
- \(\beta_0\) becomes the expected value for the **baseline category** (category 1).  
- Each remaining coefficient \(\beta_j\) measures the difference between category \(j\) and the baseline.

The design matrix is now full rank because the columns are no longer linearly dependent.  
Mathematically:
\[
\text{Intercept} \neq D_2 + \ldots + D_k
\]
because \(D_1\) is implicit.

---

#### 3. Alternative Coding Schemes & Their Logic  
Multiple coding methods avoid the dummy trap while preserving interpretability:

- **One-hot encoding for ML models:** Same k–1 strategy, but many libraries handle the missing column automatically.  
- **Effect coding:** Uses −1 and +1 to compare categories to the grand mean.  
- **Orthogonal coding:** Creates uncorrelated predictors useful in ANOVA designs.

Even though these differ, each enforces **k–1 degrees of freedom** for a k-category variable.

---

#### 4. Why Perfect Multicollinearity Is a “Conceptual Error,” Not a Data Error  
Students sometimes think this is a software bug. It isn’t.  
The trap reveals:

- You are trying to estimate **more parameters than the data allow**.  
- The redundancy is not in the *data* but in the *parameterization*.  
- Regression is fundamentally about estimating contrasts between groups; including all dummies violates the logic of contrasts.

---

#### 5. Practical Implications in Modeling Pipelines  
- Most statistical software will automatically drop one dummy, but ML pipelines (like scikit-learn’s one-hot encoder with `drop='if_binary'` or `drop='first'`) require explicit instructions.  
- When interpreting results in business or research settings, analysts must know which category is the baseline; failure to do so leads to incorrect conclusions.  
- In interaction models, forgetting the hierarchical structure can accidentally recreate multicollinearity.

---

**Summary Insight:**  
The dummy variable trap is a *structural impossibility* of linear regression design matrices. Avoiding it is not optional—it's part of building a valid model. Dropping one dummy not only resolves the mathematics but also defines the interpretive framework for categorical variables in regression.
:::



# Polynomial Regression


## Polynomial regression extends linear models with transformed features

- Linear regression can be extended by adding polynomial terms of predictors.  
- Example (single variable):  
  $$
  y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \epsilon
  $$  
- This is still a **linear model in the parameters** $(\beta)$; only the inputs are transformed.  

::: {.notes}
### Slide:: Polynomial regression extends linear models with transformed features

### Detailed Notes  
- Teach this slide as a conceptual bridge between **linear models** and more flexible function approximators. Students often think linear regression is only about straight lines; this slide shows that its reach is broader.  
- Emphasize the key phrase: *linear in the parameters*. Even though the relationship between \(x\) and \(y\) may be nonlinear, the model remains linear in the \(\beta\)'s, meaning all the estimation machinery of OLS still applies.  
- Make explicit that polynomial regression works by **expanding the feature space**: instead of using only \(x\), the model uses \(x, x^2, x^3, \dots\).  
- Help students understand why this matters: polynomial terms allow regression to bend, curve, and adapt to nonlinear patterns while staying interpretable and mathematically tractable.  
- Pedagogically, prepare learners for the tradeoffs: this flexibility improves fit but increases risk of overfitting, which upcoming slides will address.  
- If teaching with software: note that tools like scikit-learn separate *feature construction* from *model fitting*, reinforcing the idea that polynomial regression is fundamentally a feature-engineering technique.

---

### Deeper Dive  
Polynomial regression leverages a core mathematical insight: **any smooth function can be approximated arbitrarily well by a polynomial**, given enough terms (Weierstrass Approximation Theorem). This makes polynomial models an important theoretical and practical step toward more flexible learning systems.

#### 1. Why polynomial regression is still “linear”  
Despite appearances, the functional form:
\[
y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \dots + \epsilon
\]
is linear in the coefficients \(\beta_j\).  
The transformed predictors \(x^2, x^3,\dots\) become new columns in the design matrix \(X\). Thus:
- The optimization landscape is convex.  
- Closed-form solutions via \((X^\top X)^{-1} X^\top y\) still exist.  
- Classical OLS inference remains valid (subject to assumptions on errors).

This distinguishes polynomial regression from nonlinear regression models in which parameters enter nonlinearly, requiring iterative optimization and risking multiple local minima.

---

#### 2. Feature space expansion and geometric interpretation  
In simple regression, data live along a one-dimensional predictor axis.  
With polynomial terms, each observation becomes a point in a **higher-dimensional feature space**:
\[
(x, x^2, x^3, \ldots, x^d).
\]

OLS finds a **hyperplane** in this expanded space. When this hyperplane is projected back onto the original \(x\)-axis, it produces a curved function.

This “lifting” of data into a richer feature space is a precursor to:
- Kernel methods  
- Spline regression  
- Neural networks  
- Generalized additive models (GAMs)

Polynomial regression is the simplest example of this powerful idea.

---

#### 3. Approximation power and limitations  
Polynomials can approximate complex functions but come with notable challenges:

- **Runge’s phenomenon:** High-degree polynomials oscillate wildly, especially near boundaries.  
- **Sensitivity to scaling:** Predictors often require standardization to avoid numerical instability.  
- **Global basis functions:** Changing one region of the \(x\)-domain affects the entire curve, unlike splines or local regressions.  

These challenges motivate smoother, more stable nonlinear modeling techniques used later in the course.

---

#### 4. Bias–variance implications  
As polynomial degree increases:
- **Bias decreases** — the model becomes more flexible and better fits nonlinear patterns.  
- **Variance increases** — the model becomes more sensitive to noise, leading to overfitting.  

Polynomial regression therefore becomes an ideal teaching tool for illustrating the **bias–variance tradeoff**, cross-validation for tuning polynomial degree, and the usefulness of regularization (e.g., Ridge, Lasso) in high-degree settings.

---

#### 5. Links to modern machine learning  
Polynomial regression is a conceptual ancestor of a wide class of models:  
- Neural networks use nonlinear transformations (activation functions) to build complex features.  
- Kernel methods (e.g., polynomial kernels) implicitly construct polynomial feature spaces.  
- Additive models construct nonlinear transformations of inputs while maintaining interpretability.

Understanding polynomial regression prepares students for these more advanced techniques because it illustrates how *nonlinearity can be created through deterministic feature transformations while maintaining linear estimators*.

---

**Summary insight:**  
Polynomial regression is the simplest and most interpretable way to move beyond straight lines. It keeps the mathematical elegance of linear models while demonstrating how expanding the feature space increases expressive power, introduces overfitting risks, and motivates the modern ML tools that follow.
:::




## Residual plots reveal nonlinearity

:::: {.columns}
::: {.column width="50%"}
- If relationship is nonlinear, residuals show patterns.  
- Example: 
  - Linear model: residuals show a U-shape.  
  - (this example from horsepower.csv)
- Indicates that a straight line misses systematic structure.  


:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/poly__residuals.png"
       alt="**Figure:** Residual plot from linear fit showing curved U pattern">
  <figcaption>**Figure:** Residual plot from linear fit showing curved U pattern</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Residual plots reveal nonlinearity

### Detailed Notes  
- Use this slide to reinforce one of the most important diagnostic principles in regression: **the residual plot tells you whether your model is capturing the true structure of the data**.  
- Emphasize that a *good* residual plot looks like **random noise**—no curves, no funnels, no patterns.  
- Highlight that when a linear model is fitted to a nonlinear relationship, the signal doesn’t disappear—it shows up in the **residuals** instead.  
- Draw attention to the U-shape in the plot: this pattern is the classic indicator that the true response curve bends while the regression line cannot.  
- Pedagogically, encourage students to always check residuals *before* adding polynomial terms or switching to complex models. Diagnostics should guide modeling—not the other way around.  
- Reinforce that residual plots are model-agnostic: these principles apply equally to ML models, generalized linear models, and non-linear regressions.

---

### Deeper Dive  
Residual diagnostics provide information about model adequacy by examining:
\[
e_i = y_i - \hat{y}_i.
\]
Under a properly specified linear model, these residuals should behave like i.i.d. noise:
- mean zero  
- constant variance  
- no structure  
- no relation to predictors  

When residuals depart from this ideal, they reveal **systematic information the model failed to capture**.

#### 1. Why nonlinear patterns appear in residuals  
Suppose the true relationship is:
\[
y = f(x) + \epsilon,
\]
with \(f(x)\) curved (e.g., quadratic).  
A fitted linear model approximates:
\[
\hat{y} = \beta_0 + \beta_1 x.
\]

Where the linear model underestimates \(f(x)\), residuals become **positive**;  
where it overestimates, residuals become **negative**.  
This produces the characteristic U-shape or inverted U-shape in residual-vs-fitted plots.

This is a direct illustration of the model failing to capture the true functional form.

---

#### 2. Why the U-shape matters  
The U-shape indicates:
- systematic under- and over-prediction  
- violation of the linearity assumption  
- violation of the “errors have mean zero given \(X\)” assumption  
- potential inefficiency of OLS  
- invalidity of inference (standard errors, t-tests, F-tests)  

In other words: **the model is wrong in form, not just imprecise**.

---

#### 3. Motivation for polynomial regression  
Polynomial regression explicitly adds curvature:
\[
y = \beta_0 + \beta_1 x + \beta_2 x^2 + \dots + \epsilon.
\]

By adding the squared term, the regression function can bend in the same direction as the underlying relationship.  
When done correctly:
- the systematic curvature in the residual plot disappears  
- residuals become approximately random  
- inference becomes valid again  
- predictive accuracy improves

This is a textbook example of “let the residuals tell you what the model needs.”

---

#### 4. Relationship to broader ML ideas  
Residual diagnostics anticipate deeper ML concepts:

- Nonlinearity → neural networks, kernels, splines  
- Model misspecification → bias–variance tradeoff  
- Underfitting → insufficient expressiveness of the function class  
- Feature engineering → creating nonlinear transformations manually  

Residual plots are effectively **low-tech interpretability tools** that reveal when the model’s representational capacity is inadequate.

---

#### 5. Practical caution  
While polynomial terms can correct simple curvature, excessive polynomial degree introduces:
- instability  
- interpretability challenges  
- boundary oscillations (Runge’s phenomenon)  
- increased variance  

Thus, the residual plot is not only a diagnostic tool—it is also a **warning system** that signals when a model is too simple and when care must be taken in adding complexity.

---

**Summary Insight:**  
A U-shaped residual pattern is the model’s way of telling you:  
*“The world is curved, and you’re forcing me to draw a straight line.”*  
Residual diagnostics are indispensable because they expose this mismatch directly and intuitively.
:::


## Polynomial regression can reduce residual patterns

:::: {.columns}
::: {.column width="60%"}
- Add squared term:  
  $$
  mpg = \beta_0 + \beta_1 \cdot horsepower + \beta_2 \cdot horsepower^2 + \epsilon
  $$  
- Residuals become more random.  
- $R^2$ increases (0.61 → 0.69).  
- Model captures curvature without overfitting.  

:::

::: {.column width="40%"}
<figure>
  <img src="../materials/assets/images/poly__degree_fits_short.png"
       alt="**Figure:** Residual plot after quadratic fit, showing no clear pattern">
  <figcaption>**Figure:** Residual plot after quadratic fit, showing no clear pattern</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Polynomial regression can reduce residual patterns

### Detailed Notes  
- Use this slide as a **before-and-after demonstration** of why polynomial regression is a principled fix for certain nonlinearities.  
- Emphasize that we are not adding polynomial terms “because they make the fit look nicer”—we add them because the **residual plot told us** the linear model was systematically wrong.  
- Walk through the example explicitly:  
  - The original linear regression underfit the true functional shape (U-shaped residuals).  
  - Adding a squared term lets the model bend appropriately.  
  - The residuals now appear random—exactly what we want.  
- Explain that an increase in \(R^2\) is expected but not the main justification. The real goal is a **better-specified model**, not merely a higher numeric score.  
- Pedagogically, remind students that model improvement must be matched by diagnostic improvement. If residuals still show structure, additional transformations or model classes may be needed.  
- Reinforce that this is one of the few cases where “visual inspection” directly drives model refinement.

---

### Deeper Dive  
Polynomial regression resolves systematic model misspecification by expanding the functional form of the regression. Understanding *why* the quadratic term fixes the pattern requires walking through the geometry, algebra, and assumptions of regression.

#### 1. Why quadratic terms fix U-shaped residuals  
If the true model is something like:
\[
f(x) = \theta_0 + \theta_1 x + \theta_2 x^2,
\]
and we fit only a linear model, the linear approximation will locally **cut across** the curved true function.

This produces:
- **positive residuals** at one end of the distribution,  
- **negative residuals** in the middle,  
- **positive residuals** at the other end.

That is precisely the U-shape we observed.

By adding \(x^2\), we allow:
\[
\hat{f}(x) = \beta_0 + \beta_1 x + \beta_2 x^2,
\]
which can approximate curvature. When the correct curvature is modeled, the systematic pattern disappears.

---

#### 2. Statistical interpretation: correcting the conditional expectation  
OLS estimates:
\[
E[y \mid x] = f(x),
\]
under the assumption that:
\[
E[\epsilon \mid x] = 0.
\]

But when the functional form is wrong:
- the linear model *violates* this assumption,  
- residuals become correlated with \(x\),  
- leading to biased estimates of linear coefficients,  
- invalid inference (incorrect standard errors),  
- and poor generalization.

Adding the missing term restores:
\[
E[\epsilon \mid x] = 0,
\]
and therefore stabilizes inference.

This is why the *residuals becoming random* is the real victory—not the change in \(R^2\).

---

#### 3. Why the increase in \(R^2\) matters but is not the main point  
Moving from \(R^2 = 0.61\) to \(0.69\) signals better model fit.  
But:

- \(R^2\) **always increases** when you add predictors, even useless ones.  
- Residual improvement is what validates the added term.  
- This helps distinguish **genuine model improvement** from **overfitting through parameter expansion**.

Thus, polynomial regression must always be paired with diagnostics.

---

#### 4. Avoiding overfitting with moderate polynomial degree  
Quadratic models (degree 2) often succeed because they introduce curvature without dramatically increasing model variance.

By contrast:
- Degrees 5, 6, … 20 can create oscillations and instability (Runge’s phenomenon).  
- Prediction variance increases exponentially with degree.  
- Boundary behavior becomes unreliable.  

Thus, quadratic terms are a **controlled way** to introduce flexibility.

Cross-validation provides a principled way to pick the optimal degree.

---

#### 5. Connection to basis expansion and general nonlinear modeling  
Polynomial regression is a simple case of a broader idea:
- Transform predictors into **basis functions**  
- Fit a linear model on those transformed features  

Examples of more advanced basis sets:
- splines  
- Fourier expansions  
- wavelets  
- kernels  
- neural network activations  

Polynomial regression is therefore a **gateway** to understanding richer nonparametric and semiparametric models.

---

**Summary Insight:**  
Adding a quadratic term works **not** because it raises \(R^2\), but because it repairs a broken assumption:  
> the model must capture the systematic structure in \(E[y \mid x]\).  
Residual diagnostics validate that the functional form is now appropriate and that the model’s errors behave like noise again.
:::


## Polynomial regression highlights the importance of visualizing model fits {.layout-two-content}

:::: {.columns}
::: {.column width="50%"}
- **Degree 1:** Straight line, underfits if relationship is nonlinear.  
- **Degree 2:** Parabola, good fit for quadratic relationships.  
- **Degree 10:** Highly wiggly curve, nearly passes through every data point → overfit.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/poly__degree_fits.png"
       alt="**Figure:** Data points with polynomial fits of different degrees">
  <figcaption>**Figure:** Data points with polynomial fits of different degrees</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Polynomial regression highlights the importance of visualizing model fits

### Detailed Notes  
- Use this slide as a moment to emphasize that **visual diagnostics are often more powerful than numerical summaries** when evaluating model flexibility.  
- Walk students through the three curves deliberately:  
  - **Degree 1:** A straight line—too rigid, misses curvature, produces systematic residual patterns.  
  - **Degree 2:** A smooth parabola that captures the essential shape—often the “sweet spot” between bias and variance.  
  - **Degree 10:** A highly flexible curve that hugs every wiggle in the data—a classic visual signature of **overfitting**.  
- Emphasize that simply looking at metrics like \(R^2\) is misleading because they *always* increase with model complexity.  
- Pedagogically, make the point that **visual inspection reveals the geometry of fit** in a way no single statistic can.  
- Reinforce that visualization should be a **routine modeling habit**, not a nice-to-have. It often reveals misspecification, overfitting, and model instability long before metrics do.

---

### Deeper Dive  
Polynomial regression provides a uniquely clear window into the **bias–variance tradeoff**, and visualization is the most intuitive way to understand this phenomenon.

#### 1. Why visualization matters more than metrics  
Numerical metrics such as \(R^2\), RMSE, or even cross-validation scores often compress model behavior into a single scalar. Scalars hide important structural information:

- Is the model capturing the correct **shape** of the relationship?  
- Does it extrapolate sensibly?  
- Is the model oscillatory or unstable at boundaries?  
- Does the fitted function respond appropriately to local trends?  

Plots provide this information immediately.

---

#### 2. Geometry of underfitting vs. appropriate fitting vs. overfitting  
Visual cues map directly to statistical properties:

- **Degree 1:** High bias; model cannot bend → underfitting.  
- **Degree 2:** Reasonable curvature; residuals show no major patterns → good fit.  
- **Degree 10:** Low bias but extremely high variance → overfitting.

The high-degree polynomial fits *every training point* because each additional term increases model flexibility. But this flexibility is dangerous:
- The curve between data points is unconstrained and unpredictable.  
- Boundary regions (edges of \(x\)) become especially unstable.  
- Slight changes in input lead to large swings in output—a hallmark of high variance.

---

#### 3. Mathematical intuition for why high-degree polynomials misbehave  
Polynomials of high degree create **global basis functions**: each term \(x^d\) influences the entire curve. This means:

- A small change at one end of the domain ripples across the whole function.  
- The function can oscillate to satisfy individual points (interpolation).  
- This behavior is especially pronounced with equally spaced points (Runge’s phenomenon).

This illustrates a key modeling insight:  
> More flexibility is not always better—local smoothness is often preferable to global wiggliness.

---

#### 4. Implications for real-world data science  
In applied settings, models that interpolate training data are often worse at prediction. Visualization helps detect:

- **Overly complex feature engineering**  
- **Excessive model flexibility** in tree ensembles or neural nets  
- **Boundary instability**, common in forecasting problems  
- **Failure modes in extrapolation**, which can be catastrophic in finance, healthcare, or operations settings  

Polynomial regression acts as a microcosm for all of these issues.

---

#### 5. Connection to modern ML techniques  
Visualization of fits sets the stage for understanding:

- Why **regularization** is needed  
- Why **splines, GAMs, and kernels** outperform global polynomials in practice  
- How neural networks build flexibility gradually through local transformations  
- Why **cross-validation** must guide complexity choices  

Even deep learning practitioners use learning curves and activation visualizations for the same reason: **model + diagnostic + visualization beats metrics alone**.

---

**Summary Insight:**  
Polynomial regression is more than a nonlinear trick—it is a teaching laboratory for understanding model complexity. Visualization exposes the underlying geometry of fit and makes the bias–variance tradeoff intuitive in a way no equation alone can accomplish.  
:::


## Adding polynomial terms increases flexibility but risks overfitting {.layout-two-content}

:::: {.columns}
::: {.column width="50%"}
- Higher-degree polynomials allow the model to fit data more closely.  
- Risk: The model captures noise rather than signal.  
- Trade-off:  
  - **Low degree:** Underfitting: model is too simple.  
  - **High degree:** Overfitting: model fits training data but fails to generalize.  
- Connects directly to the **bias-variance tradeoff**.  


:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/poly__degree_fits.png"
       alt="**Figure:** Data points with polynomial fits of different degrees">
  <figcaption>**Figure:** Data points with polynomial fits of different degrees</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Adding polynomial terms increases flexibility but risks overfitting

### Detailed Notes  
- Introduce this slide as the formal articulation of the **flexibility ↔ risk** dynamic students have been seeing visually.  
- Emphasize the intuitive idea: giving a model more “degrees of freedom” lets it bend and twist to fit the data more closely—but this same freedom can cause it to chase noise.  
- Reinforce that this is not about polynomial regression alone; polynomial models simply make the abstract concept **visible**.  
- Walk through the trade-off explicitly:  
  - **Low degree (1 or 2):** the model is too rigid and misses systematic curvature (**underfitting**).  
  - **Moderate degree (2–4):** often captures meaningful structure without becoming unstable.  
  - **High degree (10+):** the model fits individual fluctuations and training noise (**overfitting**).  
- Connect this directly to the **bias–variance tradeoff**, since students will revisit that concept repeatedly in later modules.  
- Pedagogically, highlight a key modeling mindset: *adding parameters always reduces training error—but that does NOT mean the model is improving.*

---

### Deeper Dive  
Polynomial regression provides a mathematically clean and visually intuitive way to illustrate the **bias–variance tradeoff**, one of the most fundamental principles in machine learning.

#### 1. Flexibility and the space of functions  
A polynomial of degree \(d\) belongs to a function class:
\[
\mathcal{F}_d = \{ \beta_0 + \beta_1 x + \dots + \beta_d x^d \}.
\]
As \(d\) increases:
- \(|\mathcal{F}_d|\) expands dramatically,  
- the function class becomes more expressive,  
- the model can approximate more complex shapes.

This creates a one-dimensional control knob for model capacity.

---

#### 2. Bias–variance tradeoff underlying polynomial degree  
Expected test error can be decomposed into:
\[
\text{Test Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}.
\]

Moving through polynomial degrees:

- **Low degree:**  
  - Model too simple  
  - High bias (systematic underestimation of curvature)  
  - Low variance  
  - Bad predictions because the hypothesis class can't express the true function  

- **High degree:**  
  - Model extremely flexible  
  - Low bias (fits training signal well)  
  - Very high variance (sensitive to random fluctuations)  
  - Bad predictions because the model memorizes noise rather than structure  

Polynomial regression gives students a tangible picture of this decomposition.

---

#### 3. Why high-degree polynomials overfit even when training error is low  
OLS chooses coefficients to minimize:
\[
RSS = \sum (y_i - \hat{y}_i)^2.
\]
As polynomial degree increases:
- RSS decreases monotonically—sometimes to nearly zero.  
- Training error becomes misleading as a proxy for model quality.  

High-degree models interpolate noise rather than capturing the underlying function.  
This is the essence of overfitting: **low training error but poor generalization**.

---

#### 4. Instability and boundary behavior  
High-degree polynomials are global basis functions.  
This means:
- A small change in one part of the predictor domain affects the entire fitted curve.  
- The model oscillates wildly near boundaries (Runge’s phenomenon).  
- Predictions become extremely unstable outside the range of the data.

This global instability is a major practical reason polynomial regression seldom uses degrees beyond 3–4 in real applications.

---

#### 5. Cross-validation and regularization as balancing mechanisms  
Choosing polynomial degree is a **model selection** problem.  
Tools include:
- **Cross-validation:** select the degree with lowest estimated test error.  
- **Regularization (Ridge, Lasso):** shrink or penalize high-degree terms to control variance.  
- **Splines:** replace global polynomials with local ones for better stability.

Polynomial regression is therefore a steppingstone to understanding:
- model complexity control,  
- smoothing techniques,  
- and the foundations of modern ML generalization theory.

---

#### 6. Broader ML implications  
The lesson from polynomial regression generalizes to:
- Neural networks (layer width/depth ↔ model variance)  
- Decision trees (depth ↔ flexibility)  
- Ensemble methods (overfitting in boosting)  
- Kernel methods (degree of polynomial kernels ↔ capacity)  

Polynomial degree is the simplest knob for understanding **capacity control**—which all advanced ML models must manage.

---

**Summary Insight:**  
Higher-degree polynomials demonstrate that **more flexibility is not inherently better**.  
The goal is not minimal training error but optimal *generalization*, which requires balancing model capacity with noise, sample size, and structure.  
Polynomial regression is an elegant vehicle for seeing this tradeoff unfold.
:::



## Flexibility improves fit in-sample but can harm generalization

- Flexible models often fit the training data better.  
- But too much flexibility reduces test performance.  
- Polynomial regression is a classic case of **flexibility vs. generalization**.  
 

::: {.notes}
### Slide:: Flexibility improves fit in-sample but can harm generalization

### Detailed Notes  
- Present this slide as the **conceptual climax** of the polynomial regression story: everything students have seen visually now ties into the central principle of generalization.  
- Emphasize that it is trivially easy for a model to look good on training data—flexibility guarantees this. What is hard (and what matters) is **performing well on unseen data**.  
- Reinforce that polynomial regression is a perfect teaching tool because it shows—in a single dimension—how flexible models can memorize noise rather than capture true structure.  
- Point out explicitly:  
  - Flexible models **reduce bias** but increase variance.  
  - In-sample fit improves monotonically as the model becomes more complex—but test error follows a U-shape.  
- Pedagogically, stress that students must internalize this distinction: *training error is not the objective of machine learning; generalization error is.*  
- Connect forward: this tension motivates regularization (Ridge/Lasso), cross-validation, neural network architecture choices, and almost every advanced ML method.

---

### Deeper Dive  
The statement “flexibility improves in-sample fit but can harm generalization” is the foundation of modern machine learning theory. Polynomial regression makes this principle tangible, but its implications reach across nearly all model classes.

#### 1. Why flexible models always improve training fit  
For a model class \(\mathcal{F}\) indexed by complexity (e.g., polynomial degree \(d\)), the training error satisfies:
\[
\text{TrainingError}(d+1) \leq \text{TrainingError}(d).
\]
This is a mathematical identity: adding parameters cannot increase RSS because OLS chooses the optimal coefficients within the expanded function class.

This leads to a dangerous illusion:  
*Lower training error does not mean a better model.*

#### 2. Expected test error and the U-shaped curve  
Expected generalization error decomposes into:
\[
\mathbb{E}[(y - \hat{f}(x))^2] = \text{Bias}^2 + \text{Variance} + \sigma^2.
\]

As flexibility increases:
- Bias decreases  
- Variance increases  

The test error curve therefore follows a U-shape:
- Too simple (high bias): underfitting  
- Just right: optimal capacity  
- Too complex (high variance): overfitting  

Polynomial regression visualizes this curve because degree directly controls model complexity.

#### 3. The formal role of variance  
High-degree polynomial models vary dramatically with small changes in:
- training samples  
- noise realizations  
- predictor values (especially near boundaries)  

Mathematically, the variance of the fitted function:
\[
\text{Var}[\hat{f}(x)]
\]
increases rapidly with model degree, making predictions unstable.

#### 4. Why in-sample fit is a poor indicator of model quality  
The key conceptual error beginners make is assuming that a visually perfect fit implies a good model. But high in-sample accuracy often reflects **memorization**, not learning.

Memorization destroys:
- generalization  
- interpretability  
- robustness to noise  
- stability across samples  

This underscores why the train/test split and cross-validation exist—to estimate generalization error, not training performance.

#### 5. Broader consequences for ML  
This flexibility–generalization tension appears everywhere in machine learning:
- **Decision trees:** deeper trees overfit; pruning reduces variance  
- **Neural networks:** more layers/parameters → better training fit but risk of overfitting  
- **Boosting:** too many rounds → perfect training accuracy, degraded test performance  
- **Kernel methods:** high-degree polynomial kernels behave like high-degree polynomials  
- **Nonparametric methods:** KNN with \(k=1\) has zero training error and terrible generalization  

Understanding this principle prepares students for:
- cross-validation  
- regularization  
- early stopping  
- structural risk minimization  
- model selection criteria (AIC/BIC)  
- ensemble stability analysis  

#### 6. Why polynomial regression is the ideal teaching example  
Polynomial degree gives a **single, intuitive knob** for complexity.  
This simplicity allows students to:
- see bias–variance tradeoff clearly  
- diagnose overfitting visually  
- experiment with capacity control  
- link theory to practice through simulation  

In higher dimensions and more complex models, the same logic applies—but is harder to visualize.

---

**Summary Insight:**  
Flexibility is a double-edged sword.  
- It **guarantees** better training fit.  
- It **threatens** generalization without validation, diagnostics, and regularization.  

Polynomial regression teaches this lesson in its most vivid and elegant form.
:::




# Parametric vs. Nonparametric


## Parametric vs. nonparametric regression

- **Parametric methods** (e.g., OLS):  
  - Assume a functional form (line, polynomial).  
  - Estimate a small set of parameters.  
  - Pros: interpretable, efficient with limited data.  

- **Nonparametric methods** (e.g., k-nearest neighbors, trees):  
  - Make minimal assumptions about form of \(f(X)\).  
  - Fit flexible shapes directly from data.  
  - Pros: capture complex patterns. Cons: need more data.  


::: {.notes}
### Slide:: Parametric vs. nonparametric regression

### Detailed Notes  
- Use this slide to create a **conceptual map** of the modeling landscape. Students often encounter “parametric” and “nonparametric” as vague buzzwords—this is where you make the distinction operational and meaningful.  
- Emphasize that parametric methods begin by **assuming a specific functional form** for \(f(X)\). This imposes structure, which simplifies estimation, enables interpretation, and requires fewer data points.  
- Clarify that nonparametric methods **do not specify a functional form upfront**. Instead, they allow the data to determine the shape of \(f(X)\). This flexibility is powerful but demands more data to avoid overfitting.  
- Give students mental anchors:  
  - OLS ≈ “fit a line or low-degree curve.”  
  - KNN ≈ “fit the data locally, point-by-point.”  
  - Trees ≈ “partition the predictor space to approximate \(f\).”  
- Pedagogically, stress that neither approach is always better. The choice depends on:  
  - sample size,  
  - dimensionality,  
  - interpretability needs,  
  - noise level,  
  - and the complexity of the true underlying function.  
- Reinforce that this distinction reappears throughout the course—in smoothing, in ensemble methods, and even in neural networks.

---

### Deeper Dive  
The parametric vs. nonparametric distinction is foundational in statistical learning theory because it defines the **size and shape of the hypothesis space**—the set of functions the model can represent.

---

#### 1. Parametric models: fixed functional families  
Parametric models assume that the true relationship belongs to a restricted family of functions:
\[
\mathcal{F}_\theta = \{ f_\theta(X) : \theta \in \mathbb{R}^p \},
\]
where \(p\) is small and fixed (e.g., 2 parameters in simple linear regression).

Key implications:  
- **Efficiency:** With small \(p\), estimates converge quickly with limited data.  
- **Interpretability:** Coefficients have clear meanings (e.g., slope = “effect size”).  
- **Bias risk:** If the assumed form is wrong, the model is systematically mis-specified.  
- **Stability:** Parametric estimators tend to be low-variance.  

Examples include:  
- Linear and polynomial regression  
- Logistic regression  
- Generalized linear models  
- Low-degree splines  

The tradeoff is **bias vs. simplicity**.

---

#### 2. Nonparametric models: expanding function classes  
Nonparametric models posit a very large or effectively infinite hypothesis space:
\[
\mathcal{F} = \{ f : f \text{ is any smooth (or not-so-smooth) function} \}.
\]

Examples:  
- k-Nearest Neighbors (local averaging)  
- Decision trees and random forests  
- Kernel smoothers  
- Local polynomial regression  
- Gaussian processes  
- Neural networks (which can approximate any continuous function with enough parameters)

Key properties:  
- **Flexibility:** Can approximate highly nonlinear patterns.  
- **Variance risk:** Without regularization or sufficient data, they overfit.  
- **Data hunger:** Because hypothesis space is large, more observations are required to achieve reliable estimates.  
- **Local behavior:** Many nonparametric methods adapt to local structure (e.g., KNN, kernel regression).

---

#### 3. Capacity, degrees of freedom, and effective complexity  
Parametric models have **fixed degrees of freedom**; nonparametric models have **data-dependent degrees of freedom**.

Example:  
- KNN with \(k=1\) has ~\(n\) degrees of freedom—it memorizes the dataset.  
- Decision trees grow complexity as depth increases.  
- Neural networks expand representational capacity with more layers and neurons.

Thus, nonparametric methods adjust their effective complexity as sample size grows, enabling them to learn more detailed structure.

---

#### 4. The curse of dimensionality  
Nonparametric estimators degrade sharply as the number of predictors increases.

For KNN, the volume of a d-dimensional ball grows exponentially; points become “far apart,” and local neighborhoods become effectively empty.  
This makes nonparametric methods **fragile in high dimensions without massive data**.

Parametric methods, by contrast, do not require dense sampling—because the functional form restricts the space of possibilities.

This distinction explains why:
- Linear models work shockingly well on many business datasets.  
- Nonparametric models dominate image, text, and signal domains—where nonlinear structure is rich and abundant training data exist.

---

#### 5. Regularization blurs the boundary  
Modern approaches often create **semi-parametric** or **regularized nonparametric** hybrids:
- Splines and GAMs are flexible but structured.  
- Random forests are nonparametric but regularized via averaging and depth limits.  
- Neural networks are nonparametric but constrained by architecture, weight decay, dropout, etc.

In practice, the landscape is a continuum, not a binary.

---

#### 6. Choosing between parametric and nonparametric methods  
Good choices depend on:  
- **Sample size:** Small → parametric; large → nonparametric.  
- **Interpretability:** Parametric models shine.  
- **Dimensionality:** Parametric models scale better.  
- **True function complexity:** Nonparametric models capture nuance parametric models cannot.  
- **Noise level:** High noise conditions favor parametric stability.

Model selection is ultimately about matching **assumptions and flexibility** to the data and decision context.

---

**Summary Insight:**  
Parametric models gain efficiency by restricting the hypothesis space.  
Nonparametric models gain expressiveness by expanding it.  
The art of modeling lies in choosing the right balance of structure and flexibility to maximize generalization while respecting data constraints.
:::




## Linear regression vs. k-nearest neighbors

:::: {.columns}
::: {.column width="50%"}
- **Linear regression:**  
  - Performs well if the true relationship is (approximately) linear.  
  - Low variance, but potentially biased.  

- **KNN regression:**  
  - Predicts using the average of nearest neighbors.  
  - Very flexible, adapts to nonlinear patterns.  
  - High variance when \(k\) is small.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/pnp__knn_lr.png"
       alt="**Figure:** Linear regression line vs. KNN step function">
  <figcaption>**Figure:** Linear regression line vs. KNN step function</figcaption>
</figure>

:::
::::


::: {.notes}
### Slide:: Linear regression vs. k-nearest neighbors

### Detailed Notes  
- Use this slide to help students **contrast two fundamentally different modeling philosophies**:  
  - Linear regression → assume a structure and estimate a small number of parameters.  
  - KNN regression → assume almost no structure and let the data define the shape of \(f(x)\).  
- Emphasize that both models make tradeoffs involving bias, variance, interpretability, and data requirements—this slide is an ideal point to connect back to the **bias–variance framework**.  
- Walk through the visual carefully:  
  - The **blue line** represents linear regression: a single global relationship fitted through all points.  
  - The **orange step function** represents KNN with \(k=1\): perfect interpolation of training data but catastrophic generalization risk.  
  - The **green KNN curve with \(k=9\)** shows how increasing \(k\) smooths the estimate, reducing variance and improving generalization.  
- Pedagogically, stress that models with no built-in structure (like KNN) rely heavily on **local density of data**. They excel when data are plentiful and the true relationship is nonlinear, but struggle in high dimensions or sparse settings.  
- Reinforce the comparative insights:  
  - Linear regression = **stable, interpretable, low data requirement, high bias**.  
  - KNN = **flexible, low bias, data-hungry, high variance**.  
- Set expectations: This comparison will motivate upcoming discussion on the curse of dimensionality and why nonparametric methods fail in high-dimensional business datasets.

---

### Deeper Dive  
This slide contrasts two opposite ends of the modeling spectrum in statistical learning. Understanding their differences illuminates generalization theory, geometric behavior in feature space, and why some methods succeed or fail depending on data structure.

---

#### 1. Function class and capacity  
**Linear regression** restricts \(f(x)\) to the space of linear functions:
\[
\mathcal{F}_{\text{LR}} = \{\beta_0 + \beta_1 x : \beta_0, \beta_1 \in \mathbb{R}\}.
\]
Low capacity → low variance → easier generalization with small samples, but potentially large **bias** if the true relationship is nonlinear.

**KNN regression** approximates:
\[
\hat{f}(x) = \frac{1}{k} \sum_{x_i \in \mathcal{N}_k(x)} y_i,
\]
where \(\mathcal{N}_k(x)\) is the neighborhood of the \(k\) closest points.  
Capacity depends on \(k\):  
- \(k = 1\): extremely high capacity → the model interpolates noise.  
- Large \(k\): lower capacity, more stable, but may reintroduce bias.

---

#### 2. Bias–variance decomposition  
These two methods illustrate the poles of the bias–variance tradeoff:

- **Linear Regression**  
  - High bias: cannot capture curvature.  
  - Low variance: predictions change little between samples.  
  - Works surprisingly well when data are scarce or noisy.

- **KNN**  
  - Low bias: can approximate highly nonlinear functions.  
  - High variance: small changes in training data change neighborhoods and predictions.  
  - Requires abundant, dense data to stabilize.

This decomposition explains the visual differences between the smooth LR line and the jagged KNN curves.

---

#### 3. Geometry of prediction  
Linear regression relies on a **global fit**. Every prediction uses all data points.

KNN relies on a **local geometry**:  
- Predictions depend only on points in the neighborhood.  
- Local density, scaling of features, and distance metric choice all materially affect predictions.  
- KNN’s performance degrades drastically in high dimensions because all points become far apart—this is the **curse of dimensionality**.

This geometric perspective is essential for understanding why KNN excels in low-dimensional settings but fails in most tabular business datasets.

---

#### 4. Interpretability and model transparency  
Linear regression is fully transparent:
- Coefficients have direct meanings.  
- Diagnostics reveal structure (residual plots, influence measures).  

KNN is **opaque**:  
- No explicit model form.  
- No coefficients to interpret.  
- Local neighborhoods can shift unpredictably.

This interpretability gap is one of the major practical reasons linear regression remains foundational despite its simplicity.

---

#### 5. Computational implications  
Linear regression is computationally light:
- Closed-form solutions exist.  
- The design matrix size—not the number of predictions—dominates computation.

KNN is computationally heavier:
- Predictions require distance computations to *every* training point.  
- In large datasets, storing and querying neighbors can be expensive.  
- Specialized algorithms (KD-trees, Ball trees, FAISS) exist but degrade in high dimensions.

This illustrates the tradeoff between **training cost** (LR) and **prediction-time cost** (KNN).

---

#### 6. Robustness to noise  
KNN with small \(k\) copies the noise in the training data → high variance.  
Linear regression averages over all errors → more robust.

This distinction is vital in messy real-world settings with measurement error or reporting noise.

---

**Summary Insight:**  
Linear regression and KNN illustrate two extremes:  
- **LR:** strong assumptions, high stability, lower data requirement.  
- **KNN:** minimal assumptions, high flexibility, strong dependence on sample size and dimensionality.  

Understanding this spectrum prepares students for the richer landscape of modern ML, where all models navigate the same tension between **assumptions, flexibility, data requirements, and generalization**.
:::




## The curse of dimensionality

:::: {.columns}
::: {.column width="50%"}
- In high dimensions, “neighbors” are far apart.  
- KNN suffers: each test point has few truly close neighbors.  
- Linear regression less affected by dimensionality.  
- Practical lesson:  
  - **Use LR as baseline.**  
  - Nonparametric methods require much larger datasets.  
:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/curse_of_dimensionality.png"
       alt="**Figure:** Illustration of sparse neighbors in high-dimensional space">
  <figcaption>**Figure:** Illustration of sparse neighbors in high-dimensional space</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: The curse of dimensionality

### Detailed Notes  
- Teach this slide as the **central limitation** of nonparametric and distance-based methods in modern ML. Students often hear the phrase, but this is where they learn what it *means* in practice.  
- Begin with the geometric intuition: as the number of predictors increases, points in the feature space spread out—everything becomes far from everything else.  
- Emphasize that KNN (and many other nonparametric methods) rely on the idea that “nearby” points exist and are informative. In high-dimensional spaces, this breaks down.  
- Explain why linear regression is less affected: it does not rely on local neighborhoods; it imposes a global functional form, which allows it to behave more stably even when predictors increase.  
- Pedagogically, stress the practical lesson:  
  - Use **linear regression as a strong baseline** in tabular data with moderate sample sizes.  
  - Apply nonparametric methods only when you have enough data density to support meaningful local structure.  
- Reinforce that the curse of dimensionality is not a weakness of KNN alone—it affects *all* local, distance-based, and nonparametric methods.

---

### Deeper Dive  
The curse of dimensionality is not a single phenomenon but a collection of geometric, statistical, and computational effects that become severe as dimensionality grows. Understanding it at a deeper level provides crucial insight into why some ML models thrive in high dimensions and others collapse.

---

#### 1. Volume grows exponentially with dimension  
Consider a unit hypercube. Its volume in \(d\) dimensions is:
\[
V = 1^d = 1.
\]
But to capture a fixed fraction of the space (say, 1%), a “ball” of radius \(r\) must grow rapidly as \(d\) increases.  

Formally, the volume of a \(d\)-dimensional ball is:
\[
V_d(r) = \frac{\pi^{d/2} r^d}{\Gamma(d/2 + 1)}.
\]

To capture the same proportion of space, \(r\) increases → meaning neighbors get farther and farther away.  
**Locality breaks down.**

---

#### 2. Nearest neighbor distances converge to far distances  
In high-dimensional data, the ratio:
\[
\frac{\text{distance to nearest neighbor}}{\text{distance to farthest neighbor}}
\]
approaches **1**.  
This means:
- The nearest neighbor is no longer “meaningfully closer” than a random point.  
- KNN loses its local smoothing advantage entirely.

This is why KNN performance deteriorates catastrophically in high dimensions.

---

#### 3. Data sparsity becomes structural  
With fixed sample size \(n\), the density of data points plummets as dimensions grow.  
To maintain constant density, you need:
\[
n \propto c^d
\]
for some constant \(c > 1\).  
This is exponential growth—completely infeasible in practice.

Nonparametric methods need *massive* datasets to avoid undersmoothing in high-dimensional spaces.  
Linear regression needs only enough observations to estimate parameters reliably.

---

#### 4. Linear regression’s “dimensional immunity”  
Linear regression does not rely on neighborhoods or local structure.  
It assumes:
\[
E[y \mid X] = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p.
\]

Thus, even in 50 or 100 dimensions:
- Linear models remain stable.  
- Variance increases only because of parameter estimation, not because geometry collapses.  
- Regularization (Ridge/Lasso) further stabilizes linear models.

This explains why linear and generalized linear models remain dominant in business analytics, economics, and applied social science.

---

#### 5. Beyond KNN: who else suffers?  
The curse affects many methods:

- Kernel regression (bandwidth must grow with dimension).  
- Density estimation (dimension exponentially increases required samples).  
- Clustering (distance-based clusters become meaningless).  
- Tree methods (splits become sparse; depth must grow excessively).  
- Gaussian processes (kernel length scales become hard to tune).

Even sophisticated models like neural networks suffer unless data are extremely abundant.

---

#### 6. Where the curse doesn’t hurt (as much)  
Some contexts naturally have effective low-dimensional structure, making nonparametric methods feasible:
- Images → lie (approximately) on a low-dimensional manifold  
- Text embeddings → contextual compression  
- Recommender systems → latent factor models reduce dimension  
- Biological signals → often governed by a small number of latent processes

Dimensionality reduction is therefore a core technique to “beat the curse.”

---

#### 7. Practical implications  
- With **small to moderate datasets**, use parametric or regularized parametric models first.  
- Apply KNN or nonparametric regressions only when \(n\) is large and dimension is low.  
- Always consider preprocessing steps like PCA, autoencoders, or feature selection when using nonparametric methods.  

---

**Summary Insight:**  
In high dimensions, “local” neighborhoods stop being local, making nonparametric methods unreliable without enormous datasets.  
Linear regression—and other parametric models—remain stable because they restrict the hypothesis space rather than relying on geometric proximity.  
Understanding the curse of dimensionality is essential for making intelligent model choices in real-world ML.
:::


::: {.notes}
### Slide:: Why start with linear regression?

### Detailed Notes  
- Use this slide to cement linear regression as the **foundational baseline** for the entire modeling workflow—not because it is always the best model, but because it provides essential structure, interpretability, and diagnostic clarity.  
- Emphasize that linear regression is often the **first model you fit** in practice, even when you expect nonlinear relationships. It provides a clean, stable reference point for comparing more complex models.  
- Highlight the reasons listed on the slide:  
  - **Interpretability:** Coefficients, significance tests, diagnostics, and residual analysis create a transparent view of relationships.  
  - **Strong performance with limited data:** Parametric models shine when data are sparse, noisy, or expensive to collect.  
  - **Benchmarking:** Having a simple baseline is crucial—if a nonlinear or black-box model doesn’t beat linear regression, something is wrong.  
  - **Nonparametric caution:** Flexible models (KNN, trees, neural nets) require more data, tuning, and computational care. Starting simple avoids premature complexity.  
- Pedagogically, frame linear regression as a *modeling compass*: it helps you understand your data, evaluate assumptions, and orient yourself before exploring the richer ML landscape.

---

### Deeper Dive  
Linear regression’s foundational role in statistical learning theory and practice stems from its deep mathematical structure, its stability, and its tight connection to core ML ideas.

#### 1. Why linear regression is statistically efficient  
OLS estimators have desirable properties under mild assumptions:
- **BLUE (Best Linear Unbiased Estimator)** under Gauss–Markov  
- **Consistent** as \(n \to \infty\)  
- **Asymptotically normal** (enabling inference even with moderately sized samples)

The efficiency of OLS is tied to its low parameter dimensionality—only \(p+1\) parameters for \(p\) predictors. Nonparametric methods, by contrast, effectively estimate a function at *every point* in the predictor space, which requires far more data.

---

#### 2. Interpretability as an analytical advantage  
Linear regression provides:
- direct effect estimates (\(\beta_j\)),  
- clear conditional interpretations,  
- confidence intervals and hypothesis tests,  
- transparent diagnostics (residuals, leverage, influence),  
- simple communication to stakeholders.

Nonparametric and black-box models obscure these relationships, making it harder to reason about mechanism or causality without additional tools.

---

#### 3. Linear regression as a diagnostic engine  
You can use linear regression to:
- identify nonlinearities (via residual plots),  
- detect multicollinearity (via VIF),  
- assess outliers and leverage (studentized residuals, Cook’s \(D\)),  
- uncover heteroscedasticity and correlated errors.

Even if your final model is nonlinear, linear regression informs **what is going wrong structurally**, and where to look next. No nonparametric method offers such rich, built-in diagnostics.

---

#### 4. Baseline benchmarking: the machine learning philosophy  
In ML practice:
- The first question is not “What is the best model?”  
- The first question is “Can anything beat a linear model?”  

If a more complex method does *not* outperform linear regression:
- the data may not contain nonlinear signal,  
- the model may be mis-specified or under-tuned,  
- there may be leakage or preprocessing issues,  
- variance or regularization may be mismanaged.

Linear regression thus functions as a **sanity check** for the entire ML pipeline.

---

#### 5. Stability in high dimensions and small samples  
Unlike KNN, trees, and kernel methods, linear regression:
- does not rely on local neighborhoods,  
- remains stable in moderately high dimensions,  
- benefits strongly from regularization (Ridge, Lasso),  
- produces predictable variance and inference outcomes.

This robustness is one reason linear models dominate business analytics, econometrics, social science, health applications, and other structured-data domains.

---

#### 6. Linear regression as a conceptual gateway  
Much of modern ML generalizes or modifies linear regression:

- **Regularized linear models** (Ridge, Lasso, Elastic Net)  
- **Generalized linear models** (logistic, Poisson)  
- **Kernelized models** (SVMs, Gaussian processes)  
- **Neural networks** (linear layers + nonlinear activations)  
- **Additive models** (splines, GAMs)

In a very real sense, linear regression is the “template” for model architectures across ML.

---

**Summary Insight:**  
We start with linear regression not because it is simple,  
but because it is **interpretable, stable, theoretically grounded, diagnostically rich, and an essential baseline for evaluating every more flexible model that follows**.  
Mastering linear regression sets the foundation for the rest of the machine learning universe.
:::



# Regularization Techniques  


## Regularization balances bias and variance to prevent overfitting

- **Bias:** Error from oversimplification (underfitting).  
- **Variance:** Error from over-sensitivity to training data (overfitting).  
- Regularization adds a penalty on large coefficients to control complexity.  
- Visual intuition:  
  - High bias → predictions far from truth.  
  - High variance → fits noise in data.  
  - Regularization → smoother, generalizable model.  

::: {.notes}
### Slide:: Regularization balances bias and variance to prevent overfitting

### Detailed Notes  
- Introduce this slide as the **conceptual heart** of why regularization exists. Students have now seen underfitting (rigid models) and overfitting (too-flexible polynomials, KNN with small \(k\)). Regularization is the principled tool for navigating this tradeoff.  
- Start by clearly defining the two errors:  
  - **Bias:** systematic error from using an overly simple model.  
  - **Variance:** instability from fitting noise or idiosyncrasies in the sample.  
- Emphasize that neither extreme is desirable—regularization aims to find the “middle path.”  
- Explain that regularization works by **penalizing large coefficients**, effectively shrinking them toward zero. This lowers model variance, smooths predictions, and stabilizes generalization.  
- Pedagogically, note that regularization does *not* always improve training performance—it deliberately sacrifices in-sample accuracy for better out-of-sample performance.  
- Use the visual intuition cues to prepare students for Ridge, Lasso, and Elastic Net: regularization pulls the model toward simpler, smoother shapes without collapsing it back into naive underfitting.

---

### Deeper Dive  
Regularization is one of the most important ideas in modern statistical learning because it provides a mathematically and computationally elegant method for controlling model complexity. Its theoretical foundations span optimization, geometry, and Bayesian statistics.

---

#### 1. Formal bias–variance decomposition  
The expected prediction error for a regression estimator \(\hat{f}\) at a point \(x\) decomposes as:
\[
\mathbb{E}[(y - \hat{f}(x))^2] = \text{Bias}^2[\hat{f}(x)] + \text{Var}[\hat{f}(x)] + \sigma^2.
\]

- **Bias** increases when we restrict the model excessively (e.g., force it to be linear when nonlinear structure exists).  
- **Variance** increases when the model is too flexible (e.g., high-degree polynomial).  
- **\(\sigma^2\)** is irreducible noise.

Regularization explicitly reduces the **variance** term at the cost of slightly increasing **bias**—a tradeoff that usually results in lower overall prediction error.

---

#### 2. Optimization view: adding penalties to the loss function  
Traditional OLS minimizes:
\[
RSS = \sum (y_i - \hat{y}_i)^2.
\]

Regularized models add a penalty term:
\[
\min_\beta \; RSS + \lambda P(\beta),
\]
where \(P(\beta)\) encourages small or sparse coefficients.

Examples:
- Ridge: \(P(\beta) = \sum \beta_j^2\)  
- Lasso: \(P(\beta) = \sum |\beta_j|\)

The hyperparameter \(\lambda\):
- controls the strength of the penalty,  
- tunes the bias–variance balance,  
- must be selected via cross-validation.

This formulation transforms model fitting into a constrained optimization problem:
\[
\min RSS \quad \text{subject to} \quad P(\beta) \leq c.
\]

---

#### 3. Geometric intuition: coefficient shrinkage  
Regularization constrains the parameter space. Ridge regression shrinks coefficients toward the origin in an L2 “ball,” while Lasso shrinks them toward the corners of an L1 “diamond,” enabling sparsity.

The geometric shape of the constraint drives:
- smooth shrinkage for Ridge,  
- variable selection for Lasso.

This connection between geometry and statistical behavior is foundational for understanding how advanced ML models control complexity.

---

#### 4. Probabilistic (Bayesian) interpretation  
Regularization corresponds to imposing **priors** on coefficients:

- Ridge = Gaussian prior on \(\beta_j\)  
- Lasso = Laplace prior on \(\beta_j\)

Thus, regularization has a dual identity:
- **Frequentist perspective:** shrink coefficients to reduce variance.  
- **Bayesian perspective:** express prior beliefs that coefficients should be small.

This dual view helps unify statistical modeling frameworks and prepares students for Bayesian ML.

---

#### 5. High-dimensional advantage  
When \(p\) (number of predictors) approaches or exceeds \(n\) (sample size), OLS becomes unstable or undefined.  
Regularization:
- ensures a unique solution,  
- stabilizes estimation,  
- prevents explosive coefficient variance,  
- is essential for modern datasets with many correlated features.

This is particularly important in domains like:
- genomics,  
- marketing attribution,  
- text and NLP features,  
- large-scale behavioral or clickstream data.

---

#### 6. Regularization as the backbone of modern ML  
Regularization is not limited to linear models—it underlies virtually all powerful ML algorithms:

- **Neural networks:** weight decay (L2), dropout, early stopping  
- **Tree-based models:** pruning, leaf-size tuning  
- **Boosting:** shrinkage (learning rate)  
- **Support Vector Machines:** margin maximization ↔ regularization tradeoff  
- **Gaussian processes:** kernel bandwidths as implicit regularizers  

Understanding the regularization principle here prepares students for a unified view of how ML models manage complexity.

---

**Summary Insight:**  
Regularization is the mathematical mechanism that stabilizes learning.  
It deliberately trades a bit of bias for a dramatic reduction in variance, leading to smoother models that generalize far better.  
This is the foundation of Ridge, Lasso, Elastic Net, and many of the most important algorithms in modern machine learning.
:::




## Lasso regression uses an L1 penalty to shrink coefficients to zero {.layout-two-content}

:::: {.columns}
::: {.column width="50%"}
- Loss function:  
  $$
  \text{Loss} = \text{SSE} + \lambda \sum |\beta_j|
  $$  
- Effect: Some coefficients shrink to **exactly zero**.  
- Useful for **feature selection** in high-dimensional data.  
- Example: Predicting house prices with dozens of features → Lasso highlights the most important predictors.  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/reg__lasso_paths.png"
       alt="**Figure:** Coefficient shrinkage under Lasso regularization">
  <figcaption>**Figure:** Coefficient shrinkage under Lasso regularization</figcaption>
</figure>

:::
::::


::: {.notes}
### Slide:: Lasso regression uses an L1 penalty to shrink coefficients to zero

### Detailed Notes  
- Introduce this slide as the **first major example** of how regularization controls model complexity—not by smoothing coefficients (as Ridge does), but by enforcing *sparsity*.  
- Emphasize that Lasso’s defining characteristic is the **L1 penalty**, which exerts a constant “pull” toward zero. This geometric property is what makes Lasso capable of driving some coefficients to **exactly zero**, performing built-in feature selection.  
- Highlight the practical intuition: in many business or scientific settings with dozens or hundreds of predictors, only a handful truly matter. Lasso helps automatically identify those predictors by shrinking weaker effects away.  
- Use the path plot to illustrate visually how coefficients decrease as \(\lambda\) increases. Point out that many lines flatten at zero long before others, signaling which predictors drop out first.  
- Pedagogically, emphasize the workflow:  
  1. Fit Lasso across a range of penalties.  
  2. Use cross-validation to choose \(\lambda\).  
  3. Interpret which predictors survive.  
- Stress that Lasso is not simply “better Ridge”—it solves a different problem. It is ideal when you believe many predictors may be irrelevant or redundant.

---

### Deeper Dive  
Lasso (Least Absolute Shrinkage and Selection Operator) solves the optimization problem:
\[
\hat{\beta} = \arg\min_\beta \left\{ \text{SSE} + \lambda \sum_j |\beta_j| \right\}.
\]

Its distinctive behavior comes from the **geometry of the L1 norm** and the optimization landscape it creates.

---

#### 1. Geometry: why L1 induces sparsity  
The constraint region for Lasso is an **L1 ball**—a diamond-shaped polytope in coefficient space.  
The OLS loss contours are ellipses.  
Optimal solutions occur where an ellipse first touches the constraint region.

Because the corners of an L1 ball lie exactly on axes, many optimal solutions fall *on* those corners → **coefficients are forced to zero**.

By contrast:
- Ridge uses an L2 ball (a circle/sphere) → smooth shrinkage, no zeros.  
- Lasso uses a diamond → corners → sparsity.

This geometric insight is the foundational explanation for why Lasso performs variable selection.

---

#### 2. Optimization perspective: non-differentiable penalty at zero  
The absolute value penalty:
\[
|\beta_j|
\]
has a **sharp kink at 0**.  
This kink produces subgradients that can “trap” coefficients at exactly zero when the data do not strongly pull them away.

Thus, Lasso’s sparsity is not just conceptual—it arises mathematically from nondifferentiability.

---

#### 3. Model selection: Lasso as a continuous generalization of subset selection  
Classical subset selection attempts to find:
\[
\min_{\beta: \|\beta\|_0 \leq k} \text{SSE},
\]
but this is combinatorial and computationally expensive (requires checking \(2^p\) models).

Lasso approximates this by replacing the \(\ell_0\) constraint with an \(\ell_1\) constraint:
\[
\|\beta\|_1 \leq t.
\]

The L1 norm is convex → problem becomes tractable → Lasso offers a computationally efficient, gradient-based way to perform *approximate subset selection*.

---

#### 4. High-dimensional regimes: \(p \gg n\)  
When the number of predictors exceeds the number of observations, OLS is undefined and Ridge shrinks but does not select.  
Lasso:
- still yields a unique solution (under mild conditions),  
- often selects a manageable subset of predictors,  
- is able to generalize well with cross-validation,  
- provides interpretability in settings where no classical regression can even be estimated.

This makes Lasso invaluable for:
- genomics,  
- text features (bag-of-words),  
- high-cardinality categorical encodings,  
- marketing mix models,  
- sensor streams with many derived features.

---

#### 5. Conditions under which Lasso succeeds (or fails)  
Lasso works best when:
- predictors are not extremely correlated,  
- only a subset of predictors truly matter (sparsity assumption).

Lasso struggles when:
- predictors are highly collinear (tends to pick 1 arbitrarily),  
- many small but meaningful effects exist (Lasso overshrinks them),  
- prediction requires grouping correlated variables (Elastic Net is better).

Understanding these regimes prepares students for comparing Lasso to Ridge and Elastic Net.

---

#### 6. Interpretation of coefficient paths  
In a regularization path:
- As \(\lambda \to 0\): Lasso approaches OLS.  
- As \(\lambda \to \infty\): all coefficients shrink to zero.  
- Along the path: variables enter the model sequentially, providing insights about effect strength.

This path can be used for **variable ranking**, **feature importance**, and **model stability analysis**.

---

**Summary Insight:**  
Lasso is powerful not because it shrinks coefficients, but because it **selects** among them.  
It turns high-dimensional problems into interpretable models by combining statistical regularization, optimization geometry, and sparsity assumptions—all of which underpin modern ML practices.
:::


## Ridge regression uses an L2 penalty to shrink coefficients smoothly {.layout-two-content}

:::: {.columns}
::: {.column width="50%"}
- Loss function:  
  $$
  \text{Loss} = \text{SSE} + \lambda \sum \beta_j^2
  $$  
- Effect: Coefficients are shrunk toward zero but rarely eliminated.  
- Useful for **stabilizing models** when predictors are correlated (multicollinearity).  
- Example: Housing dataset with correlated features like “size in square feet” and “number of rooms.”  

:::

::: {.column width="50%"}
<figure>
  <img src="../materials/assets/images/reg__ridge_paths.png"
       alt="**Figure:** Coefficient shrinkage under Ridge regression">
  <figcaption>**Figure:** Coefficient shrinkage under Ridge regression</figcaption>
</figure>

:::
::::

::: {.notes}
### Slide:: Ridge regression uses an L2 penalty to shrink coefficients smoothly

### Detailed Notes  
- Use this slide to contrast Ridge with Lasso: instead of enforcing sparsity, Ridge enforces **smooth shrinkage** of all coefficients.  
- Emphasize the key intuition: Ridge is designed to handle **multicollinearity**, where predictors overlap in the information they provide.  
- Explain that the L2 penalty adds resistance to large coefficients; this stabilizes the model when the design matrix is nearly singular or ill-conditioned.  
- Highlight that Ridge rarely sets coefficients to zero—meaning it does *not* perform feature selection—but it **reduces variance** by discouraging large swings in coefficient estimates.  
- Use the coefficient path plot to illustrate smooth decay: the lines taper toward zero in unison but do not hit the axis exactly.  
- Pedagogically, stress that Ridge is often the better default than Lasso when predictors are highly correlated, because Lasso tends to arbitrarily choose one variable among correlated groups.  
- Reinforce practical workflow: use cross-validation to choose \(\lambda\), especially in settings with correlated features or limited data.

---

### Deeper Dive  
Ridge regression solves the optimization problem:
\[
\hat{\beta} = \arg\min_\beta \left\{ \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda \sum_j \beta_j^2 \right\}.
\]

The L2 penalty fundamentally alters the geometry, estimation properties, and bias–variance tradeoff of the model.

---

#### 1. Why L2 improves numerical stability  
In OLS, coefficients are estimated via:
\[
\hat{\beta}_{OLS} = (X^\top X)^{-1}X^\top y.
\]

When predictors are highly correlated, \(X^\top X\) becomes close to singular, causing:
- huge variance in \(\hat{\beta}\),  
- sensitivity to small perturbations in the data,  
- unstable predictions.

Ridge modifies this to:
\[
\hat{\beta}_{Ridge} = (X^\top X + \lambda I)^{-1} X^\top y.
\]

Adding \(\lambda I\):
- increases diagonal dominance,  
- ensures invertibility even when \(X^\top X\) is singular or ill-conditioned,  
- shrinks eigenvalues away from zero, dramatically stabilizing estimation.

This is why Ridge is *essential* when \(p > n\) or predictors are nearly collinear.

---

#### 2. Geometric intuition: L2 balls create smooth shrinkage  
Ridge constrains coefficients to lie within an L2 sphere:
\[
\sum_j \beta_j^2 \leq t.
\]

The constraint region is round, not angular, so the optimal solution rarely lies exactly on an axis.  
This is why:
- coefficients shrink continuously toward zero,  
- none are set precisely to zero,  
- groups of correlated predictors shrink together.

This collective shrinkage is a key difference from the axis-aligned L1 diamond used by Lasso.

---

#### 3. Eigendecomposition view: Ridge shrinks unstable directions  
Let:
\[
X^\top X = Q \Lambda Q^\top
\]
be the eigendecomposition.

OLS coefficients can be expressed as:
\[
\hat{\beta}_{OLS} = Q \Lambda^{-1} Q^\top X^\top y.
\]

If \(\Lambda\) has small eigenvalues (high collinearity), those directions contribute disproportionately large, unstable components.

Ridge modifies this to:
\[
\hat{\beta}_{Ridge} = Q (\Lambda + \lambda I)^{-1} Q^\top X^\top y.
\]

Thus:
- small eigenvalues are inflated by \(\lambda\),  
- unstable directions are suppressed,  
- coefficients in poorly identified directions shrink the most.

This eigenspace interpretation is core to understanding *why* Ridge combats multicollinearity.

---

#### 4. Bayesian interpretation of Ridge  
Ridge corresponds to a **Gaussian prior**:
\[
\beta_j \sim \mathcal{N}(0, \tau^2),
\]
with \(\lambda = \sigma^2/\tau^2\).

Implications:
- shrinkage reflects prior belief that coefficients should be small,  
- larger \(\lambda\) = stronger belief in small coefficients,  
- provides a probabilistic rationale for shrinkage.

This duality deepens theoretical understanding and prepares students for Bayesian regression and Gaussian process models.

---

#### 5. Prediction vs. interpretability  
Ridge is ideal for:
- stabilizing predictions in noisy or collinear settings,  
- improving cross-validation performance,  
- high-dimensional modeling.

But Ridge complicates interpretability because:
- coefficient magnitudes depend on \(\lambda\),  
- shrinkage attenuates effect sizes,  
- multicollinearity obscures individual predictor importance.

This is why Ridge is often paired with methods like:
- standardizing predictors,  
- looking at prediction performance rather than coefficient magnitude,  
- using Elastic Net for grouped selection behavior.

---

#### 6. Ridge vs. Lasso vs. Elastic Net  
Clarify the differences:

| Method | Penalty | Behavior | Best for |
|-------|---------|----------|----------|
| **Ridge** | L2 | Smooth shrinkage; no zeros | Multicollinearity, \(p>n\), prediction stability |
| **Lasso** | L1 | Sparse solutions; zero coefficients | Feature selection, high-dimensional sparse problems |
| **Elastic Net** | L1 + L2 | Hybrid | Correlated features + sparsity |

This table helps students understand why Ridge is often the best choice when predictors are correlated.

---

**Summary Insight:**  
Ridge regression is the go-to method for stabilizing linear models in the presence of multicollinearity or high dimensionality.  
Its L2 penalty produces smooth shrinkage, improves numerical stability, reduces variance, and delivers robust predictions—even when OLS breaks down.  
It is a foundational technique in both classical statistics and modern machine learning.
:::


## Elastic Net combines L1 and L2 penalties for balanced regularization 

- Loss function:  
  $$
  \text{Loss} = \text{SSE} + \lambda_1 \sum |\beta_j| + \lambda_2 \sum \beta_j^2
  $$  
- Effect: Combines sparsity (L1) with stability (L2).  
- Works well when:  
  - Many correlated features exist.  
  - Only some predictors are expected to matter.  


::: {.notes}
### Slide:: Elastic Net combines L1 and L2 penalties for balanced regularization

### Detailed Notes  
- Teach this slide as the solution to the weaknesses of using **Lasso or Ridge alone**. Elastic Net is the “best of both worlds” when feature spaces are high-dimensional, noisy, or highly correlated.  
- Emphasize that Elastic Net uses **both** penalties:  
  - L1 → encourages sparsity (drops irrelevant predictors).  
  - L2 → stabilizes coefficients, especially when predictors are correlated.  
- Explain to students that in real datasets—especially business, marketing, financial, genomic, and NLP datasets—predictors often come in **groups that move together**.  
  - Lasso tends to pick *one* predictor arbitrarily and drop the others → unstable.  
  - Ridge keeps *all* predictors but does not perform feature selection.  
  - Elastic Net **keeps correlated predictors together**, and still performs feature selection.  
- Use the loss formulation to emphasize that there are *two hyperparameters* (\(\lambda_1, \lambda_2\)), which control the tradeoff between sparsity and smooth shrinkage.  
- Pedagogically, reinforce that Elastic Net is often the **default choice** in high-dimensional modeling unless there is a strong reason to prefer pure L1 or pure L2.

---

### Deeper Dive  
Elastic Net solves the regularized optimization problem:
\[
\hat{\beta} = \arg\min_\beta \left\{ \sum_{i=1}^n (y_i - \hat{y}_i)^2 + 
\lambda_1 \sum_j |\beta_j| + \lambda_2 \sum_j \beta_j^2 \right\}.
\]

The combination of penalties profoundly changes the estimator’s geometry, stability, and behavior.

---

#### 1. Geometry: hybrid constraint region  
Elastic Net’s constraint region is the intersection of:
- an **L1 ball** (diamond),  
- an **L2 ball** (circle/sphere).

The resulting feasible set has:
- corners (which promote sparsity),  
- rounded edges (which stabilize coefficient magnitudes).

This hybrid geometry makes Elastic Net:
- less aggressive than Lasso in zeroing coefficients,  
- more stable in the presence of correlated predictors.

---

#### 2. Grouping effect: why Elastic Net excels with correlated predictors  
When two predictors \(X_1\) and \(X_2\) are highly correlated:

- **Lasso** tends to pick one and discard the other → unstable model.  
- **Ridge** shrinks both but never eliminates either → no feature selection.  
- **Elastic Net** assigns *similar* coefficients to correlated variables (the “grouping effect”), while still being able to shrink irrelevant variables to zero.

This property is crucial for:
- marketing mix models (many correlated channels),  
- genomics (genes in co-expression pathways),  
- NLP (correlated word or token indicators),  
- financial modeling (co-moving economic indicators).

---

#### 3. Variants: the α-parameterization used in practice  
Most software uses:
\[
\text{Loss} = \text{SSE} + \lambda\left[ \alpha \sum |\beta_j| + (1-\alpha)\sum \beta_j^2 \right].
\]

Where:
- \(\alpha = 1\) → Lasso  
- \(\alpha = 0\) → Ridge  
- \(0 < \alpha < 1\) → Elastic Net

Thus:
- \(\lambda\) controls **overall** regularization strength,  
- \(\alpha\) controls the **mix** of L1 and L2 penalties.

Cross-validation typically tunes both.

---

#### 4. High-dimensional performance and consistency  
When \(p \gg n\), Elastic Net retains several desirable theoretical properties:
- Still provides a unique solution (unlike OLS).  
- Better variable-selection consistency than Lasso when predictors are correlated.  
- Lower estimation variance than 
:::




## Hyperparameters must be tuned with cross-validation to optimize regularization

- Regularization strength (\(\lambda\) or `alpha`) is a hyperparameter.  
- Use cross-validation to choose the best value.  

::: {.notes}
### Slide:: Hyperparameters must be tuned with cross-validation to optimize regularization

### Detailed Notes  
- Use this slide to emphasize that **regularization strength is not fixed**—it must be *chosen*, and the choice dramatically affects model behavior.  
- Clarify that hyperparameters like \(\lambda\) (or `alpha` in scikit-learn) control the **tradeoff between bias and variance**. Students must understand that these parameters are not learned from the data the way coefficients are—they must be tuned externally.  
- Emphasize that choosing \(\lambda\) by inspecting training error alone is meaningless because training error *always* decreases as regularization weakens.  
- Tell students the *only* reliable way to choose regularization strength is through **cross-validation**, where performance is evaluated on held-out folds that simulate unseen data.  
- Explain that in practice, tuning is systematic:  
  1. Choose a grid of possible \(\lambda\) values.  
  2. Fit the model on training folds.  
  3. Evaluate performance on validation folds.  
  4. Select the \(\lambda\) that yields the lowest mean validation error.  
- Prepare students for the idea that tuning always introduces computational cost—but is essential for achieving models that generalize well.

---

### Deeper Dive  
Regularization hyperparameters like \(\lambda\) (Ridge/Lasso/Elastic Net), `alpha` (mixing parameter in Elastic Net), or other penalty controls govern model complexity. They do *not* describe the data-generating process—they describe **how much we trust the model to fit complexity in the data**.  

Understanding how to tune these hyperparameters is foundational to supervised learning.

---

#### 1. Why hyperparameters cannot be learned from training error  
For Ridge, Lasso, and Elastic Net, the training error behaves monotonically with respect to \(\lambda\):
- As \(\lambda \to 0\), the model becomes OLS → training error decreases.  
- As \(\lambda \to \infty\), coefficients shrink → training error increases.  

Thus, minimizing training error will *always* choose \(\lambda = 0\), which defeats the entire purpose of regularization.  
This is why **training performance is an unreliable metric for tuning** hyperparameters.

---

#### 2. Cross-validation as a proxy for generalization error  
Cross-validation estimates:
\[
\mathbb{E}[(y_{\text{new}} - \hat{f}_{\lambda}(x_{\text{new}}))^2],
\]
which approximates true out-of-sample performance.

K-fold CV works by partitioning data into training and validation splits and ensures:
- Each observation is used for both training and validation.  
- Variance in the performance estimate is reduced.  
- Hyperparameters are optimized for *generalization*, not memorization.

This makes CV the statistical foundation of modern model selection.

---

#### 3. Regularization path and stability  
Regularized models define a **solution path** across \(\lambda\):  
- For Ridge: smooth coefficient shrinkage.  
- For Lasso: coefficients enter and leave the model in a piecewise-linear path.  
- For Elastic Net: hybrid behavior.

Cross-validation traverses this path, choosing the point where variance is controlled without overly inflating bias.

This method ensures model estimates are stable, especially in:
- high-dimensional data (\(p \gg n\)),  
- noisy datasets,  
- correlated predictor environments.

---

#### 4. Hyperparameters beyond \(\lambda\)  
In Elastic Net, the mixing parameter \(\alpha\) controls the balance of L1 and L2 penalties:
- \(\alpha = 1\) → Lasso behavior.  
- \(\alpha = 0\) → Ridge behavior.  
- \(0 < \alpha < 1\) → hybrid.

Hyperparameter tuning therefore requires optimizing a **two-dimensional search space** over \((\lambda, \alpha)\).  
Cross-validation handles this naturally, but computational cost increases.

In more complex models (trees, SVMs, neural nets), the number of hyperparameters can grow dramatically—yet **cross-validation remains the gold standard** for selecting them.

---

#### 5. Connection to statistical learning theory  
Choosing \(\lambda\) via cross-validation aligns with the principle of:
- minimizing **expected predictive risk**,  
- selecting models using **data-driven regularization**,  
- avoiding overfitting through *structural risk minimization*.  

Regularization and CV together embody the central ML pipeline:
- restrict the hypothesis space,  
- choose complexity using validation error,  
- fit the final model using all available data and the chosen hyperparameters.

---

#### 6. Practical considerations  
- Cross-validation requires more computation but yields dramatically more robust models.  
- Libraries like scikit-learn automate the procedure via `GridSearchCV`, `RandomizedSearchCV`, and coordinate descent optimizers.  
- In very large datasets, approximate CV or out-of-fold methods are used to scale tuning efficiently.  

Ultimately, hyperparameter tuning is not optional—it is what separates **modeling** from **curve-fitting**.

---

**Summary Insight:**  
Regularization strength determines how flexible or stable a model is.  
The only reliable way to choose this strength is **cross-validation**, which directly tunes the bias–variance balance to optimize generalization.  
This principle scales from linear models to the most advanced deep learning systems.
:::




## High-dimensional data: when \(p \geq n\)

- In some settings, predictors (\(p\)) ≥ sample size (\(n\)).  
- Examples:  
  - Genomics (10,000+ genes, hundreds of samples).  
  - Marketing (thousands of features, limited campaigns).  
- Problem:  
  - OLS cannot estimate unique coefficients.  
  - Model is **not identifiable**.  

::: {.notes}
### Slide:: High-dimensional data — when \(p \geq n\)

### Detailed Notes  
- Use this slide to highlight a **major structural problem** in modern data science: the number of predictors can easily exceed the number of observations. Students must understand this regime because it is increasingly common across domains.  
- Begin by explaining the intuition: if we have more features than samples, the model has *too many degrees of freedom*—it can fit the training data perfectly, but the solution is not unique, and generalization is disastrous.  
- Emphasize the failure mode of OLS: the normal equations require inverting \(X^\top X\), but when \(p \ge n\), that matrix is singular (or nearly singular). No unique solution exists.  
- Reinforce real-world examples: genomics, marketing attribution, digital advertising logs, NLP bag-of-words features, computer vision embeddings, and sensor data.  
- Pedagogically, stress that high dimensionality is not rare with modern feature engineering (dummy variables, interactions, polynomials). Students should know that OLS is NOT safe in these scenarios and must recognize the signs of identifiability failure.

---

### Deeper Dive  
High-dimensional data fundamentally challenges classical regression by breaking identifiability, inflating variance, and allowing models to interpolate noise. Understanding the mathematics behind this breakdown prepares students for modern regularization and sparsity-based modeling.

---

#### 1. Why OLS fails algebraically when \(p \ge n\)
OLS requires:
\[
\hat{\beta}_{OLS} = (X^\top X)^{-1} X^\top y.
\]

But when \(p \geq n\):
- The rank of \(X\) is at most \(n\).  
- \(X^\top X\) is at most rank \(n\).  
- This matrix cannot be full rank when \(p > n\).  
- Therefore, \((X^\top X)^{-1}\) **does not exist**.

The model is **underdetermined**: many (in fact infinitely many) \(\beta\) vectors give exactly the same fitted values \(\hat{y}\).

This is not a numerical issue—it’s a *mathematical impossibility*.

---

#### 2. Why perfect training fit is a red flag  
In high-dimensional regimes, there always exists a solution with **zero training error**:
\[
RSS = 0.
\]

This is simply because the model has enough degrees of freedom to pass a hyperplane exactly through all \(n\) points.  
But:
- The interpolating model generalizes poorly.  
- Predictions become unstable under tiny perturbations.  
- Coefficient magnitudes explode.  

This is a classic example of **overfitting due to high variance**.

---

#### 3. Variance explosion in high dimensions  
Even when a solution exists, coefficient estimates are wildly unstable.  
If \(v\) is an eigenvector corresponding to a small eigenvalue of \(X^\top X\), the OLS solution along that direction is:
\[
\hat{\beta} = \hat{\beta} + c v
\]
for any constant \(c\).  
This means:
- small data changes → large coefficient changes,  
- no stable interpretation or inference is possible,  
- predictions become extremely sensitive.

This is why high-dimensional OLS is unusable.

---

#### 4. Connection to the interpolation threshold in modern ML  
Recent theory shows that many models—including neural networks—operate in regimes where parameters exceed sample size.  
High-dimensional linear regression is the simplest case of the **double descent** phenomenon, where test error first increases (due to variance blow-up) and then decreases again as model capacity grows even further.

Understanding the \(p \ge n\) case provides intuition for:
- why over-parameterized neural networks can still generalize,  
- why explicit regularization is necessary,  
- how model capacity interacts with noise.

---

#### 5. How regularization rescues the high-dimensional case  
Ridge regression:
\[
\hat{\beta}_{Ridge} = (X^\top X + \lambda I)^{-1} X^\top y
\]
fixes singularity by adding \(\lambda I\), ensuring the matrix is invertible even when \(p > n\).

Lasso:
\[
\min_\beta \; RSS + \lambda \sum |\beta_j|
\]
produces sparse solutions, effectively reducing dimensionality.

Elastic Net:
- handles correlated high-dimensional features and provides both stability and sparsity.

Thus, **regularization is not optional** when \(p \ge n\). It is the only way to obtain:
- unique estimates,  
- stable predictions,  
- interpretable models,  
- generalization capacity.

---

#### 6. Practical examples of \(p \ge n\) pitfalls  
- Dummy variables for a categorical feature with many levels can push \(p\) past \(n\) instantly.  
- Interaction terms or polynomial terms grow combinatorially.  
- NLP text models (bag-of-words, n-grams) routinely have tens of thousands of predictors.

Students must be aware that feature engineering can *accidentally* create high-dimensional regressions—even from simple-seeming data.

---

**Summary Insight:**  
When \(p \ge n\), OLS collapses because the model is non-identifiable and infinitely flexible.  
Regularization is the mathematical necessity that restores stability, uniqueness, and generalization.  
High-dimensionality is not exotic—it is the default in many modern data problems, and understanding this regime is essential for responsible modeling.
:::




## Regularization solves the high-dimensional problem

- Penalties like Ridge, Lasso, Elastic Net:  
  - Shrink coefficients.  
  - Ensure unique solution even if \(p > n\).  
- Lasso: selects key features, drops the rest.  
- Ridge: spreads weight across correlated predictors.  
- Elastic Net: balance of both.  


::: {.notes}
### Slide:: Regularization solves the high-dimensional problem

### Detailed Notes  
- Use this slide to connect the previous discussion (“OLS fails when \(p \ge n\)”) to the **practical solution**: regularization.  
- Emphasize that in high-dimensional spaces, regularization is not merely a refinement—it is a **mathematical requirement** for obtaining stable, unique, and generalizable models.  
- Walk through each method conceptually:  
  - **Ridge** shrinks coefficients smoothly and stabilizes correlated predictors; useful when you want to keep all variables but reduce variance.  
  - **Lasso** performs variable selection by shrinking some coefficients exactly to zero, reducing dimensionality automatically.  
  - **Elastic Net** blends both properties, making it ideal when predictors are correlated *and* sparsity is desirable.  
- Highlight that each penalty changes the geometry of the optimization problem in ways that force uniqueness and stability even when the design matrix is not full rank.  
- Pedagogically, reinforce the workflow:  
  - (1) detect high-dimensional structure,  
  - (2) avoid OLS completely,  
  - (3) immediately switch to regularized models + cross-validation.  
- This slide marks a mindset shift: *regularization is the default in high-dimensional modeling.*

---

### Deeper Dive  
High-dimensional regression—where \(p\) approaches or exceeds \(n\)—breaks OLS due to non-invertibility, identifiability failures, and extreme variance. Regularization techniques modify the optimization landscape to restore well-posedness and control complexity.

---

#### 1. Why regularization restores uniqueness when \(p > n\)  
The core issue is that OLS requires:
\[
(X^\top X)^{-1} \quad \text{to exist}.
\]
When \(p \ge n\), \(X^\top X\) is singular.

**Ridge** adds \(\lambda I\):
\[
\hat{\beta}_{Ridge} = (X^\top X + \lambda I)^{-1} X^\top y.
\]
Since \(\lambda I\) is positive definite,  
\[
X^\top X + \lambda I
\]
becomes strictly invertible → **unique solution guaranteed**, even with \(p \gg n\).

Lasso and Elastic Net, through convex penalties, guarantee uniqueness (given mild design conditions) by creating **strictly convex** or **sufficiently regularized** objective functions.

---

#### 2. How shrinkage combats high-dimensional variance  
When \(p\) is large, the OLS estimator suffers from:
- huge parameter variance,  
- unstable predictions,  
- sensitivity to noise,  
- misleading inference.

Regularization constrains the solution space:
\[
\|\beta\|_2^2 \quad \text{or} \quad \|\beta\|_1,
\]
reducing variance by:
- dampening directions associated with small eigenvalues of \(X^\top X\),  
- suppressing noise-amplifying components,  
- smoothing or zeroing coefficients that cannot be estimated reliably.

Shrinkage therefore **counteracts the explosive variance** inherent in high dimensions.

---

#### 3. Feature selection vs. shrinkage vs. grouping  
Each regularization type solves a different aspect of the high-dimensional challenge:

- **Lasso**  
  - Performs variable selection (sparsity).  
  - Helps when only a subset of predictors matter.  
  - Reduces dimensionality to an interpretable subset.  
  - Struggles with correlated predictors.

- **Ridge**  
  - Does not select variables but stabilizes them through smooth shrinkage.  
  - Handles multicollinearity elegantly.  
  - Ideal for dense models where many predictors contribute small effects.

- **Elastic Net**  
  - Uses both L1 and L2 penalties.  
  - Encourages *grouping* effect: keeps correlated features together.  
  - Combines sparsity + stability, making it the best default for high-dimensional correlated predictors.

Thus regularization is not a single tool but a **family of strategies** designed for different shapes of high-dimensional problems.

---

#### 4. Connection to modern machine learning and deep learning  
Many ML systems operate with parameters far exceeding sample size:
- neural networks with millions of weights,  
- boosted trees with hundreds of depth-level splits,  
- kernel machines with implicit high-dimensional feature maps.

They remain trainable and generalizable *only because regularization is implicit or explicit*:
- weight decay (L2),  
- dropout (sparsifying mask),  
- early stopping (path-based regularization),  
- pruning and shrinkage in tree ensembles.

Understanding Ridge, Lasso, and Elastic Net gives students the conceptual foundation for grasping how the bias–variance tradeoff is managed in these advanced models.

---

#### 5. High-dimensional asymptotics: what theory says  
In the classical fixed-\(p\) setting, OLS is unbiased and efficient.  
But when both \(p\) and \(n\) grow, or when \(p > n\):
- OLS becomes inconsistent,  
- variance dominates bias,  
- prediction error can diverge.

Regularized estimators, by contrast:
- achieve consistency under sparsity (Lasso),  
- remain stable under correlation (Ridge),  
- adapt to both conditions (Elastic Net).

This shift from classical to **high-dimensional asymptotic theory** is one of the major revolutions in statistical learning.

---

#### 6. Practical implications for analysts and data scientists  
When you see:
- tens/hundreds/thousands of predictors,  
- heavy feature engineering (one-hot encoding, interactions),  
- multicollinearity indicators (VIF explosions),  
- small sample sizes relative to features,

you **must** move to regularization-based modeling.

OLS is not “suboptimal” here—it is *invalid*.

---

**Summary Insight:**  
In high-dimensional regression, regularization is the mechanism that restores identifiability, stability, interpretability, and generalization.  
Lasso selects, Ridge stabilizes, and Elastic Net harmonizes both—making regularization indispensable in modern applied machine learning.
:::




## Key takeaway: \(p \geq n\) changes the game

- **OLS fails**: too many predictors, not enough data.  
- **Regularization is required** for estimation and prediction.  
- Practical guidance:  
  - Always check \(p\) vs. \(n\).  
  - If \(p\) is close to or exceeds \(n\), skip OLS and go straight to Ridge/Lasso/Elastic Net.  

::: {.notes}
### Slide:: Key takeaway — \(p \geq n\) changes the game

### Detailed Notes  
- Present this slide as the **decision rule** students must internalize from the high-dimensional section.  
- Emphasize that once \(p\) approaches or exceeds \(n\), the modeling landscape fundamentally shifts: OLS is no longer just suboptimal—it is **mathematically invalid** or unstable.  
- Stress that analysts must **always compute or at least estimate** the relationship between the number of predictors and sample size before choosing a modeling strategy.  
- Explain that regularization is not a stylistic choice in this regime—it is a **requirement** for:  
  - identifiability,  
  - numerical stability,  
  - and generalization.  
- Pedagogically, note that this is the inflection point where classical linear models give way to modern statistical learning tools. Recognizing the high-dimensional regime early protects students against subtle modeling errors that are otherwise easy to miss.

---

### Deeper Dive  
In high-dimensional data settings, the statistical, computational, and geometric properties of regression shift dramatically. This slide encapsulates that shift and ties together fundamental concepts in modern machine learning.

---

#### 1. The identifiability breakdown of OLS  
When \(p \ge n\), the equation system \(X\beta = y\) is underdetermined:
- there exist infinitely many \(\beta\) vectors that perfectly fit the data,  
- OLS cannot select a unique solution,  
- \((X^\top X)^{-1}\) does not exist.

This is not a computational glitch—it's a structural impossibility.

Even when \(p\) is just “close to” \(n\), the matrix becomes ill-conditioned, causing:
- enormous variance,  
- unstable coefficients,  
- wildly fluctuating predictions,  
- meaningless inference.

Thus, OLS is unreliable *long before* \(p\) exceeds \(n\).

---

#### 2. Why regularization restores solvability and stability  
Regularization modifies the optimization problem so that:
- the matrix to be inverted becomes full rank (Ridge),  
- the solution becomes sparse and interpretable (Lasso),  
- correlated predictors are handled coherently (Elastic Net).

This transforms an ill-posed problem into a well-posed one.

Key idea: **Regularization restricts the hypothesis space**, forcing unique and stable solutions.

---

#### 3. High-dimensional asymptotics: when traditional statistics breaks  
Classical linear regression theory assumes:
- fixed \(p\),  
- \(n \to \infty\).

Modern ML datasets violate this constantly:
- \(p\) grows with \(n\),  
- or \(p\) grows *faster* than \(n\).

Under such conditions:
- OLS estimates become inconsistent,  
- variance dominates bias,  
- prediction error can diverge,  
- hypothesis testing breaks down entirely.

Regularization gives estimators that are:
- consistent under sparsity (Lasso),  
- stable under correlation (Ridge),  
- adaptable across regimes (Elastic Net).

This shift marks the emergence of high-dimensional statistical learning.

---

#### 4. Geometric intuition: volume and directions of instability  
In high dimensions:
- Predictors define a geometric space with many fragile, poorly sampled directions.  
- OLS amplifies noise along these unstable directions, producing huge coefficients.  
- Regularization dampens or eliminates these directions.

Thus, regularization serves as a **dimensionality stabilizer**.

---

#### 5. Practical modeling implications  
Students and practitioners must adopt the following rules:

- **Always check \(p\) vs. \(n\)** early in the workflow.  
  Even \(p = 0.3n\) can introduce instability if predictors are correlated.

- **Avoid OLS when \(p\) is large relative to \(n\).**  
  It yields misleading coefficient magnitude, incorrect inference, and unreliable predictions.

- **Use regularized models by default in wide datasets:**  
  - Ridge when predictors are correlated.  
  - Lasso when sparsity is expected.  
  - Elastic Net when both phenomena are present (the common case).

- **Combine regularization with cross-validation** to tune hyperparameters.

This is not optional—it is modern best practice in machine learning and applied regression.

---

**Summary Insight:**  
Once \(p\) approaches or exceeds \(n\), the classical OLS paradigm breaks.  
Regularization is the only path to stable, identifiable, and generalizable models in high-dimensional spaces.  
Recognizing this regime—and responding appropriately—is a foundational skill for any modern data scientist.
:::



# Evaluation Metrics for Regression  



## Different regression metrics capture different aspects of performance

- Metrics provide multiple perspectives on model performance.  
- The best metric depends on context (goals or scientific needs.)

::: {.notes}
### Slide:: Different regression metrics capture different aspects of performance

### Detailed Notes  
- Use this slide to shift students from *model building* to *model evaluation*. Stress that evaluating a regression model is multi-dimensional—there is no single “best” metric that works in all settings.  
- Emphasize that each regression metric answers a different question about model behavior. For example:  
  - **How large are errors?** (RMSE, MAE)  
  - **How bad are large errors?** (MSE, RMSE)  
  - **How much variation does the model explain?** (R²)  
  - **How consistent is prediction error across the range of outcomes?** (RSE, residual plots)  
- Encourage students to avoid over-reliance on one metric (especially R²). Many novices incorrectly assume R² alone reflects predictive performance.  
- Pedagogically, make evaluation metrics feel *purpose-driven*: choice depends on downstream decisions, domain considerations, and the cost structure associated with prediction errors.  
- Prepare students to learn each metric in the upcoming slides, and stress that they will need to interpret metrics **together**, not in isolation.

---

### Deeper Dive  
Regression evaluation metrics quantify different characteristics of prediction error and model fit. Understanding these distinctions is essential for selecting and defending a model in real-world analytical settings.

---

#### 1. Different metrics correspond to different loss functions  
In statistical learning, the metric often aligns with the loss function minimized during training:
- OLS minimizes **squared error**, making RMSE and MSE natural evaluative metrics.  
- Models trained with robust or L1-based losses align better with MAE.

This alignment ensures that evaluation reflects the model’s training objective—an important consideration when comparing models.

---

#### 2. Scale vs. scale-free metrics  
Some metrics depend on the scale of the target variable, while others do not:

- **Scale-dependent:**  
  - MSE  
  - RMSE  
  - MAE  
  - RSE  

- **Scale-independent:**  
  - R²  
  - Adjusted R²  
  - MAPE (with caveats)

Scale dependence affects interpretability and comparability:
- RMSE of 10 might be excellent in one domain and terrible in another.  
- R² allows comparison across different units or outcome variables but hides absolute error magnitude.

Understanding this difference is crucial when communicating results.

---

#### 3. Metrics emphasize different error characteristics  
- **MSE/RMSE** disproportionately penalize large errors because errors are squared. Useful when outliers are costly or when big mistakes dominate decision
:::




## Mean Squared Error penalizes large errors heavily 

- Definition:  
  $$
  \text{MSE} = \frac{1}{n} \sum (y_i - \hat{y}_i)^2
  $$  
- Properties:  
  - Squares errors → penalizes large mistakes disproportionately.  
  - Sensitive to outliers.  
- Example: A housing model that underestimates one mansion by \$1M gets heavily penalized.  

::: {.notes}
### Slide:: Mean Squared Error penalizes large errors heavily

### Detailed Notes  
- Present this slide as the natural starting point for discussing error-based metrics because MSE aligns directly with the **squared-error loss** minimized by OLS.  
- Emphasize that the defining characteristic of MSE is the **squaring of errors**. Students should internalize what this means practically: big mistakes count far more than small ones.  
- Encourage students to think about domains where this behavior is desirable—e.g., pricing mistakes on large homes, high-stakes forecasting—versus situations where it may distort evaluation because a single outlier dominates the metric.  
- Use the mansion example to highlight the asymmetry: a \$1M miss on one property can outweigh hundreds of small, accurate predictions.  
- Pedagogically, clarify that MSE should almost never be used in isolation—it's important, but not always aligned with business goals or risk preferences.

---

### Deeper Dive  
Mean Squared Error (MSE) is central to regression because it is mathematically convenient, statistically well-behaved, and tightly connected to OLS. But its deeper properties reveal why it is both powerful and potentially misleading.

---

#### 1. Squared error magnifies large deviations  
The MSE formula:
\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
\]
makes errors grow quadratically.  
If one prediction error is 10 times larger than another, it contributes **100 times** as much to MSE.

This penalizes:
- rare but catastrophic mistakes,  
- outliers in the data,  
- misfitting the tail of the distribution.

This behavior is valuable in settings where **big errors are extremely costly**, but problematic when outliers arise from noise or data-quality issues.

---

#### 2. Why OLS minimizes MSE  
OLS finds coefficients that minimize:
\[
\sum (y_i - \hat{y}_i)^2,
\]
the numerator of MSE.  

Squared error loss is differentiable, convex, and leads to:
- closed-form solutions for linear regression,  
- strong theoretical guarantees (BLUE, asymptotics),  
- convenient optimization properties.

Thus, MSE is the “natural” performance metric of OLS—but not always the best measure of predictive success.

---

#### 3. Bias–variance consequences  
Squared error loss implies that MSE can be decomposed into:
\[
\text{Bias}^2 + \text{Variance} + \sigma^2.
\]

This decomposition underlies:
- the bias–variance tradeoff,  
- regularization theory,  
- model selection criteria (AIC, BIC, Cp),  
- cross-validation strategies.

Large squared losses mean MSE is particularly sensitive to model variance—a reason flexible models often perform poorly on MSE unless regularized.

---

#### 4. Susceptibility to outliers  
Because MSE grows quadratically, even a single outlier can dominate the metric.  
For example:
- If 99 predictions err by \$5,000, and one errs by \$1,000,000, the outlier’s contribution dwarfs the rest.  
- MSE becomes more a measure of outlier severity than typical performance.

This vulnerability makes MSE inappropriate when:
- the outcome distribution is heavy-tailed,  
- outliers are frequent or due to measurement error,  
- robustness is desired (MAE is better suited).

---

#### 5. Relation to RMSE and interpretability  
MSE’s units are **squared**, making it difficult to interpret directly.  
Practitioners often use **RMSE**:
\[
\text{RMSE} = \sqrt{\text{MSE}},
\]
which is in the original units of \(y\).

However, RMSE preserves the squared-error sensitivity, so big mistakes still disproportionately influence the metric.

---

#### 6. Statistical optimality  
Under the classical linear model with normal errors:
- OLS is the **maximum likelihood estimator** (MLE),  
- Squared error loss is the **Bayes estimator** under a normal likelihood,  
- MSE is asymptotically optimal.

Understanding this connection shows why MSE is deeply embedded in statistical modeling—but also why it inherits the normal distribution’s sensitivity to extreme values.

---

**Summary Insight:**  
MSE is a mathematically elegant and widely used error metric, but the squaring of errors makes it exceptionally sensitive to large deviations. Its strength—penalizing catastrophic errors—is also its greatest weakness in noisy or heavy-tailed settings.  
It is essential to pair MSE with complementary metrics like MAE or RMSE and to evaluate models using domain-specific cost structures.
:::




## Root Mean Squared Error is interpretable in the same units as the target {.layout-two-content}

- Definition:  
  $$
  \text{RMSE} = \sqrt{\text{MSE}}
  $$  
- Properties:  
  - Expressed in same units as the target.  
  - Easier to interpret than MSE.  
- Example: RMSE = 15 → “On average, predictions are off by ~15 units.”  


::: {.notes}
### Slide:: Root Mean Squared Error is interpretable in the same units as the target

### Detailed Notes  
- Use this slide to help students understand **why RMSE is often preferred over MSE** in applied analytics. The key is interpretability: RMSE translates squared-error loss back into the original units of the response variable.  
- Emphasize that RMSE retains the same sensitivity to large errors as MSE, but presents the result in a form that stakeholders can immediately understand.  
- Highlight that RMSE is essentially the “typical prediction error” when using a squared-loss model, though technically it is the square root of the mean of squared deviations.  
- Encourage students to always contextualize RMSE (e.g., RMSE = 15 units might be excellent or terrible depending on the scale of \(y\)).  
- Use the example: RMSE = 15 → predictions typically deviate from true values by around 15 units—much more interpretable than MSE = 225.  
- Pedagogically, reinforce that RMSE is appropriate when the cost of large errors is high, and when squared-error training has been used.

---

### Deeper Dive  
Root Mean Squared Error (RMSE) is one of the most widely reported regression metrics because it balances mathematical convenience with interpretability. Its deeper properties reveal how it relates to MSE, optimization theory, error distributions, and decision-making.

---

#### 1. Mathematical relationship to MSE  
RMSE is defined as:
\[
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}.
\]

Thus:
- RMSE is the **L2 norm** of the residual vector scaled by \(\frac{1}{\sqrt{n}}\).  
- It inherits all properties of MSE except for the squared units.

The square root transforms a **quadratic error metric** into a **linear scale**, making it easier to compare directly to the magnitude of \(y\).

---

#### 2. Interpretability advantages  
Because RMSE is expressed in the units of the target variable, it provides immediate insight into:
- the scale of prediction errors,  
- how predictions may differ from actual values in practice,  
- whether model performance is meaningfully good or bad relative to domain expectations.

For communication with executives or practitioners, RMSE is often the most intuitive metric.

---

#### 3. Sensitivity to large errors remains  
RMSE still heavily penalizes large residuals. For example:
- An error of 10 contributes 100 to MSE.  
- An error of 100 contributes 10,000 to MSE.  

RMSE normalizes back to units of \(y\), but the influence of large deviations persists.  
Thus, RMSE:
- is appropriate when large errors matter,  
- is inappropriate when outliers distort performance evaluation.

In heavy-tailed or noisy domains, MAE may be superior.

---

#### 4. Statistical meaning: RMSE approximates the standard deviation of residuals  
If the model is correctly specified and errors are independent with variance \(\sigma^2\), then:
\[
\text{RMSE} \approx \sigma,
\]
i.e., the RMSE estimates the **standard deviation of the model’s errors**.

This connects RMSE to classical inference:
- Smaller RMSE → tighter prediction intervals,  
- Larger RMSE → wider uncertainty.

---

#### 5. RMSE and the Gaussian likelihood  
Under a normal error model:
\[
\epsilon \sim \mathcal{N}(0, \sigma^2),
\]
minimizing RMSE (or MSE) corresponds to maximizing the log-likelihood of the model.

Thus, RMSE is not merely convenient—it is **statistically principled** when errors approximate normality.

---

#### 6. RMSE in model comparison and tuning  
RMSE is widely used in:
- cross-validation,  
- hyperparameter tuning,  
- model selection,  
- forecasting competitions (e.g., Kaggle).  

But analysts must interpret it *relative* to:
- baseline models,  
- the variance of \(y\),  
- business tolerances.

---

**Summary Insight:**  
RMSE is a powerful metric because it preserves the desirable mathematical properties of squared error while translating results into the same units as the target variable.  
It provides an interpretable summary of prediction accuracy—especially when large errors matter—while remaining deeply connected to the statistical foundations of regression.
:::




## Mean Absolute Error is robust to outliers

- Definition:  
  $$
  \text{MAE} = \frac{1}{n} \sum |y_i - \hat{y}_i|
  $$  
- Properties:  
  - Applies a linear penalty, so outliers don’t dominate.  
  - Interpretable as the average absolute deviation.  
- Example: MAE = \$10,000 in housing → predictions are off by \$10,000 on average.  

::: {.notes}
### Slide:: Mean Absolute Error is robust to outliers

### Detailed Notes  
- Use this slide to contrast MAE with MSE/RMSE—students should understand *why* absolute-error metrics behave differently.  
- Emphasize that MAE applies a **linear penalty** to deviations, meaning errors contribute to the metric proportionally rather than quadratically.  
- Reinforce that this property makes MAE **robust to outliers**, which is especially important when the dataset includes extreme values (e.g., luxury homes, unusually large transactions, anomalous measurements).  
- Point out the interpretability advantage: MAE corresponds directly to a typical “absolute” deviation. Stakeholders often find this simpler to understand than squared-error metrics.  
- Remind students that MAE sometimes better supports decision-making when “average error” matters more than “penalizing extremes.”  
- Prepare them to recognize that different loss functions—squared vs. absolute—produce different optimal predictions and different models.

---

### Deeper Dive  
Mean Absolute Error (MAE) is a fundamental regression metric defined as:
\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|.
\]

Its deeper properties reveal why it offers robustness, interpretability, and distinct optimization behavior compared to MSE.

---

#### 1. Linear penalty → robustness  
Unlike MSE, which grows quadratically, MAE increases **linearly** with error magnitude.

If two errors differ by a factor of 10:
- MSE penalizes the larger error by a factor of 100.  
- MAE penalizes it by a factor of exactly 10.

This reduces the influence of:
- outliers,  
- heavy-tailed distributions,  
- measurement noise or data entry errors.

Because many real-world datasets are messy, MAE is often more *practically reliable* than MSE.

---

#### 2. Optimal predictions under MAE are medians  
A key theoretical distinction:

- Minimizing **MSE** yields the **mean** as the optimal constant predictor.  
- Minimizing **MAE** yields the **median** as the optimal constant predictor.

This reflects how MAE handles asymmetry and outliers:
- Means shift toward extreme values.  
- Medians remain stable even under large deviations.

Thus, MAE aligns with **median-based estimation**, a foundational idea in robust statistics.

---

#### 3. Different sensitivity across the distribution  
MAE weighs all deviations equally, so:
- errors at the center and errors in the tails contribute proportionally,  
- the metric emphasizes *typical predictive accuracy*,  
- it avoids the distorted perspective that squared errors produce when extreme values are present.

This makes MAE especially suitable for:
- operational forecasts,  
- inventory and demand prediction,  
- cost-sensitive applications with symmetric linear penalties.

---

#### 4. Loss surface is not differentiable at 0  
Because \(|x|\) is not differentiable at zero, MAE-based optimization:
- is more challenging for gradient-based algorithms,  
- requires subgradients or specialized solvers,  
- often converges more slowly than squared-error optimization.

This is one reason OLS (MSE-based) has attractive computational properties.

In deep learning, the Huber loss is often used to blend differentiability with robustness.

---

#### 5. Interpretability: a straightforward “average miss”  
Because MAE is in the same units as \(y\) and does not square them:
- It measures **typical prediction error** directly.  
- It is easy to explain to non-technical audiences.  
- It avoids exaggerating rare events.

For example:  
> MAE = \$10,000 means “on average, we miss by \$10,000.”

This clarity makes MAE a favorite in business and applied environments.

---

#### 6. When MAE is preferred over MSE/RMSE  
MAE is advantageous when:
- outliers are present or expected,  
- training data include noise or anomalous values,  
- squared penalty structure mismatches domain costs,  
- the goal is median-accuracy forecasting.

However, when large errors carry disproportionate cost (e.g., failing to detect major failures), squared-error metrics may be more appropriate.

---

**Summary Insight:**  
MAE is a robust, interpretable measure of typical predictive error.  
It avoids the outlier sensitivity of MSE and RMSE and aligns naturally with median-based reasoning.  
In many real-world settings, MAE provides a more faithful picture of model performance, especially when data imperfections or heavy tails are present.
:::




## R² measures the proportion of variance explained by the model {.layout-two-content}

- Definition:  
  $$
  R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}}
  $$  
- Range: 0 (no explanatory power) → 1 (perfect fit).  
- Caveats:  
  - Can be negative if the model is worse than predicting the mean.  
  - Does not always reflect predictive quality, especially in nonlinear contexts.  


::: {.notes}
### Slide:: R² measures the proportion of variance explained by the model

### Detailed Notes  
- Use this slide to clarify the *interpretive* role of R² rather than its predictive value. Students often overuse or misinterpret R², so emphasize that it is a **descriptive statistic**, not a universal measure of model quality.  
- Explain the formula intuitively: R² compares the model’s residual variation to the total variation in the outcome. If the model reduces a large portion of that variation, R² increases.  
- Clarify that R² can be negative—this is a sign the model performs worse than simply predicting the mean of \(y\).  
- Stress that R² does not penalize model complexity. Adding predictors always increases R² or leaves it unchanged, even if those predictors are useless.  
- Highlight that R² is not always related to predictive accuracy, especially in nonlinear or high-variance settings. A model with elegant explanatory fit may still perform poorly on unseen data.  
- Teach students when R² *is* useful: understanding variance decomposition, communicating effect magnitude to nontechnical audiences, and evaluating explanatory—not predictive—models.  
- Prepare them for adjusted R² (upcoming) which corrects for the inflation caused by adding predictors.

---

### Deeper Dive  
R², or the coefficient of determination, is defined as:
\[
R^2 = 1 - \frac{\text{SS}_{res}}{\text{SS}_{tot}},
\]
where:
- \(\text{SS}_{tot} = \sum (y_i - \bar{y})^2\),  
- \(\text{SS}_{res} = \sum (y_i - \hat{y}_i)^2\).

Despite its simplicity, R² has deep connections to variance decomposition, linear algebra, and inferential statistics—and subtle limitations students must understand.

---

#### 1. Variance decomposition interpretation  
In OLS with an intercept, we have the exact identity:
\[
\text{SS}_{tot} = \text{SS}_{reg} + \text{SS}_{res}.
\]

Thus,
\[
R^2 = \frac{\text{SS}_{reg}}{\text{SS}_{tot}}
\]
measures the fraction of total outcome variability explained by the model.  

This is a property of *linear projection* and does not guarantee predictive performance.

---

#### 2. Geometric interpretation  
In the geometry of least squares:
- \(y\) is a vector in \(\mathbb{R}^n\),  
- \(\hat{y}\) is its projection onto the column space of \(X\),  
- residuals \(e = y - \hat{y}\) lie in the orthogonal complement.

R² measures how much of the “length” of \(y\) is captured by its projection onto the model subspace.

Thus, R² is a **projection quality metric**, not a generalization metric.

---

#### 3. Why R² can be negative  
If the fitted model does worse than predicting \(\bar{y}\), then:
\[
\text{SS}_{res} > \text{SS}_{tot},
\]
so R² becomes negative.  
This often happens when:
- the model is misspecified,  
- the model has no intercept,  
- nonlinear structure is ignored,  
- small datasets produce unstable fits.

Negative R² is an early warning sign of model misalignment.

---

#### 4. R² is not comparable across models with different complexity  
R² *always increases* when predictors are added—regardless of whether they help.

Thus:
- R² is misleading for model selection,  
- comparing R² across models with different numbers of predictors is inappropriate,  
- this motivates **Adjusted R²**, AIC, BIC, and cross-validation.

A high R² does **not** mean the model is good; it may simply be overparameterized.

---

#### 5. R² does not measure predictive accuracy  
R² reflects explanatory power *in-sample*, not error magnitude or generalization.  
Examples:
- A model with R² = 0.95 may have RMSE that is unacceptably large in dollars.  
- A nonlinear relationship fit with a linear model may have a high R² but terrible predictive performance on new data.  
- In forecasting, R² is nearly always inferior to RMSE/MAE.

Thus, R² is **not a predictive metric** and should not be used to optimize or tune models.

---

#### 6. When R² is useful  
R² is valuable when:
- the goal is explanation, not prediction,  
- communicating proportional variance reduction,  
- comparing nested models under OLS,  
- conducting ANOVA-based decomposition in controlled experiments.

In business, economics, and social science, R² is often used for interpretive reporting rather than model selection.

---

#### 7. R² in nonlinear and nonparametric contexts  
R² is not guaranteed to behave sensibly when:
- the model is nonlinear,  
- the data undergo transformation,  
- predictions are shrunk or regularized (e.g., Ridge tends to lower in-sample R²).

In regularized models, a lower R² may accompany **better** generalization.

This reinforces: *R² is not a universal measure of model quality.*

---

**Summary Insight:**  
R² is a measure of variance explained—not accuracy, not generalization, and not model quality.  
It is useful for interpreting linear relationships but must be paired with other metrics such as RMSE, MAE, and cross-validation results to truly assess a model’s performance.
:::




## The right regression metric depends on your context

- **RMSE:** Common choice, interpretable, but sensitive to outliers.  
- **MAE:** Robust to outliers, useful when errors should be treated equally.  
- **MSE:** Good for optimization but less interpretable.  
- **R²:** Useful for communication but should not be used alone.  

::: {.notes}
### Slide:: The right regression metric depends on your context

### Detailed Notes  
- Use this slide to drive home the idea that **model evaluation is not one-size-fits-all**. Students should understand that choosing a regression metric is a *strategic decision* guided by domain needs, error costs, and communication goals—not habit or convenience.  
- Emphasize that different metrics highlight different characteristics of model performance:  
  - **RMSE**: emphasizes large errors; useful when big mistakes are costly.  
  - **MAE**: treats all errors equally; useful when robustness to outliers matters.  
  - **MSE**: mathematically convenient for optimization but hard to interpret.  
  - **R²**: intuitive for stakeholders but does *not* measure prediction accuracy.  
- Stress that selecting a metric requires thinking carefully about the structure of the problem and the consequences of predictive errors.  
- Encourage students to evaluate models using **multiple metrics** simultaneously to avoid misleading conclusions.  
- Frame this slide as a judgment exercise: students should learn to justify why a particular metric supports the analytic or business goal.

---

### Deeper Dive  
Choosing the right metric is fundamentally tied to understanding the cost structure of the problem, the distribution of errors, the modeling assumptions, and the types of decisions models support. Different metrics emphasize different theoretical and practical aspects of model performance.

---

#### 1. Error distributions and robustness  
If errors are normally distributed and the model is trained with squared-error loss:
- RMSE/MSE aligns naturally with the underlying statistical assumptions.  

If errors are heavy-tailed or contain outliers:
- RMSE and MSE can be dominated by a few large deviations.  
- MAE remains stable, making it a robust alternative.

This demonstrates that **metric choice must reflect data characteristics**.

---

#### 2. Decision-theoretic perspective  
Metrics implicitly encode *loss functions*:

- **MSE/RMSE** correspond to a *quadratic loss*, which makes large mistakes very expensive.  
- **MAE** corresponds to a *linear loss*, which treats all deviations proportionally.  
- **Asymmetric costs** (e.g., underprediction worse than overprediction) require custom metrics or loss functions.

Thus, the “right” metric depends on the *real-world consequences of being wrong*.

---

#### 3. Communication vs. accuracy  
Metrics differ dramatically in interpretability:

- **RMSE/MAE** → concrete, relatable (“average miss in dollars/units”).  
- **R²** → abstract but intuitive for explaining variance captured.  
- **MSE** → mathematically elegant but communication-unfriendly.

In stakeholder-oriented contexts—marketing, finance, operations—transparent metrics like RMSE or MAE are far more persuasive than R² or MSE alone.

---

#### 4. Distributional sensitivity of metrics  
Metrics weight different parts of the error distribution differently:

- RMSE → sensitive to tails.  
- MAE → sensitive to median behavior.  
- MSE → extremely sensitive to extremes.  
- R² → not sensitive to absolute magnitude of errors (a small RMSE in a low-variance dataset may produce a low R²).

This reinforces that **no metric captures everything**, and blind reliance on any single metric is risky.

---

#### 5. Scaling and comparability  
- RMSE/MAE are **scale-dependent** → cannot compare across distinct outcome variables.  
- R² is **scale-free**, but only loosely related to predictive performance.  

Thus, cross-model comparisons must consider whether metrics are comparable in scale and meaning.

---

#### 6. Metrics for model selection vs. model evaluation  
Some metrics are best for tuning; others are best for interpretation:

- **Tuning:** RMSE, MAE, cross-validated estimates.  
- **Interpretation:** R², Adjusted R².  
- **Optimization:** MSE aligns with OLS training.

Students should recognize that model *selection* and model *communication* may require different metrics.

---

#### 7. Beyond classical metrics  
Advanced applications may require specialized metrics:
- **MAPE** for percentage errors (with caveats).  
- **Quantile loss** for asymmetric risk.  
- **Pinball loss** in forecasting.  
- **Custom cost functions** for domain-specific misprediction losses.

The key insight: metric choice should reflect **what matters most** in application.

---

**Summary Insight:**  
There is no universally “best” regression metric.  
RMSE, MAE, MSE, and R² each reveal different aspects of model performance and encode different assumptions about error tolerance.  
Thoughtful metric selection ensures that evaluation aligns with data properties, domain requirements, and decision-making priorities.
:::




## Classical model selection approaches

- Early regression practice: compare many models with different subsets of predictors.  
- Tools:  
  - **Adjusted R²** → penalizes adding predictors that don’t help.  
  - **AIC / BIC** → information criteria; lower is better.  
- Goal: find a model that explains data **without overfitting**.  

::: {.notes}
### Slide:: Classical model selection approaches

### Detailed Notes  
- Use this slide to give students *historical and conceptual context* for how model selection was performed before cross-validation and regularization became standard in machine learning.  
- Emphasize that these classical tools—Adjusted R², AIC, BIC—were developed for an era when computation was expensive and models were small. They remain valuable today, but they serve *specific* purposes and rely on assumptions that may not hold in modern high-dimensional settings.  
- Explain the shared goal: **balance goodness-of-fit with model simplicity**. Each method penalizes model complexity differently, and students should see these tools as mechanisms for preventing overfitting in fixed-\(p\) regression.  
- Clarify that classical approaches are best suited for parametric models (like linear regression) and small/moderate numbers of predictors.  
- Help students recognize their practical role today: useful for quick diagnostics, for comparing *nested* regression models, and for teaching underlying principles of model selection—but no longer the main workhorse for predictive modeling.  
- Prepare them to understand how these approaches relate to cross-validation and regularization, which operate under more general conditions and are more robust for modern predictive analytics.

---

### Deeper Dive  
Classical model selection approaches revolve around **penalized likelihood** and **degrees-of-freedom adjustments**. They assume fixed \(p\), relatively large sample sizes, and parametric models whose complexity increases in discrete steps (e.g., adding predictors).

Understanding their mechanics helps students appreciate the evolution toward modern validation-based methods.

---

#### 1. Adjusted R²: correcting R²’s optimism  
Adjusted R² is defined as:
\[
\text{Adjusted } R^2 = 1 - \left( \frac{\text{SS}_{res}/(n - p - 1)}{\text{SS}_{tot}/(n - 1)} \right).
\]

Key features:
- Penalizes adding predictors by adjusting degrees of freedom.  
- Can **decrease** when a new predictor fails to improve the model enough.  
- Helps prevent blind reliance on vanilla R², which always increases with added predictors.

However:
- It still assumes linearity and homoscedasticity.  
- It is less reliable when predictors are correlated or when \(p\) is large relative to \(n\).

---

#### 2. Information criteria (AIC, BIC): likelihood-based penalties  
Both AIC and BIC aim to balance goodness-of-fit with model complexity, but they do so using different philosophies.

**AIC (Akaike Information Criterion):**
\[
\text{AIC} = -2\log(L) + 2p.
\]
- Derived from information theory.  
- Penalizes complexity lightly.  
- Favored when prediction is the goal.

**BIC (Bayesian Information Criterion):**
\[
\text{BIC} = -2\log(L) + p \log(n).
\]
- Derived from Bayesian model evidence approximation.  
- Penalizes complexity more heavily as \(n\) grows.  
- Tends to favor simpler models (“model selection consistency”).

Together, they illustrate how **likelihood + penalty** frameworks function as precursors to modern regularization.

---

#### 3. Cp and Mallows' Cp  
Although not listed explicitly on the slide, Mallows' Cp is historically central:
\[
C_p = \frac{\text{RSS}}{\hat{\sigma}^2} - (n - 2p).
\]

Interpretation:
- Measures expected prediction error.  
- Smaller Cp → better tradeoff between fit and complexity.

Cp laid conceptual groundwork for cross-validation by explicitly estimating prediction error.

---

#### 4. Why these approaches matter historically  
Before resampling methods were computationally feasible:
- exhaustive search over subsets was common,  
- selection relied on heuristics (stepwise procedures),  
- penalty-based criteria provided fast model comparison.

These approaches also helped shape modern concepts like:
- structural risk minimization,  
- penalized likelihood,  
- complexity control.

---

#### 5. Limitations in modern ML contexts  
These classical criteria were not designed for:
- high-dimensional data (\(p \approx n\) or \(p > n\)),  
- multicollinearity,  
- nonparametric or black-box models,  
- correlated predictors in large numbers,  
- cross-validation-based hyperparameter tuning.

They assume that:
- increasing predictors increases variance in a predictable way,  
- the true model is among the candidate linear models,  
- model complexity is discrete, not continuous.

In modern ML:
- cross-validation directly estimates out-of-sample performance,  
- regularization controls complexity continuously,  
- information criteria often give misleading results when assumptions fail.

---

#### 6. When classical methods are still useful  
Despite limitations, Adjusted R², AIC, and BIC remain relevant when:
- the model is linear or generalized linear,  
- the number of predictors is small,  
- interpretability is key,  
- nested model comparison is needed,  
- computational constraints favor fast heuristics.

They are especially valuable for teaching **principles**:  
- Occam’s razor (simplicity),  
- overfitting penalty,  
- balancing model complexity with data.

---

**Summary Insight:**  
Classical model selection tools like Adjusted R², AIC, and BIC introduced the idea that model fit must be weighed against model complexity.  
They paved the way for modern cross-validation and regularization, but in today’s high-dimensional and predictive modeling landscapes, they serve mainly as conceptual anchors and quick heuristics—not as primary selection tools.
:::




## Subset selection strategies

- **Forward selection:**  
  - Start empty → add variables one by one.  
- **Backward selection:**  
  - Start with all variables → remove least useful.  
- **Stepwise / mixed:**  
  - Add and remove iteratively.  

- Problem: exponential number of possible models (\(2^p\)).  
- Practical only for small \(p\).  

::: {.notes}
### Slide:: Subset selection strategies

### Detailed Notes  
- Use this slide to connect *historical variable selection methods* with the modern techniques students have just learned. Make clear that subset selection was a major early attempt to automate model building before regularization became widespread.  
- Explain each method in intuitive terms:  
  - **Forward selection:** start with nothing, add predictors that most improve the model. Good when \(p\) is small and interpretability is a priority.  
  - **Backward selection:** start with everything, remove predictors that contribute least. Requires \(n > p\).  
  - **Stepwise selection:** a hybrid that adds and removes predictors based on criteria like p-values, AIC, or adjusted R².  
- Emphasize that although these strategies were practical in low-dimensional contexts, they do not scale because the total number of possible models is \(2^p\), which grows explosively.  
- Pedagogically, point out that these approaches are **greedy heuristics**, not globally optimal searches. They often behave unpredictably in the presence of correlated variables or noise.  
- Prepare students to understand why the statistical learning community moved toward cross-validation and regularization—which are more principled, scalable, and stable.

---

### Deeper Dive  
Subset selection explores the combinatorial space of all possible models defined by choosing subsets of predictors. The methods are best understood through their computational structure, statistical properties, and limitations.

---

#### 1. Exhaustive search and the combinatorial explosion  
In principle, we could evaluate all \(2^p\) subsets of predictors:
- Fit every possible model.  
- Choose the best according to some criterion (AIC, BIC, adjusted R²).

But:
- \(p = 20\) ⇒ over 1 million models.  
- \(p = 30\) ⇒ over 1 billion models.  

Thus, exact subset selection becomes computationally infeasible even for moderate \(p\).  
This bottleneck motivated development of *approximate* (greedy) strategies like forward and backward selection.

---

#### 2. Forward selection and backward elimination  
These are **greedy algorithms**:
- **Forward:**  
  - Start with no predictors.  
  - Add the variable that most improves model fit (e.g., reduces RSS or AIC).  
  - Repeat until no meaningful improvement remains.

- **Backward:**  
  - Start with all predictors.  
  - Remove the variable that worsens the model least when deleted.  
  - Requires \(n > p\) because an initial full-model OLS must be fit.

Advantages:
- Computationally cheaper than exhaustive search.  
- Easy to interpret and implement.  

Disadvantages:
- Myopic: choices made early cannot be undone (unless using stepwise).  
- Sensitive to multicollinearity.  
- Can miss optimal subsets when relationships are complex.

---

#### 3. Stepwise (mixed) selection  
Stepwise selection alternates between adding and removing predictors based on:
- p-values (classical statistics),  
- information criteria (AIC/BIC),  
- adjusted R².

Statistical issues:
- Inflates Type I error due to repeated testing.  
- Some variables enter or leave arbitrarily in the presence of correlated predictors.  
- Coefficients from stepwise models tend to be biased downward (selection bias).

Although widely taught, stepwise methods have poor reproducibility and weak theoretical guarantees.

---

#### 4. Model uncertainty and selection instability  
Subset selection implicitly assumes:
- There exists a single “correct” model.  
- Small differences in predictive value justify hard inclusion/exclusion decisions.

But in reality:
- Many subsets perform similarly.  
- Different samples produce different selected models.  
- Correlated predictors make selection particularly unstable.

These limitations undermine interpretability and reproducibility.

---

#### 5. Why subset selection motivated regularization  
Regularization methods (Ridge, Lasso, Elastic Net) address subset selection’s shortcomings:

- They scale to high-dimensional settings.  
- They handle correlated predictors more gracefully.  
- They convert a combinatorial search into a **convex optimization problem**.  
- Lasso achieves sparsity without enumerating subsets.  
- Elastic Net stabilizes correlated groups of variables.  
- Cross-validation estimates out-of-sample performance directly.

Regularization essentially turns variable selection into a **continuous shrinkage problem**, replacing brittle inclusion/exclusion decisions with smooth coefficient trajectories.

---

#### 6. When subset selection is still reasonable  
Although largely obsolete for predictive modeling, subset selection is still useful when:
- \(p\) is very small (e.g., < 10 predictors).  
- The goal is interpretability, not prediction.  
- Analysts require explicit model forms that stakeholders can read and audit.  
- Computational and regulatory environments demand parsimonious models.

However, even in these cases, cross-validation is still preferred for final evaluation.

---

**Summary Insight:**  
Subset selection reflects early attempts to balance model fit and simplicity, but the combinatorial explosion and statistical instability of these methods limit their modern relevance.  
Regularization and cross-validation now serve as more robust, scalable, and principled approaches to model selection in real-world regression tasks.
:::



## Classical vs. modern model selection

- **Classical:**  
  - Adjusted R², Cp, AIC, BIC, subset selection.  
  - Strength: interpretable, quick for small problems.  
  - Weakness: doesn’t scale; prone to multiple-testing problems.  

- **Modern (preferred):**  
  - Cross-validation for performance.  
  - Regularization (Ridge, Lasso, Elastic Net) for complexity control.  

- Today: classical tools are mostly **complements** or quick heuristics.  

::: {.notes}
### Slide:: Classical vs. modern model selection

### Detailed Notes  
- Use this slide to help students contrast two fundamentally different eras of model selection:  
  **(1) Classical statistics**, where model selection relied on analytical criteria, and  
  **(2) Modern machine learning**, where model selection relies on empirical performance and regularization.  
- Emphasize the main pedagogical point: classical tools are **not wrong**, but they were designed for small, low-dimensional problems. Modern tools emerged because data and model complexity have grown dramatically.  
- Highlight that classical methods—Adjusted R², Cp, AIC, BIC, subset selection—are computationally lightweight and interpretable, but they struggle with large \(p\), multicollinearity, and correlated feature spaces.  
- Stress the strengths of modern approaches:  
  - **Cross-validation** measures actual out-of-sample performance rather than relying on asymptotic approximations.  
  - **Regularization** controls overfitting continuously, enabling stable solutions in high-dimensional or correlated predictor settings.  
- Use this slide to reinforce the mindset shift: modern ML practice emphasizes **predictive generalization**, not in-sample fit or analytical convenience.  
- Encourage students to see classical tools as useful *heuristics* or starting points—but not as substitutes for validation-based selection in real-world modeling.

---

### Deeper Dive  
Model selection has undergone a conceptual evolution. Classical approaches were rooted in analytical approximations and low-dimensional assumptions, while modern approaches rely on empirical validation and convex regularization. Understanding this evolution reveals why cross-validation and regularization dominate contemporary practice.

---

#### 1. The classical paradigm: penalize complexity analytically  
Classical model selection tools used penalties derived from statistical theory:

- **Adjusted R²** penalizes added predictors through degrees-of-freedom adjustments.  
- **AIC** penalizes complexity lightly using information-theoretic arguments.  
- **BIC** penalizes complexity heavily using Bayesian marginal likelihood approximations.  
- **Cp and subset selection** compare models based on stopping rules or exhaustive search.

These methods assume:
- fixed \(p\),  
- relatively large \(n\),  
- linear models,  
- nested or comparable model structures.

Their purpose was to avoid overfitting when computation was limited and resampling was infeasible.

---

#### 2. Why classical criteria struggle today  
Classical tools become unreliable when:
- \(p\) is not small,  
- predictors are correlated,  
- models are nonlinear or high-dimensional,  
- selection involves many candidate models (**multiple testing problem**),  
- the true model is unknown or not in the candidate set.

Moreover:
- These tools do not measure **generalization error** directly.  
- Their penalty structures are derived from asymptotic approximations that do not hold in modern ML contexts.

Thus, they cannot reliably select models for predictive performance.

---

#### 3. The modern paradigm: empirical performance + regularization  
Cross-validation and regularization embody a different philosophy:

- **Cross-validation** directly estimates out-of-sample performance.  
- **Ridge, Lasso, Elastic Net** impose continuous regularization rather than discrete inclusion/exclusion decisions.  
- Modern selection considers a *continuum of models* parameterized by \(\lambda\), rather than a finite set.  

The result:
- better generalization,  
- smoother selection paths,  
- stability under noise and correlation,  
- scalability to large or wide datasets,  
- applicability to nonlinear and high-dimensional models.

---

#### 4. Multiple-testing issues in classical methods  
Subset selection, stepwise regression, and sequential testing inflate Type I error dramatically. Each model comparison effectively runs another hypothesis test, compounding false discoveries.

Modern methods avoid this:
- Cross-validation uses *data splitting* rather than repeated testing.  
- Regularization shrinks coefficients continuously rather than relying on binary decisions.

This difference is crucial for interpretability and reliability.

---

#### 5. When classical methods still matter  
Classical tools continue to be useful as:
- fast diagnostics,  
- teaching tools,  
- initial exploration methods,  
- interpretable heuristics in small-\(p\), small-\(n\) regimes.

They can guide intuition, but final model selection should rely on:
- cross-validation estimates of prediction error,  
- regularized model paths,  
- stability checks.

---

#### 6. Unifying view: structural risk minimization  
Both paradigms share a core idea:
> Balance model complexity with fit.

But the mechanisms differ:
- Classical → *fixed penalties* derived analytically  
- Modern → *data-driven penalties* tuned empirically  

This shift reflects the growing role of computation, resampling, and predictive performance in modern machine learning.

---

**Summary Insight:**  
Classical model selection approaches introduced foundational ideas about parsimony and model comparison, but they do not scale to modern high-dimensional or predictive contexts.  
Cross-validation and regularization—flexible, empirical, and robust—now serve as the preferred tools for building models that generalize well.  
Classical tools remain valuable complements, but modern practice demands validation-driven selection.
:::



# Model Selection and Validation  



## Cross-validation provides more stable performance estimates

- **Problem:** A single train/test split may give unstable results, especially with small datasets.  
- **Solution: k-fold cross-validation**  
  - Split data into k folds.  
  - Train on k–1 folds, validate on the remaining fold.  
  - Repeat k times and average results.  
- **Benefits:**  
  - Provides a more reliable estimate of generalization.  
  - Reduces dependence on a single split.  

::: {.notes}
### Slide:: Cross-validation provides more stable performance estimates

### Detailed Notes  
- Use this slide to introduce cross-validation as the **central tool** for estimating generalization error in modern machine learning.  
- Emphasize the key motivation: a single train/test split can yield **highly variable** results, especially when datasets are small, imbalanced, or noisy. Students must understand that a single split is just one of many possible partitions of the data—its estimate of performance is therefore unstable.  
- Explain k-fold cross-validation step-by-step:  
  1. Partition the data into \(k\) roughly equal folds.  
  2. Train the model on \(k-1\) folds.  
  3. Validate it on the remaining fold.  
  4. Repeat this process so that every fold serves as the validation set once.  
- Stress that the **average validation performance** across folds provides a more reliable and less noisy estimate of out-of-sample error.  
- Pedagogically, connect this to the earlier discussion of bias–variance: cross-validation stabilizes the estimate of prediction error by reducing variance from the train/test split.  
- Highlight that cross-validation is also the backbone of hyperparameter tuning in regularized regression, tree-based models, and virtually all modern ML algorithms.

---

### Deeper Dive  
Cross-validation (CV) is a resampling-based strategy for estimating predictive risk. It addresses a fundamental problem: the performance estimate derived from any *single* partition of the data is a random variable, and its variance can be large. CV mitigates this instability by averaging across multiple partitions.

---

#### 1. Why train/test splits are unstable  
Let a dataset be split 80/20. The resulting model performance depends on:
- which observations fell into training or test sets,  
- whether rare or influential points ended up in one split or the other,  
- whether the training subset contained enough representative variation to learn the signal.

With small or noisy datasets, the variance in estimates across different random splits is often large:
\[
\hat{E}_{split1} \neq \hat{E}_{split2} \neq \hat{E}_{split3}.
\]

Cross-validation reduces this instability by **averaging across many possible splits**.

---

#### 2. Formal definition of k-fold cross-validation  
Given dataset \(D\) and model \(f\), CV seeks to estimate the risk:
\[
R(f) = \mathbb{E}_{(x,y)}[(y - f(x))^2].
\]

k-fold CV approximates this by:
\[
\hat{R}_{CV} = \frac{1}{k} \sum_{i=1}^k \hat{R}_i,
\]
where \(\hat{R}_i\) is the error on fold \(i\).

This estimate:
- uses all data for both training and validation,  
- accounts for variability across partitions,  
- reduces estimator variance.

---

#### 3. Choice of \(k\): bias–variance tradeoff  
- **Large \(k\)** (e.g., LOOCV, leave-one-out CV):  
  - Low bias (uses nearly full dataset to train).  
  - High variance (validation sets are tiny).  
  - Computationally expensive.

- **Small \(k\)** (e.g., 5-fold or 10-fold):  
  - Higher bias (models train on slightly less data).  
  - Lower variance (validation sets are larger and more stable).  
  - Computationally reasonable.

Empirically, **5–10 folds** balance computation and stability well and are standard practice.

---

#### 4. Cross-validation as a general-purpose evaluation engine  
CV works for:
- linear models (for estimating generalization error),  
- hyperparameter tuning for Ridge, Lasso, Elastic Net,  
- selecting polynomial degrees,  
- comparing models (e.g., linear regression vs. KNN vs. trees),  
- preventing overfitting when building pipelines.

It also extends naturally to:
- **stratified folds** (classification imbalance),  
- **time-series CV** (rolling-origin evaluation),  
- **nested CV** (unbiased hyperparameter tuning).

Thus, CV is foundational across ML workflows.

---

#### 5. Why cross-validation outperforms classical criteria  
Unlike AIC, BIC, or adjusted R²:
- CV makes **no parametric assumptions**,  
- directly estimates predictive error,  
- works with complex, high-dimensional, and nonlinear models,  
- scales with modern computational capabilities.

It measures what we actually care about: **performance on unseen data**.

---

#### 6. Limitations and cautions  
Cross-validation is powerful, but:
- It can be computationally expensive for large models.  
- Data leakage inside folds must be avoided (preprocessing must be fit only on the training portion of each fold).  
- For time-series forecasting, standard CV is invalid; one must use **blocked** or **rolling** CV.

These cautions prepare students for responsible CV use in professional workflows.

---

**Summary Insight:**  
Cross-validation is the modern gold standard for estimating model generalization.  
By averaging over multiple train/validation splits, it provides a stable, low-variance estimate of out-of-sample error—essential for comparing models and tuning hyperparameters across the entire machine learning pipeline.
:::




## The bias–variance tradeoff explains underfitting and overfitting {.layout-two-content}


- **High bias (underfitting):** Model too simple → consistently wrong.  
- **High variance (overfitting):** Model too complex → fits noise in training data.  
- **Graph:** Test error vs. model complexity → U-shaped curve.  
- Examples:  
  - Polynomial regression: variance grows with degree.  
  - Ridge/Lasso: reduce variance by shrinking coefficients.  




::: {.notes}
### Slide:: The bias–variance tradeoff explains underfitting and overfitting

### Detailed Notes  
- Use this slide to synthesize many ideas students have seen: polynomial regression, regularization, cross-validation, and generalization. Frame the bias–variance tradeoff as the **unifying conceptual lens** behind why models fail and why techniques like regularization work.  
- Emphasize the two failure modes:  
  - **High bias (underfitting):** The model is too simple to capture true patterns. Predictions are systematically wrong.  
  - **High variance (overfitting):** The model is too flexible and captures noise as if it were signal. Predictions vary wildly across samples.  
- Guide students’ eyes to the U-shaped test-error curve:  
  - Left side: error high because the model is overly rigid.  
  - Right side: error high because the model memorizes noise.  
  - Middle: **optimal complexity**—where the model generalizes best.  
- Highlight familiar examples:  
  - Polynomial regression: test error decreases as degree increases, then rises again.  
  - Regularization (Ridge, Lasso): shrinks coefficients to reduce variance, shifting the model leftward on the U-shape toward optimal complexity.  
- Pedagogically, stress that the entire discipline of model selection—cross-validation, regularization, pruning, early stopping—exists because of the bias–variance tradeoff.  
- Reinforce that students should always interpret model behavior through this lens: “Is my model too simple, too complex, or just right?”

---

### Deeper Dive  
The bias–variance decomposition is one of the most important theoretical results in statistical learning. It explains why predictive error behaves non-monotonically with model complexity and clarifies the fundamental tension every modeling approach must manage.

---

#### 1. Formal decomposition of expected prediction error  
For a regression estimator \(\hat{f}(x)\), the expected squared prediction error decomposes as:
\[
\mathbb{E}[(y - \hat{f}(x))^2]  
= \underbrace{\big( \mathbb{E}[\hat{f}(x)] - f(x) \big)^2}_{\text{Bias}^2}
+ \underbrace{\mathbb{E}\left[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2\right]}_{\text{Variance}}
+ \underbrace{\sigma^2}_{\text{Irreducible noise}}.
\]

Interpretation:  
- **Bias** measures how far the model’s *average* prediction is from the truth.  
- **Variance** measures how much the model fluctuates across different training samples.  
- \(\sigma^2\) is noise we can never eliminate.

The U-shaped test-error curve follows directly from this decomposition.

---

#### 2. Behavior of bias and variance with model complexity  
As model flexibility increases:
- **Bias decreases** because the model can better approximate \(f(x)\).  
- **Variance increases** because the model becomes sensitive to idiosyncrasies in the training data.  

This produces:
- Underfitting at low complexity,  
- Overfitting at high complexity,  
- An optimal point balancing both.

Polynomial regression is the clearest demonstration:
- Degree 1 → high bias.  
- Degree 10 → high variance.  
- Moderate degree → best predictive accuracy.

---

#### 3. Regularization as variance control  
Regularization techniques shrink coefficients toward zero:
- **Ridge** reduces variance by damping the influence of unstable directions in the parameter space.  
- **Lasso** reduces variance and enforces sparsity, removing noisy predictors.  
- **Elastic Net** stabilizes correlated predictors while promoting sparsity.

Regularization shifts the model leftward on the U-curve, reducing overfitting and improving generalization.

---

#### 4. Connection to cross-validation  
Cross-validation empirically identifies the point on the U-shaped curve where validation error is minimized.  
This allows practitioners to:
- select polynomial degree,  
- tune \(\lambda\) in Ridge/Lasso,  
- choose tree depth or number of neighbors in KNN.

Thus, CV operationalizes the bias–variance tradeoff in practice.

---

#### 5. The tradeoff in nonparametric and ML models  
The same tension appears across all models:
- **KNN:**  
  - Small \(k\) → low bias, high variance (wiggly curve).  
  - Large \(k\) → high bias, low variance (oversmoothing).  
- **Decision trees:**  
  - Deep tree → high variance.  
  - Shallow tree → high bias.  
  - Pruning reduces variance.  
- **Neural networks:**  
  - Many layers/parameters → high variance.  
  - Regularization + early stopping → variance control.  
- **Boosting:**  
  - Too many boosting rounds → overfitting; shrinkage combats variance.

The tradeoff is universal.

---

#### 6. Why modern ML sometimes “breaks” classical intuition  
In very overparameterized models (e.g., deep networks), test error sometimes decreases again after the interpolation threshold—a phenomenon known as **double descent**.  
However, this behavior *still reflects* bias–variance dynamics in a more complex regime:
- The classical U-curve expands into a double-U shape.  
- Regularization remains essential to ensure stability.  

Understanding classical bias–variance helps students interpret even modern anomalies.

---

**Summary Insight:**  
The bias–variance tradeoff is the organizing principle behind model complexity, regularization, cross-validation, and predictive robustness.  
Underfitting comes from high bias; overfitting from high variance.  
Optimal models sit at the intersection—minimizing test error through the right balance of flexibility and constraint.
:::



## The bias–variance tradeoff explains underfitting and overfitting


<figure>
  <img src="../materials/assets/images/bias_variance_tradeoff.svg"
       alt="**Figure:** U-shaped curve of bias–variance tradeoff">
  <figcaption>**Figure:** U-shaped curve of bias–variance tradeoff</figcaption>
</figure>



## Learning curves and residual plots help diagnose model performance 

<figure>
  <img src="../materials/assets/images/learningcurves.png"
       alt="**Figure:** Predicted vs Residuals charts">
  <figcaption>**Figure:** Predicted vs Residuals charts</figcaption>
</figure>



::: {.notes}
### Slide:: Learning curves and residual plots help diagnose model performance

### Detailed Notes  
- Use this slide to show students how **diagnostic visualization** complements numerical metrics and cross-validation. These tools help answer *why* a model performs poorly, not just *how well* it performs.  
- Start with **learning curves**: emphasize that they compare training error vs. validation error as a function of training set size. They diagnose:  
  - **Underfitting:** both training and validation errors remain high and close together → the model is too simple or the functional form is wrong.  
  - **Overfitting:** training error is very low, but validation error remains high → the model is too flexible relative to available data.  
  - **Data-limited regimes:** validation error decreases as training size grows → more data would likely improve performance.  
- Then transition to **residual plots**: residual analysis tests whether the model captures the underlying structure of the data.  
  - Random scatter around zero indicates an adequate specification.  
  - Patterns (curvature, funnels, trends) signal misspecification, heteroscedasticity, or unmodeled interactions.  
- Pedagogically, emphasize that these complementary tools provide *modeling insight*, not just evaluation. They guide decisions about adding features, transforming variables, regularizing more strongly, or selecting different model classes.

---

### Deeper Dive  
Learning curves and residual plots are two of the most powerful diagnostic tools in regression and machine learning. They provide insights into model capacity, data sufficiency, functional specification, and generalization behavior.

---

#### 1. Learning curves formalize how models behave with increasing data  
Let training error be \(E_{\text{train}}(m)\) and validation error be \(E_{\text{val}}(m)\) for training set size \(m\).  

Typical behaviors:

- **Underfitting (high bias):**  
  - \(E_{\text{train}}\) high  
  - \(E_{\text{val}}\) high  
  - Curve plateau → model cannot learn the signal regardless of data  
  - Remedy: increase model flexibility, add interactions or nonlinearities.

- **Overfitting (high variance):**  
  - \(E_{\text{train}}\) very low  
  - \(E_{\text{val}}\) high  
  - Large gap between curves  
  - Remedy: regularization (Ridge/Lasso/Elastic Net), reduce model complexity, or acquire more data.

- **Just right:**  
  - Curves converge at a reasonably low value  
  - Validation error stabilizes  
  - Indicates proper alignment between model flexibility and data size.

Learning curves thus expose whether **capacity** or **data quantity** is the limiting factor.

---

#### 2. Learning curves estimate data value and ROI in data collection  
Because they show how validation error changes with sample size, learning curves help answer questions like:
- “Will more data reduce test error?”  
- “Are we data-limited or model-limited?”  
- “Is it worth investing in additional data collection?”  

This is crucial for business and scientific applications where data acquisition is costly.

---

#### 3. Residual plots diagnose specification failures  
Residuals:
\[
e_i = y_i - \hat{y}_i.
\]

A correct model (under standard assumptions) yields:
\[
\mathbb{E}[e_i \mid X] = 0,
\]
and residuals should be randomly distributed around zero.

Patterns suggest issues:
- **Curvature** → missing polynomial terms or interactions.  
- **Funnel shape** → heteroscedasticity (non-constant variance).  
- **Clusters** → omitted categorical structure or latent groups.  
- **Serial correlation** → time-dependent errors.  

Residual plots are therefore an essential component of model specification testing.

---

#### 4. Residual distribution reveals error structure  
Histograms or Q–Q plots of residuals can diagnose:
- normality violations (affect inference),  
- heavy tails (suggest MAE or robust regression),  
- skewness (suggest log-transformation or different link function).

Thus, residual analysis bridges evaluation and *model refinement*.

---

#### 5. Complementarity of learning curves and residual analysis  
Together, these tools reveal different dimensions of model performance:

- Learning curves → **capacity vs. data**  
- Residual plots → **specification vs. assumptions**  

For example:
- A model may overfit according to learning curves but also show curvature patterns in residuals → the model is too flexible for the data *and* incorrectly specified.  
- A model may underfit and show strong curvature → the model lacks nonlinear terms.

These diagnostics guide the next modeling step more effectively than metrics alone.

---

**Summary Insight:**  
Learning curves diagnose generalization failure by analyzing how errors behave as data increases.  
Residual plots diagnose specification failure by revealing structure the model has missed.  
Together, they form a comprehensive diagnostic toolkit for improving regression models and understanding where—and why—they succeed or fail.
:::



## Learning curves and residual plots help diagnose model performance {.layout-two-content}

<figure>
  <img src="../materials/assets/images/residualanalysis.png"
       alt="**Figure:** Example learning curve and residual plot">
  <figcaption>**Figure:** Example learning curve and residual plot</figcaption>
</figure>


::: {.notes}
### Slide:: Learning curves and residual plots help diagnose model performance

### Detailed Notes  
- Use this slide to revisit residual analysis as a *model specification diagnostic*, emphasizing that even when learning curves look acceptable, residuals may reveal deeper structural problems.  
- Focus on the **residual plot** shown: teach students that residuals are the “x-ray” of a regression model—if there is unmodeled structure, it will appear here.  
- Emphasize the core diagnostic logic:  
  - **Random scatter around zero** → model is capturing the systematic patterns in the data.  
  - **Patterns (curves, funnels, clusters, trends)** → the model is missing something important.  
- Connect the visuals to real modeling steps:  
  - Curvature → add polynomial terms or interactions.  
  - Funnel shape → heteroscedasticity; consider transformations or robust regression.  
  - Clusters → categorical structure or segmentation may be missing.  
  - Outliers → investigate data quality or consider robust loss functions.  
- Reinforce that residual analysis should be a *default step* after fitting any regression model, just like checking validation error.  
- Pedagogically, explain that residual plots diagnose **how** the model is wrong, not just **how much** it is wrong. This distinction helps students move from evaluation to model improvement.

---

### Deeper Dive  
Residual analysis is one of the most powerful tools in regression diagnostics. It reveals violations of assumptions, structural deficiencies in the model, and opportunities for improvement.

---

#### 1. The fundamental expectation for residuals  
Under a correctly specified model:
\[
\mathbb{E}[e_i \mid X] = 0,
\]
and residuals should be:
- independent,  
- identically distributed,  
- centered around zero,  
- free of patterns when plotted against any predictor or fitted values.

Residual plots test this visually.

---

#### 2. Diagnosing nonlinearity  
If residuals exhibit a curved or wave-like pattern, the model is failing to capture nonlinear structure.  
Common remedies:
- add polynomial features,  
- use spline regression,  
- switch to nonparametric or tree-based models,  
- introduce interaction terms.

This aligns with insights from earlier slides on polynomial regression and interactions.

---

#### 3. Diagnosing heteroscedasticity  
If the spread of residuals increases or decreases with fitted values (a funnel shape), variance is not constant:
\[
\text{Var}(e_i \mid X) \ne \sigma^2.
\]

Consequences:
- OLS coefficients remain unbiased but **standard errors are invalid**, harming inference.

Remedies include:
- transforming the outcome (e.g., log),  
- using weighted least squares,  
- applying robust standard errors.

---

#### 4. Diagnosing correlated or structured errors  
Residual plots may reveal:
- sequential patterns (time dependence),  
- clusters (latent groups),  
- repeated patterns (cyclical phenomena).

These indicate violations of independence.  
Remedies:
- time-series models,  
- random effects or mixed models,  
- clustering features or hierarchical structures.

---

#### 5. Diagnosing outliers and leverage  
Individual points far from zero in residual plots should be checked using:
- studentized residuals,  
- leverage statistics,  
- Cook’s distance.

Residual plots often serve as the “first alert” before running formal diagnostics.

---

#### 6. Learning curves as complementary diagnostics  
Although this particular slide focuses on residuals, learning curves complement residual analysis by diagnosing:
- capacity mismatch (bias/variance problems),  
- whether more data would improve generalization,  
- whether regularization is needed.

Together:
- **Residual plots** diagnose *model form* problems.  
- **Learning curves** diagnose *capacity and data volume* problems.

---

**Summary Insight:**  
Residual analysis is indispensable for identifying *why* a model is failing—nonlinearity, heteroscedasticity, omitted variables, outliers, or correlation.  
Residual plots reveal structure invisible in scalar metrics, and when paired with learning curves, they provide a complete diagnostic toolkit for improving regression models.
:::



## Validation should be a habit, not an afterthought

- Models should never be judged on training performance alone.  
- Validation helps detect overfitting and guides model and feature choices.  
- In industry, cross-validation is the **default baseline** for evaluating models.  

::: {.notes}
### Slide:: Validation should be a habit, not an afterthought

### Detailed Notes  
- Use this slide to underscore one of the most important modeling principles students must internalize: **training performance is not a measure of model quality**.  
- Emphasize that without validation—via holdout sets, k-fold cross-validation, or other resampling approaches—there is no way to know whether a model generalizes.  
- Connect this to the earlier discussion of overfitting: a model can achieve extremely low training error while performing terribly on unseen data.  
- Stress that validation should be part of the modeling workflow **from the very beginning**, not something bolted on at the end.  
- Highlight that in industry, cross-validation is the *standard operating procedure* for evaluating model performance and tuning hyperparameters across regression, classification, and machine learning pipelines.  
- Encourage students to develop the habit of asking, immediately after fitting a model: *How does it perform on validation data? How stable is that performance across folds?*  
- Pedagogically, position validation as an essential discipline—analogous to “measurement reliability” in scientific experiments.

---

### Deeper Dive  
Validation is the backbone of predictive modeling. It ensures that performance estimates reflect the model’s behavior on new, unseen data rather than its ability to memorize or overfit the training set.

---

#### 1. Why training error is deceptive  
Training error measures how well a model fits the specific sample it was trained on. It does **not** estimate:
- performance on new data,  
- the model’s stability under sampling variation,  
- the impact of noise,  
- the degree of overfitting.

Even a **perfectly overfit model** (zero training error) can have terrible generalization because it has learned noise and idiosyncrasies rather than the underlying pattern.

Thus, training error is *necessary* information but **completely insufficient** for evaluating model quality.

---

#### 2. Validation as an estimator of generalization error  
Validation provides an empirical approximation of:
\[
R(f) = \mathbb{E}_{(x,y)}[(y - f(x))^2],
\]
the true predictive risk.

Because we cannot sample from the population directly, we simulate the process by holding out data and using it as a stand-in for unseen future cases.

Cross-validation improves this approximation by:
- reducing variance,  
- averaging across partitions,  
- providing more robust insights into model behavior.

---

#### 3. Validation drives model and feature selection  
Effective validation guides decisions such as:
- choosing polynomial degree,  
- selecting regularization strength (\(\lambda\)),  
- pruning a decision tree,  
- deciding on number of neighbors in KNN,  
- determining whether more data or new features are needed.

In all these cases, validation ensures the model chosen is the one that performs *best on unseen data*, not just the one that fits training data the best.

---

#### 4. Validation prevents “false discoveries” in feature engineering  
Without validation:
- spurious correlations may appear meaningful,  
- irrelevant features may seem useful,  
- dummy variables or high-cardinality categories may appear “significant,”  
- models may appear stable but collapse when deployed.

Validation exposes these issues early by showing when certain features or modeling choices degrade generalization.

---

#### 5. Validation in industry workflow  
In real-world ML pipelines:
- cross-validation is used automatically in tools like scikit-learn’s GridSearchCV or RandomizedSearchCV,  
- model selection is entirely based on validation performance,  
- hyperparameters are never tuned using training error alone,  
- production models undergo continual monitoring and re-validation as data drifts.

Thus, validation is not optional—it is **institutionalized**.

---

#### 6. Beyond k-fold CV: a validation ecosystem  
Different modeling scenarios require different validation strategies:
- **Stratified CV** for imbalanced classification.  
- **Blocked or time-series CV** when data exhibit autocorrelation.  
- **Nested CV** for unbiased hyperparameter selection.  
- **Bootstrapping** for estimating variance or constructing prediction intervals.  
- **Train/validation/test splits** in deep learning pipelines.

Validation is a diverse toolkit, not a single method.

---

**Summary Insight:**  
Validation is the central safeguard against overfitting.  
It transforms modeling from curve-fitting into disciplined statistical learning by ensuring that decisions about features, hyperparameters, and model complexity are grounded in how well the model generalizes—not how well it replicates the training data.  
Making validation a habit is essential for reliable, reproducible machine learning.
:::


#  Hands-On Exercise - Apply regression concepts end-to-end.

# References
