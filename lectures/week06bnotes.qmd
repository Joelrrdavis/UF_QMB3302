---
title: Week 6 - part B
subtitle: QMB3302 Foundations of Analytics and AI
format:
  metropolis-beamer-revealjs:
    slide-number: c
    embed-resources: true
    # Syntax highlighting theme (pick one: pygments, tango, zenburn, kate, breeze, nord, github, dracula, monokai, etc.)
    highlight-style: ../materials/assets/highlight_accessible.theme
    # Nice-to-have code UX for slides
    code-line-numbers: true      # add line numbers to all code blocks
    code-overflow: wrap          # wrap long lines (good for projectors)
    code-copy: true              # copy-to-clipboard button
    code-block-bg: true          # subtle background behind code blocks
    code-block-border-left: "#E69F00"  # UF-amber accent; pick your brand color
author:
  - name: Joel Davis
    orcid: 0000-0000-0000-0000
    email: joel.davis@warrington.ufl.edu
    affiliations: University of Florida
date: last-modified
bibliography: ../materials/assets/shared_references_courses.bib

---

# Introduction to Unsupervised Learning

---



:::: {.columns} 
::: {.column width="50%"} 


::: 
::: {.column width="50%"} 


::: 
::::

## Unsupervised learning finds structure without labeled outcomes


:::: {.columns} 
::: {.column width="50%"} 

- **Definition:** Algorithms that discover patterns or relationships in data **without target labels**.  
- Contrast with supervised learning, which learns mappings from inputs → known outputs.  
- Goal: reveal **latent structure** — groups, patterns, or anomalies hidden in the data.  


::: 
::: {.column width="50%"} 

<figure>
  <img src="../materials/assets/images/intro_supervised_vs_unsupervised.svg"
       alt="**Figure:** Comparison of supervised learning (labeled outcomes) vs. unsupervised learning (unlabeled structure discovery).">
  <figcaption>**Figure:** Comparison of supervised learning (labeled outcomes) vs. unsupervised learning (unlabeled structure discovery).</figcaption>
</figure>

::: 
::::



::: {.notes}

### Detailed Notes  
- Begin by clearly distinguishing **supervised** versus **unsupervised** learning. In supervised learning, the algorithm learns a mapping from inputs to known output labels; in unsupervised learning, there are no labels, and the goal is to uncover structure hidden within the input data itself.  
- Emphasize that “structure” refers to **latent patterns**, such as clusters or relationships that may not be directly observable.  
- Explain that unsupervised learning is **exploratory by nature**, helping analysts generate hypotheses or organize data before predictive modeling begins.  
- Reinforce that while supervised models are optimized for accuracy, unsupervised models are evaluated on **interpretability, stability, and insight**.  
- Use analogies to clarify: supervised learning is like “learning from an answer key,” while unsupervised learning is like “finding groups in a crowd without knowing who belongs together.”  
- Show or describe the figure comparing supervised (labeled) and unsupervised (unlabeled) pipelines; highlight that the absence of labeled outcomes means the model must infer similarity from the data’s internal geometry.  

### DEEPER DIVE  
Unsupervised learning refers to methods that identify patterns or groupings within data without predefined categories or labels (Hastie, Tibshirani, & Friedman, 2009). The algorithms rely on the intrinsic structure of the data rather than external supervision. Common mathematical objectives include minimizing within-group variance (as in K-Means), maximizing separability among features, or identifying statistical deviations from expected distributions.  

This learning paradigm is foundational for **exploratory data analysis (EDA)** and is often a precursor to supervised modeling. For example, cluster analysis can reveal latent market segments that later inform classification tasks. Similarly, dimensionality reduction (e.g., PCA) helps reveal the principal axes of variation in high-dimensional spaces.  

A key conceptual distinction lies in **optimization criteria**. Supervised learning optimizes an explicit loss function based on known outcomes (e.g., minimizing mean squared error). Unsupervised learning lacks this reference point; it relies on internal metrics like within-cluster compactness or reconstruction error. Thus, its success depends as much on human interpretation as on algorithmic fit.  

From a philosophical standpoint, unsupervised learning represents an epistemic shift from prediction to **discovery** (von Luxburg & Schölkopf, 2011). Rather than asking “Can we predict Y?”, it asks “What structure exists within X?” This shift underlies much of modern data-driven insight generation in fields like marketing analytics, anomaly detection, and bioinformatics.  

**References**  
- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* (2nd ed.). Springer.  
- von Luxburg, U., & Schölkopf, B. (2011). Statistical learning theory: Models, concepts, and results. *Handbook of the Philosophy of Science: Philosophy of Statistics*, 7, 651–706.  

:::

---

## The three pillars and goals of unsupervised learning


:::: {.columns} 
::: {.column width="50%"} 

- **Clustering:** group similar observations to reveal structure.  
- **Dimensionality reduction:** simplify high-dimensional data for analysis or visualization.  
- **Anomaly detection:** identify rare or unexpected events that deviate from the norm.  
- Together, these form the **core toolkit** for exploring and understanding unlabeled data.  


::: 
::: {.column width="50%"} 

<figure>
  <img src="../materials/assets/images/intro_unsupervised_roadmap.svg"
       alt="**Figure:** Conceptual roadmap diagram showing progression: Clustering → Dimensionality Reduction → Anomaly Detection, with short captions: “Find patterns,” “Simplify data,” “Detect what’s unusual.”">
  <figcaption>**Figure:** Conceptual roadmap diagram showing progression: Clustering → Dimensionality Reduction → Anomaly Detection, with short captions: “Find patterns,” “Simplify data,” “Detect what’s unusual.”</figcaption>
</figure>

::: 
::::



::: {.notes}
 

### Detailed Notes  
- Explain that unsupervised learning encompasses multiple goals—pattern discovery, simplification, and detection of irregularities.  
- Introduce the three “pillars”: **clustering**, **dimensionality reduction**, and **anomaly detection**.  
- Stress that these are not mutually exclusive; often, dimensionality reduction precedes clustering, and anomaly detection builds on both.  
- Relate to business analytics: clustering supports segmentation, dimensionality reduction enables visualization, and anomaly detection safeguards operations.  
- Present this as a roadmap for the rest of the module.  

### DEEPER DIVE  
The three primary objectives of unsupervised learning form a methodological continuum (Jain, Murty, & Flynn, 1999).  

1. **Clustering** groups similar observations based on distance or density. It is widely used for market segmentation, customer behavior profiling, and image grouping. The mathematical formulations range from partition-based methods like K-Means to hierarchical and density-based approaches such as DBSCAN.  
2. **Dimensionality reduction** seeks lower-dimensional representations that capture most of the variance or information content. Techniques like PCA or manifold learning (t-SNE, UMAP) are crucial for handling the “curse of dimensionality” (Bellman, 1961).  
3. **Anomaly detection** identifies data points that deviate significantly from normal patterns, often corresponding to rare or high-impact events.  

Together, these form the foundation for **exploratory modeling**, where the primary goal is interpretive understanding rather than prediction accuracy.  

**References**  
- Jain, A. K., Murty, M. N., & Flynn, P. J. (1999). Data clustering: A review. *ACM Computing Surveys, 31*(3), 264–323.  
- Bellman, R. (1961). *Adaptive Control Processes: A Guided Tour*. Princeton University Press.  

:::

---

## When to use unsupervised methods

- **Exploration:** discover hidden relationships before labels exist.  
- **Segmentation:** identify natural customer or product groups.  
- **Novelty detection:** detect emerging risks or rare behaviors.  


::: {.notes}

### Detailed Notes  
- Use concrete business contexts to ground the discussion: customer segmentation, product recommendation, operational monitoring, and fraud detection.  
- Emphasize the **exploratory** and **diagnostic** roles of these methods before predictive models are available.  
- Clarify that unsupervised learning is useful when labeled data are unavailable or prohibitively expensive to obtain.  
- Reinforce that the insights derived often drive data labeling strategies or feature engineering downstream.  

### DEEPER DIVE  
Unsupervised methods are most appropriate in settings where the underlying structure of the data is unknown or only partially understood. For example, in early-stage analytics projects, clustering and dimensionality reduction can expose natural groupings or latent factors that guide hypothesis formulation (Hand, Mannila, & Smyth, 2001).  

They are also essential for **novelty and anomaly detection**, where the goal is to identify previously unseen patterns. In cybersecurity, for instance, unsupervised models can flag unusual network activity that supervised systems, trained on known threats, would miss.  

Unsupervised approaches serve both as **precursors** (data exploration before labeling) and **complements** (ongoing structure monitoring) to supervised pipelines. Their value lies in surfacing meaningful structure from raw, unlabeled data.  

**References**  
- Hand, D., Mannila, H., & Smyth, P. (2001). *Principles of Data Mining*. MIT Press.  
 
:::

---

## From “nice-to-have” insight to mission-critical capability


:::: {.columns} 
::: {.column width="50%"} 


::: 
::: {.column width="50%"} 


::: 
::::

- **Segmentation** supports smarter marketing and personalization.  
- **Anomaly detection** safeguards against fraud, failure, or loss.  
- Takeaway: unsupervised methods range from exploratory insight to **critical operational tools**.  

--

<figure>
  <img src="../materials/assets/images/intro_segmentation_vs_anomaly.svg"
       alt="**Figure:** Split visualization — left: marketing segmentation; right: anomaly detection alert.">
  <figcaption>**Figure:** Split visualization — left: marketing segmentation; right: anomaly detection alert.</figcaption>
</figure>

::: {.notes}

### Detailed Notes  
- Emphasize that unsupervised methods have matured from exploratory curiosities to integral parts of enterprise analytics pipelines.  
- Provide contrasting examples: early market segmentation studies vs. modern real-time anomaly detection in cybersecurity.  
- Stress the link between unsupervised methods and operational decision-making.  
- Conclude this section by foreshadowing the practical algorithms that follow (K-Means, DBSCAN, PCA).  

### DEEPER DIVE  
Historically, unsupervised learning was treated as supplementary—useful for visualization or exploratory insights. However, as data systems have scaled, the ability to autonomously detect structure or deviation has become operationally vital. In fraud detection, for instance, unsupervised algorithms can identify suspicious transactions before labeled fraud cases exist (Chandola, Banerjee, & Kumar, 2009).  

This transition reflects a broader trend toward **self-organizing systems** capable of continuous learning without explicit supervision. The line between “exploratory” and “production-critical” analytics is now fluid.  

As enterprises deploy AI for real-time monitoring, recommendation, and optimization, unsupervised techniques form the backbone of adaptive intelligence — systems that learn from structure rather than instruction.  

**References**  
- Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. *ACM Computing Surveys, 41*(3), 1–58.  
 
:::

---

# 4.2 Clustering: K-Means

---


:::: {.columns} 
::: {.column width="50%"} 


::: 
::: {.column width="50%"} 


::: 
::::

## K-Means assigns each point to the nearest centroid

- Algorithm steps:  
  1. Randomly initialize *k* centroids.  
  2. Assign each point to the nearest centroid (Euclidean distance).  
  3. Recompute each centroid as the mean of its cluster.  
  4. Repeat until cluster assignments stabilize.  
- Output: each data point gets a **cluster label**.  

--

<figure>
  <img src="../materials/assets/images/K-Means_clustering.svg"
       alt="**Figure:** 2D points colored by cluster assignment with centroids marked by Xs.">
  <figcaption>**Figure:** 2D points colored by cluster assignment with centroids marked by Xs.</figcaption>
</figure>

::: {.notes}

### Detailed Notes  
- Introduce clustering as the foundational unsupervised learning problem.  
- Walk through the **iterative process** of K-Means: initialize centroids, assign each point to the nearest centroid, recompute centroids, and repeat until convergence.  
- Emphasize that K-Means minimizes **within-cluster sum of squares (WCSS)** — the total squared distance of points to their cluster centroids.  
- Use visual aids to show how centroids “move” as the algorithm iterates; animation helps reinforce the idea of convergence.  
- Note that the final cluster label for each point is not predefined but *emerges* from data structure.  
- Clarify terminology: “centroid” refers to the mean vector of all points in a cluster.  

### DEEPER DIVE  
K-Means, first formalized by MacQueen (1967), remains the most widely used clustering algorithm due to its conceptual simplicity and computational efficiency. The algorithm seeks to partition \( n \) observations into \( k \) clusters such that each observation belongs to the cluster with the nearest mean, serving as a prototype of that cluster.  

The objective function is to minimize:  
\[
J = \sum_{i=1}^{k} \sum_{x_j \in C_i} \|x_j - \mu_i\|^2
\]
where \( \mu_i \) is the mean vector (centroid) of cluster \( C_i \). This criterion defines compactness — minimizing within-cluster variance while implicitly maximizing separation among clusters.  

However, K-Means assumes **Euclidean geometry**, meaning that distance is measured in a continuous, convex feature space. This assumption limits its performance on non-spherical or categorical data. Moreover, it optimizes a **non-convex** objective function, which guarantees convergence but not necessarily to the global optimum (Bishop, 2006).  

Conceptually, K-Means exemplifies *prototype-based learning*: clusters are represented by central exemplars rather than density or connectivity. This approach reflects an implicit assumption that “similarity” can be measured by proximity to an average — an idea both powerful and fragile when data distributions are irregular or contain outliers.  

**References**  
- MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. *Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability*, 1, 281–297.  
- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.  
:::

---


:::: {.columns} 
::: {.column width="50%"} 


::: 
::: {.column width="50%"} 


::: 
::::

## Initialization and convergence affect results

- **Initialization:** random starting centroids can yield poor local minima.  
  - `k-means++` improves starting positions by spreading centroids apart.  
- **Convergence:** algorithm always converges but not always to the global optimum.  
- **Choosing k:**  
  - **Elbow method:** plot within-cluster sum of squares (WCSS) vs. *k*.  
  - Identify the “elbow” where added clusters give diminishing returns.  

--

<figure>
  <img src="../materials/assets/images/kmeans_elbow_wcss_vs_k.svg"
       alt="**Figure:** Line chart of WCSS vs. k showing elbow point around optimal cluster count.">
  <figcaption>**Figure:** Line chart of WCSS vs. k showing elbow point around optimal cluster count.</figcaption>
</figure>

::: {.notes}

### Detailed Notes  
- Explain that K-Means is deterministic given its initial centroid placements, but those placements are random by default.  
- Emphasize that **poor initialization** can trap the algorithm in local minima, producing suboptimal clusters.  
- Introduce the **k-means++** initialization, which improves robustness by spreading initial centroids apart (Arthur & Vassilvitskii, 2007).  
- Demonstrate the **elbow method**: plot WCSS versus number of clusters \( k \) and identify the “elbow point” where improvements level off.  
- Clarify that while K-Means always converges, it does not guarantee finding the global minimum of the objective function.  

### DEEPER DIVE  
Initialization plays a critical role in the behavior and performance of K-Means. Because the algorithm’s optimization landscape is non-convex, random starting positions for centroids often lead to different local minima (Forgy, 1965). This sensitivity motivates the practice of running the algorithm multiple times and selecting the solution with the lowest WCSS.  

The **k-means++** initialization method significantly improves reliability by selecting initial centroids that are probabilistically distant from each other. The probability of choosing a new centroid is proportional to the squared distance from the nearest existing centroid. This ensures broader coverage of the data space and reduces the likelihood of poor local optima.  

The **Elbow Method** and its alternatives (such as the Gap Statistic) help estimate an appropriate number of clusters, though these remain heuristic rather than formal criteria. The key pedagogical insight is that clustering evaluation requires interpretive judgment — the “right” number of clusters often depends on **analytic purpose**, not just numerical optimization.  

Mathematically, convergence occurs when cluster assignments stabilize and centroids stop moving. However, the final solution depends on the initialization trajectory, and the criterion for “convergence” is typically when centroid movement falls below a threshold.  

**References**  
- Arthur, D., & Vassilvitskii, S. (2007). k-means++: The advantages of careful seeding. *Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms*, 1027–1035.  
- Forgy, E. W. (1965). Cluster analysis of multivariate data: Efficiency versus interpretability of classifications. *Biometrics, 21*(3), 768–769.  
:::

---

## Feature scaling ensures fair distance comparisons

- **Preprocessing:** standardize or normalize features before clustering.  
- **Goal:** make all variables contribute equally to distance calculations.  
- Without scaling, features with large ranges (e.g., income) dominate smaller-scale features (e.g., age).  


::: {.notes}

### Detailed Notes  
- Demonstrate that features measured on different scales distort Euclidean distance.  
- Emphasize that **standardization** (z-score scaling) or **normalization** (min–max scaling) ensures equal contribution of each variable.  
- Use a simple example (e.g., “income” in dollars vs. “age” in years) to show how unscaled features dominate the clustering outcome.  
- Reinforce that scaling is a **non-negotiable preprocessing step** for any distance-based algorithm.  

### DEEPER DIVE  
Because K-Means relies on Euclidean distance, the geometry of the feature space directly influences clustering outcomes. Variables with larger numeric ranges exert disproportionate influence on centroid computation. This violates the implicit assumption that all dimensions contribute equally to similarity.  

Two common scaling strategies address this imbalance:  

1. **Z-score standardization:**  
   \[
   x' = \frac{x - \mu}{\sigma}
   \]
   Centers features at zero mean with unit variance.  
2. **Min–max normalization:**  

$$
   x' = \frac{x - \min(x)}{\max(x) - \min(x)}
$$

   Scales all features to a uniform range, typically [0, 1].  

The choice depends on context. Standardization preserves the distribution’s shape, while normalization preserves proportional relationships.  

Beyond preprocessing, scaling also affects **distance metrics** (e.g., cosine vs. Euclidean) and thus the topology of the resulting clusters. Pedagogically, scaling illustrates that data representation — not just the algorithm — defines what “similarity” means (Tan, Steinbach, & Kumar, 2018).  

**References**  
- Tan, P.-N., Steinbach, M., & Kumar, V. (2018). *Introduction to Data Mining* (2nd ed.). Pearson.  
  
:::

---

## K-Means has practical limitations

- **Scale sensitivity:** features on large numeric scales dominate distances.  
  - Example: income (in \$) vs. purchase count.  
- **Shape limitation:** assumes clusters are convex and spherical.  
  - Struggles with elongated or irregular clusters.  
- **Fixed k:** must specify the number of clusters in advance.  


::: {.notes}

### Detailed Notes  
- Emphasize that K-Means performs best on **spherical**, **uniformly sized**, and **well-separated** clusters.  
- Explain that it fails on irregularly shaped clusters or when the number of clusters is not known in advance.  
- Note that the algorithm assumes **equal cluster variance** and **feature independence** — rarely true in real-world data.  
- Use visual examples contrasting circular versus elongated cluster shapes to illustrate limitations.  

### DEEPER DIVE  
K-Means’ core assumptions — Euclidean distance, isotropic cluster variance, and fixed \( k \) — restrict its flexibility. The method partitions the feature space using **Voronoi cells**, effectively drawing linear decision boundaries perpendicular to the midpoint between centroids. This geometry implies that clusters are convex and hyperspherical.  

When data exhibit non-convex or variable-density structures (e.g., crescent shapes, nested clusters), K-Means produces misleading partitions. Its reliance on mean vectors also makes it sensitive to **outliers**, which can dramatically shift centroids (Celebi, Kingravi, & Vela, 2013).  

Alternative algorithms like DBSCAN and Gaussian Mixture Models (GMMs) relax some of these assumptions. DBSCAN defines clusters by density rather than distance to a centroid, while GMMs allow elliptical cluster boundaries through probabilistic modeling.  

The deeper conceptual issue is that **distance-based clustering** embeds an epistemic assumption: similarity = proximity. This assumption breaks down when data relationships are non-linear, motivating the transition to **density-based** and **manifold-based** methods explored later in the module.  

**References**  
- Celebi, M. E., Kingravi, H. A., & Vela, P. A. (2013). A comparative study of efficient initialization methods for the k-means clustering algorithm. *Expert Systems with Applications, 40*(1), 200–210.  

:::

---


:::: {.columns} 
::: {.column width="50%"} 


::: 
::: {.column width="50%"} 


::: 
::::

## Hierarchical clustering builds a tree of merges without pre-specifying k

- **Agglomerative**: start with each point; iteratively **merge the closest** clusters.  
- **Dendrogram**: **merge height** = dissimilarity; **cut** the tree at a height to get *k* clusters.  
- **Linkage choices**: complete (max), average (mean), single (min).  
- **Distance**: Euclidean common; **correlation distance** groups similar *profiles*.

--

<figure>
  <img src="../materials/assets/images/hierarchical_dendrogram_cut.svg"
       alt="**Figure:** Dendrogram with a horizontal cut; clusters below the cut in distinct colors.">
  <figcaption>**Figure:** Dendrogram with a horizontal cut; clusters below the cut in distinct colors.</figcaption>
</figure>

::: {.notes}
Vertical height reflects similarity; horizontal order is arbitrary.  
Add as a reference method; we’ll stay focused on K-Means and DBSCAN in practice.  


Now, compared to K-Means and Hierarchical, DBSCAN handles irregular shapes and identifies noise explicitly ... let’s see how.

:::


# 4.3 Clustering: DBSCAN and Density-Based Methods

---


## DBSCAN groups points by density, not distance to centroids


:::: {.columns} 
::: {.column width="50%"} 


::: 
::: {.column width="50%"} 


::: 
::::

K-Means struggles with **irregular shapes** and **outliers**.

DBSCANS key advantage is **flexibility** in capturing **nonlinear** or uneven density clusters.

<figure>
  <img src="../materials/assets/images/dbscan_moons_comparison_1.svg"
       alt="**Figure:** Two datasets: left—K-Means enforces circular clusters; right—DBSCAN adapts to curved and irregular shapes.">
  <figcaption>**Figure:** Two datasets: left—K-Means enforces circular clusters; right—DBSCAN adapts to curved and irregular shapes.</figcaption>
</figure>

--

<figure>
  <img src="../materials/assets/images/dbscan_moons_comparison_2.svg"
       alt="**Figure:** Two datasets: left—K-Means enforces circular clusters; right—DBSCAN adapts to curved and irregular shapes.">
  <figcaption>**Figure:** Two datasets: left—K-Means enforces circular clusters; right—DBSCAN adapts to curved and irregular shapes.</figcaption>
</figure>

::: {.notes}

### Detailed Notes  
- Start by contrasting DBSCAN with K-Means. Emphasize that while K-Means assumes spherical, evenly sized clusters defined by distance to centroids, DBSCAN defines clusters by **density** — groups of closely packed points separated by sparse regions.  
- Explain that DBSCAN automatically detects **noise points** (outliers) and does not require pre-specifying the number of clusters.  
- Highlight that this flexibility makes DBSCAN effective on **irregular, curved, or unevenly dense** data structures.  
- Reinforce that DBSCAN is based on two core parameters: `eps` (neighborhood radius) and `min_samples` (minimum points to form a cluster).  

### DEEPER DIVE  
DBSCAN (Density-Based Spatial Clustering of Applications with Noise), introduced by Ester et al. (1996), was a pivotal development in clustering methodology. Unlike partitioning algorithms that optimize a global objective, DBSCAN operates on a **local connectivity principle**: points belong to the same cluster if they can be reached from one another through a chain of dense neighborhoods.  

Formally, for a point \( p \), the **ε-neighborhood** \( N_{\epsilon}(p) \) consists of all points within distance \( \epsilon \). A point is a **core point** if \(|N_{\epsilon}(p)| \geq \text{min\_samples}\). A cluster is then the transitive closure of all density-reachable points.  

This approach has several conceptual implications:  
- **No prior k:** Clusters emerge naturally based on data topology rather than user-specified count.  
- **Noise awareness:** Points not belonging to any dense region are explicitly labeled as outliers.  
- **Arbitrary shape handling:** Clusters can follow complex, non-convex manifolds.  

DBSCAN’s success lies in redefining “similarity” not as proximity to a center but as **membership in a dense continuum**. However, it is sensitive to the choice of ε — too small and clusters fragment; too large and distinct clusters merge.  

Pedagogically, DBSCAN demonstrates a paradigm shift: clustering as **spatial reasoning** rather than optimization.  

**References**  
- Ester, M., Kriegel, H.-P., Sander, J., & Xu, X. (1996). A density-based algorithm for discovering clusters in large spatial databases with noise. *Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96)*, 226–231.  
  
:::

---

## DBSCAN groups points by density, not distance to centroids


:::: {.columns} 
::: {.column width="50%"} 


::: 
::: {.column width="50%"} 


::: 
::::

- DBSCAN (Density-Based Spatial Clustering of Applications with Noise):  
  - Groups points based on **density** — not distance to a fixed centroid.  
  - Automatically identifies **noise points** that belong to no cluster.  
- Excels where K-Means fails — clusters of arbitrary shape or variable size.  

--

<figure>
  <img src="../materials/assets/images/DBSCAN_clustering.svg"
       alt="**Figure:** Two datasets: left—K-Means enforces circular clusters; right—DBSCAN adapts to curved and irregular shapes.">
  <figcaption>**Figure:** Two datasets: left—K-Means enforces circular clusters; right—DBSCAN adapts to curved and irregular shapes.</figcaption>
</figure>

::: {.notes}

### Detailed Notes  
- Start by contrasting DBSCAN with K-Means. Emphasize that while K-Means assumes spherical, evenly sized clusters defined by distance to centroids, DBSCAN defines clusters by **density** — groups of closely packed points separated by sparse regions.  
- Explain that DBSCAN automatically detects **noise points** (outliers) and does not require pre-specifying the number of clusters.  
- Highlight that this flexibility makes DBSCAN effective on **irregular, curved, or unevenly dense** data structures.  
- Reinforce that DBSCAN is based on two core parameters: `eps` (neighborhood radius) and `min_samples` (minimum points to form a cluster).  

### DEEPER DIVE  
DBSCAN (Density-Based Spatial Clustering of Applications with Noise), introduced by Ester et al. (1996), was a pivotal development in clustering methodology. Unlike partitioning algorithms that optimize a global objective, DBSCAN operates on a **local connectivity principle**: points belong to the same cluster if they can be reached from one another through a chain of dense neighborhoods.  

Formally, for a point \( p \), the **ε-neighborhood** \( N_{\epsilon}(p) \) consists of all points within distance \( \epsilon \). A point is a **core point** if \(|N_{\epsilon}(p)| \geq \text{min\_samples}\). A cluster is then the transitive closure of all density-reachable points.  

This approach has several conceptual implications:  
- **No prior k:** Clusters emerge naturally based on data topology rather than user-specified count.  
- **Noise awareness:** Points not belonging to any dense region are explicitly labeled as outliers.  
- **Arbitrary shape handling:** Clusters can follow complex, non-convex manifolds.  

DBSCAN’s success lies in redefining “similarity” not as proximity to a center but as **membership in a dense continuum**. However, it is sensitive to the choice of ε — too small and clusters fragment; too large and distinct clusters merge.  

Pedagogically, DBSCAN demonstrates a paradigm shift: clustering as **spatial reasoning** rather than optimization.  

**References**  
- Ester, M., Kriegel, H.-P., Sander, J., & Xu, X. (1996). A density-based algorithm for discovering clusters in large spatial databases with noise. *Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96)*, 226–231.  
  
:::

---

## Core ideas behind density-based clustering


:::: {.columns} 
::: {.column width="50%"} 


::: 
::: {.column width="50%"} 


::: 
::::

- **Density threshold:** a cluster is a dense group of points separated by sparse regions.  
- **Core point:** has at least `min_samples` neighbors within distance `eps`.  
- **Reachability:** points within `eps` of a core point belong to its cluster.  
- **Noise points:** isolated points not reachable from any dense region.  

--

<figure>
  <img src="../materials/assets/images/DBSCAN_clustering.svg"
       alt="**Figure:** Illustration labeling core, border, and noise points in a 2D dataset.">
  <figcaption>**Figure:** Illustration labeling core, border, and noise points in a 2D dataset.</figcaption>
</figure>

::: {.notes}

### Detailed Notes  
- Define the key terms explicitly:  
  - **Core points:** have at least `min_samples` neighbors within `eps`.  
  - **Border points:** lie within the neighborhood of a core point but lack enough neighbors to be core points themselves.  
  - **Noise points:** are neither core nor border points.  
- Walk through DBSCAN’s cluster-building process step by step: start from an unvisited point, identify dense regions, expand by reachability, mark noise.  
- Emphasize **density reachability** as the transitive property linking points within a cluster.  

### DEEPER DIVE  
The conceptual backbone of DBSCAN is its **local density criterion**. It reframes clustering as identifying contiguous regions of high point density in a metric space.  

The distinction between **core**, **border**, and **noise** points is not just mechanical — it reflects the algorithm’s epistemic stance on what constitutes “structure.” A core point defines a dense locality; border points extend its periphery; noise points belong to sparse regions outside any coherent structure.  

From a statistical perspective, DBSCAN approximates **mode-seeking** methods, akin to **mean shift clustering** (Comaniciu & Meer, 2002), where clusters correspond to high-density modes of an underlying probability distribution. However, DBSCAN’s binary density thresholding makes it computationally simpler and conceptually clearer for teaching purposes.  

By explicitly labeling noise, DBSCAN bridges clustering and **anomaly detection**, anticipating later sections of this module. It implicitly models the “normal” manifold and identifies points deviating from it.  


## Two key parameters control cluster formation

- **`eps` (ε):** maximum distance to consider points as neighbors.  
  - Smaller `eps` → fragmented clusters, more noise.  
  - Larger `eps` → merged clusters, less noise.  

- **`min_samples`:** minimum points required to form a dense region.  
  - Larger values → stricter density requirement, fewer clusters.  


### Detailed Notes  
- Explain the meaning of `eps` (radius) and `min_samples` (minimum cluster size).  
- Use visuals to show how different values affect outcomes:  
  - Small `eps` → fragmented clusters and excessive noise.  
  - Large `eps` → merged clusters and loss of granularity.  
- Discuss the heuristic tuning process using a **k-distance plot** — plot the sorted distances to the k-th nearest neighbor and look for a “knee” to select `eps`.  
- Emphasize that parameter tuning depends on **domain knowledge** and data scale.  

### DEEPER DIVE  
Parameter selection in DBSCAN exemplifies the trade-off between **model flexibility and interpretability**. The parameters `eps` and `min_samples` jointly determine the definition of “dense.” Their tuning reflects an analyst’s implicit prior about what counts as meaningful proximity.  

Formally, `eps` defines the spatial resolution of analysis, while `min_samples` defines statistical support. Smaller `eps` and larger `min_samples` yield stricter definitions of density — useful when distinguishing tight, well-defined clusters. Conversely, larger `eps` or smaller `min_samples` produce broader, more inclusive clusters at the cost of specificity.  

The **k-distance heuristic** automates this process by identifying the point of maximum curvature in the plot of distances to the k-th nearest neighbor. This curvature marks a threshold between dense and sparse regions.  

In advanced contexts, parameter estimation can be reframed probabilistically (Campello, Moulavi, & Sander, 2013), treating `eps` as a scale parameter in density estimation. This leads naturally into hierarchical density methods like **HDBSCAN**, which extract clusters across multiple scales of density, eliminating the need for fixed parameters.  

**References**  
- Campello, R. J. G. B., Moulavi, D., & Sander, J. (2013). Density-based clustering based on hierarchical density estimates. *Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)*, 160–172.    

**References**  
- Comaniciu, D., & Meer, P. (2002). Mean shift: A robust approach toward feature space analysis. *IEEE Transactions on Pattern Analysis and Machine Intelligence, 24*(5), 603–619.  

:::

---

## DBSCAN vs. K-Means: flexibility vs. constraint

| **Feature** | **K-Means** | **DBSCAN** |
|:--|:--|:--|
| **Cluster shape** | Spherical / convex | Arbitrary (nonlinear) |
| **Number of clusters** | Must specify *k* | Found automatically |
| **Noise handling** | None | Identifies outliers explicitly |
| **Scalability** | Fast, simple | Slower on large datasets |
| **Scale sensitivity** | High | Moderate (still requires normalization) |


::: {.notes}

### Detailed Notes  
- Use the comparison table to summarize conceptual contrasts:  
  - **Cluster shape:** spherical (K-Means) vs. arbitrary (DBSCAN).  
  - **Number of clusters:** fixed vs. automatic.  
  - **Noise handling:** absent vs. explicit.  
  - **Scalability:** K-Means is faster, DBSCAN slower but more flexible.  
- Emphasize that DBSCAN relaxes geometric constraints at the cost of computational efficiency.  
- Conclude the clustering section by previewing the next topic — dimensionality reduction — which complements clustering by simplifying feature spaces.  

### DEEPER DIVE  
This comparison highlights two competing paradigms in unsupervised learning:  
- **Prototype-based clustering (K-Means):** defines structure by minimizing variance around centroids.  
- **Density-based clustering (DBSCAN):** defines structure by continuity in local density.  

K-Means represents a **parametric** approach — simple, scalable, but constrained by assumptions about cluster shape and count. DBSCAN, in contrast, is **nonparametric** — flexible, interpretable, but computationally intensive on large datasets.  

These trade-offs illustrate the broader methodological tension between **model simplicity and expressiveness**. Prototype models generalize well and are easy to interpret but fail on complex topologies. Density-based models fit reality more closely but require greater computational care.  

The conceptual takeaway is that clustering algorithms embody different philosophical views of similarity: K-Means’ “proximity to center” vs. DBSCAN’s “membership in dense neighborhoods.” Recognizing this distinction prepares students to appreciate **manifold learning** and **graph-based** methods that follow in dimensionality reduction topics.  

**References**  
- Xu, D., & Tian, Y. (2015). A comprehensive survey of clustering algorithms. *Annals of Data Science, 2*(2), 165–193.   
:::


---


# Classical Anomaly Detection

---

## Anomaly detection identifies rare, high-impact events

- **Goal:** detect data points that deviate significantly from normal patterns.  
- **Examples:**  
  - Fraudulent transactions in finance.  
  - Faulty sensor readings in manufacturing.  
  - Unusual network activity in cybersecurity.  


::: {.notes}

### Detailed Notes  
- Begin with motivation: anomalies represent **rare but consequential** events — small in frequency but large in impact.  
- Provide relatable examples: fraudulent transactions, equipment failure, or cybersecurity breaches.  
- Emphasize that anomaly detection focuses on **identifying deviations** from normal patterns, not predicting labels.  
- Reinforce that this is a fundamentally **imbalanced problem**: the normal class dominates, and anomalies are sparse.  
- Use this slide to frame anomaly detection as a **risk management tool**, not merely a statistical exercise.  

### DEEPER DIVE  
Anomaly detection formalizes the process of identifying observations that deviate significantly from the majority of data. Its intellectual roots trace back to early **statistical process control** in manufacturing (Shewhart, 1931), where the goal was to detect shifts or defects in production before catastrophic failure.  

In modern contexts, anomaly detection supports domains where **ground truth is scarce**, **risk asymmetry** is high, and **novelty** is expected. Examples include fraud detection, health monitoring, and intrusion detection.  

Conceptually, anomalies can arise from:  
1. **Noise:** random fluctuations that break regularity.  
2. **Novelty:** genuinely new phenomena outside prior experience.  
3. **Systemic change:** drift or regime shift in data generation processes.  

Mathematically, anomaly detection reframes the notion of *probability density*: anomalies reside in the tails of the data distribution. Density estimation methods (e.g., Gaussian models, kernel density) interpret anomalies as low-probability events.  

Pedagogically, this slide repositions the student’s mindset: anomaly detection is not about labeling “wrong” data, but about discovering meaningful exceptions that warrant explanation or intervention.  

**References**  
- Shewhart, W. A. (1931). *Economic Control of Quality of Manufactured Product*. D. Van Nostrand Company.  
- Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. *ACM Computing Surveys, 41*(3), 1–58.  
:::

---

## Three main strategies for detecting anomalies

| **Approach** | **Concept** | **Intuition** |
|:--|:--|:--|
| **Isolation-based** | Separate anomalies quickly via random splits | Outliers are easy to isolate |
| **Boundary-based** | Learn a frontier around normal data | Anomalies fall outside the boundary |
| **Reconstruction-based** | Measure how well data can be rebuilt | Poor reconstruction → anomaly |


::: {.notes}

### Detailed Notes  
- Introduce the three core paradigms of anomaly detection: **isolation-based**, **boundary-based**, and **reconstruction-based**.  
- Emphasize the different logics:  
  - Isolation-based: anomalies are easy to separate.  
  - Boundary-based: anomalies fall outside learned normal boundaries.  
  - Reconstruction-based: anomalies cannot be well reconstructed from compressed representations.  
- Explain that these paradigms span both classical and modern (deep learning) approaches.  
- Use the table to organize conceptual understanding before diving into algorithms.  

### DEEPER DIVE  
These three paradigms represent fundamentally different ways to formalize “deviation.”  

1. **Isolation-based methods** (e.g., Isolation Forest) rely on the intuition that anomalies can be separated from the bulk of the data with fewer rules or splits. This approach is algorithmically efficient and does not require explicit modeling of normality.  
2. **Boundary-based methods** (e.g., One-Class SVM) define a decision boundary that encloses normal instances; anomalies are points lying outside this boundary. This approach connects directly to the geometry of **support vector machines**, where normality corresponds to a high-density region in feature space.  
3. **Reconstruction-based methods** (e.g., PCA reconstruction error, autoencoders) measure how well a model trained on normal data can reproduce inputs. Poorly reconstructed points are flagged as anomalies.  

These strategies mirror broader epistemic traditions in AI: *discriminative* (boundary), *generative* (reconstruction), and *heuristic* (isolation). Each reflects a different trade-off among interpretability, scalability, and flexibility.  

Pedagogically, this taxonomy provides a mental map for navigating diverse algorithms — emphasizing conceptual unity beneath surface diversity.  

**References**  
- Zimek, A., Schubert, E., & Kriegel, H.-P. (2012). A survey on unsupervised outlier detection in high-dimensional numerical data. *Statistical Analysis and Data Mining: The ASA Data Science Journal, 5*(5), 363–387.  
:::

---

## Isolation Forest isolates anomalies instead of modeling normal data


:::: {.columns} 
::: {.column width="50%"} 


::: 
::: {.column width="50%"} 


::: 
::::

- Builds many **random trees** that split features recursively.  
- **Key idea:** outliers are easier to isolate → have **shorter path lengths** in the trees.  
- **Anomaly score:** average path length across trees (shorter = more anomalous).  
- **Strengths:** fast, scalable, effective in high dimensions.  

--

<figure>
  <img src="../materials/assets/images/anomaly_isolation_forest_concept.svg"
       alt="**Figure:** Forest of random splits showing shallow isolation of anomalies vs. deeper paths for normal data.">
  <figcaption>**Figure:** Forest of random splits showing shallow isolation of anomalies vs. deeper paths for normal data.</figcaption>
</figure>

::: {.notes}

### Detailed Notes  
- Introduce **Isolation Forest** as an algorithm that detects anomalies by randomly partitioning data.  
- Explain the core intuition: anomalies are easier to isolate and require fewer splits in a decision-tree-like structure.  
- Emphasize that this is **nonparametric** — it does not assume a specific data distribution.  
- Highlight key advantages: scalability, interpretability, and robustness to high-dimensional noise.  
- Connect conceptually to students’ prior understanding of **decision trees** from supervised learning.  

### DEEPER DIVE  
Isolation Forest (Liu, Ting, & Zhou, 2008) departs from traditional density or distance-based frameworks. Instead of estimating what “normal” looks like, it isolates anomalies directly. The algorithm constructs multiple random binary trees (isolation trees) by recursively partitioning the feature space.  

For each point \( x \), the algorithm computes its **average path length** across trees — the number of splits required to isolate it. Anomalies, being distinct, tend to be separated earlier, resulting in shorter paths.  

The anomaly score is defined as:  
\[
s(x, n) = 2^{-\frac{E(h(x))}{c(n)}}
\]
where \( E(h(x)) \) is the expected path length, and \( c(n) \) is the average path length of unsuccessful searches in binary trees of size \( n \).  

Conceptually, Isolation Forest embodies an **algorithmic minimalism**: instead of modeling probability distributions, it models *separability*. This simplicity yields high scalability (linear in \( n \)) and excellent performance on tabular data.  

Pedagogically, the algorithm’s interpretability makes it ideal for teaching anomaly detection — it connects intuitive geometric reasoning (isolation) with probabilistic scoring.  

**References**  
- Liu, F. T., Ting, K. M., & Zhou, Z.-H. (2008). Isolation forest. *Proceedings of the 2008 IEEE International Conference on Data Mining*, 413–422.  
:::

---


:::: {.columns} 
::: {.column width="50%"} 


::: 
::: {.column width="50%"} 


::: 
::::

## One-Class SVM defines a flexible boundary around normal data

- Learns a **soft boundary** that encloses most of the data.  
- Points outside → **anomalies**.  
- Uses **kernel functions** to create nonlinear boundaries.  
- **Pros:** flexible, works in complex feature spaces.  
- **Cons:** sensitive to hyperparameters, slower on large datasets.  

--

<figure>
  <img src="../materials/assets/images/SVM_Anomaly.svg"
       alt="**Figure:** Two-dimensional feature space with One-Class SVM circular boundary; anomalies fall outside.">
  <figcaption>**Figure:** Two-dimensional feature space with One-Class SVM circular boundary; anomalies fall outside.</figcaption>
</figure>

::: {.notes}

### Detailed Notes  
- Introduce **One-Class SVM** as a boundary-based method.  
- Emphasize that it learns a **decision function** that encloses most of the data, labeling points outside the boundary as anomalies.  
- Explain that the boundary can be **nonlinear** via kernel functions (RBF, polynomial, etc.).  
- Note strengths: adaptable to complex feature spaces.  
- Note weaknesses: computationally expensive and sensitive to parameter tuning (`nu`, `gamma`).  

### DEEPER DIVE  
The One-Class Support Vector Machine (Schölkopf et al., 2001) extends the geometric intuition of traditional SVMs to unsupervised settings. It learns a function \( f(x) \) that is positive for most of the data (normal) and negative for outliers.  

Mathematically, it solves:  
\[
\min_{w,\xi,\rho} \frac{1}{2}\|w\|^2 + \frac{1}{\nu n}\sum_i \xi_i - \rho
\]
subject to  
\[
(w \cdot \phi(x_i)) \geq \rho - \xi_i, \quad \xi_i \geq 0
\]
where \( \phi(x) \) is a feature mapping induced by a kernel, \( \nu \) controls the expected proportion of anomalies, and \( \rho \) defines the decision boundary offset.  

This framework implicitly estimates a high-density region in feature space, drawing an enclosing “shell” around the data. Anomalies fall outside this shell.  

One-Class SVM bridges classical geometric intuition with modern kernel methods — illustrating how nonlinear transformations expand the expressive power of traditional algorithms. However, its reliance on kernel choice and scaling makes it less robust for large or heterogeneous datasets.  

Pedagogically, this slide links anomaly detection back to familiar supervised learning geometry, reinforcing conceptual continuity.  

**References**  
- Schölkopf, B., Platt, J. C., Shawe-Taylor, J., Smola, A. J., & Williamson, R. C. (2001). Estimating the support of a high-dimensional distribution. *Neural Computation, 13*(7), 1443–1471.  

:::

---

## Comparing classical anomaly detection methods

| **Criterion** | **Isolation Forest** | **One-Class SVM** |
|:--|:--|:--|
| **Concept** | Randomly isolates outliers | Learns boundary around normal data |
| **Scalability** | Excellent (fast, parallel) | Moderate (depends on kernel) |
| **Interpretability** | High (path length) | Moderate (opaque boundaries) |
| **Tuning needs** | Low | High (`nu`, `gamma`, kernel type) |
| **Best for** | Large tabular data | Smaller, nonlinear feature spaces |

::: {.notes}

### Detailed Notes  
- Use the table to summarize trade-offs between Isolation Forest and One-Class SVM.  
- Emphasize that method selection depends on data characteristics (size, dimensionality, distribution complexity).  
- Reinforce that no single algorithm dominates across all contexts.  
- Encourage students to think of **anomaly detection as context-sensitive** — the cost of false alarms vs. misses determines choice.  

### DEEPER DIVE  
Comparing classical algorithms highlights enduring trade-offs in machine learning design:  

| **Criterion** | **Isolation Forest** | **One-Class SVM** |  
|---------------|----------------------|-------------------|  
| Model Type | Random tree ensemble | Kernel-based boundary |  
| Scalability | Excellent (O(n log n)) | Moderate (depends on kernel) |  
| Assumptions | None (random partitioning) | Implicit density enclosure |  
| Interpretability | High (path length) | Moderate |  
| Data Suitability | Tabular, high-dim | Low-dim, nonlinear |  

This comparison illustrates a broader principle: **simplicity scales, flexibility adapts**. Isolation Forest’s randomness yields generalization, while One-Class SVM’s mathematical rigor provides precision at computational cost.  

In practice, many pipelines combine the two: PCA or autoencoder-based compression followed by Isolation Forest or One-Class SVM in the latent space.  

Pedagogically, this comparison reinforces model selection as a **strategic decision** balancing speed, interpretability, and domain risk.  

**References**  
- Tax, D. M. J., & Duin, R. P. W. (2004). Support vector data description. *Machine Learning, 54*(1), 45–66.  
  
:::

---


## (Optional Reading) Representation-Based Anomaly Detection  *(Advanced Preview)*

---

## Bridge: from classical boundaries to learned representations


:::: {.columns} 
::: {.column width="50%"} 


::: 
::: {.column width="50%"} 


::: 
::::

- Classical models rely on **distance** or **boundary rules** in raw feature space.  
- Modern deep learning models learn **latent representations** of “normality.”  
- These capture richer, nonlinear relationships before evaluating anomalies.  
- Shift from **geometry-based** to **representation-based** detection.  

--

<figure>
  <img src="../materials/assets/images/anomaly_representation_bridge.svg"
       alt="**Figure:** Flow diagram — raw data → learned latent space → anomaly scoring.">
  <figcaption>**Figure:** Flow diagram — raw data → learned latent space → anomaly scoring.</figcaption>
</figure>

::: {.notes}

### Detailed Notes  
- Introduce this as a conceptual bridge rather than a technical deep dive.  
- Emphasize that classical anomaly detection operates directly in the **original feature space**, whereas modern approaches first learn **latent representations** of normality.  
- Explain that deep learning methods capture **nonlinear, hierarchical features**, allowing richer modeling of complex systems.  
- Highlight that this shift moves from **geometry-based reasoning** to **representation-based reasoning**.  
- Reinforce that this section previews ideas that will reappear in the deep learning module.  

### DEEPER DIVE  
Representation-based anomaly detection marks a paradigm shift from surface-level pattern recognition to **latent structure modeling**. Classical algorithms define normality in observed feature space (e.g., Euclidean distance, kernel boundaries), but deep methods learn transformations \( f_{\theta}(x) \) that map raw inputs into compact, informative latent spaces.  

These latent representations are designed such that “normal” data occupy dense, coherent regions, while anomalies fall in sparse, low-probability zones. Conceptually, this shift mirrors broader developments in AI: moving from **explicit modeling of patterns** to **implicit modeling of data-generating processes** (LeCun, Bengio, & Hinton, 2015).  

Representation learning captures *features of normality* that are invariant to irrelevant variation — for example, lighting or noise in images, or seasonality in time series. This makes anomaly detection more robust but less interpretable.  

Pedagogically, this slide introduces the intellectual bridge between classical geometry and learned manifolds. It situates anomaly detection within the broader trajectory of AI, from algorithms that measure to models that *understand structure*.  

**References**  
- LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. *Nature, 521*(7553), 436–444.  
:::

---


:::: {.columns} 
::: {.column width="50%"} 


::: 
::: {.column width="50%"} 


::: 
::::

## Autoencoders reconstruct normal data and expose anomalies

- **Idea:** train a neural network to reproduce its input.  
- **Training:**  
  - The encoder compresses input → latent space.  
  - The decoder reconstructs it → output.  
- **Anomaly rule:**  
  - Normal samples → low reconstruction error.  
  - Anomalies → high reconstruction error.  
- Works well on sensor data, transactions, or images.  

--

<figure>
  <img src="../materials/assets/images/anomaly_autoencoder_architecture.svg"
       alt="**Figure:** Autoencoder architecture showing input → bottleneck → reconstructed output; anomalies have higher error.">
  <figcaption>**Figure:** Autoencoder architecture showing input → bottleneck → reconstructed output; anomalies have higher error.</figcaption>
</figure>

::: {.notes}

### Detailed Notes  
- Introduce **autoencoders** as neural networks trained to reconstruct their input data.  
- Explain that during training, the network learns a **compressed latent representation** of normal examples.  
- Emphasize that normal data are reconstructed accurately, while anomalies show **high reconstruction error**.  
- Connect this to PCA: autoencoders generalize linear reconstruction to nonlinear manifolds.  
- Reinforce that this method assumes anomalies differ structurally from normal patterns, not just statistically.  

### DEEPER DIVE  
An **autoencoder** consists of two neural components: an **encoder** \( f_{\theta} \) that maps inputs to a lower-dimensional latent vector \( z = f_{\theta}(x) \), and a **decoder** \( g_{\phi} \) that reconstructs the input \( \hat{x} = g_{\phi}(z) \). Training minimizes the reconstruction loss:  
\[
L(x, \hat{x}) = \|x - \hat{x}\|^2
\]
or, in probabilistic formulations, negative log-likelihood.  

During training, only normal data are presented. The autoencoder thus learns a **manifold of normality** — a low-dimensional surface embedded in feature space. Anomalies, which lie off this manifold, yield large reconstruction errors.  

Conceptually, autoencoders extend PCA’s linear projection to nonlinear, multi-layer mappings capable of capturing curved manifolds. Variational autoencoders (VAEs) add probabilistic structure, modeling the latent space as a learned distribution \( p(z|x) \).  

Autoencoder-based anomaly detection excels in high-dimensional domains such as sensor telemetry, credit transactions, and medical imaging, where “normal” patterns follow complex nonlinear dependencies.  

Pedagogically, this slide introduces students to **representation learning as compression** — a core idea underpinning generative AI. It also reinforces that unsupervised models can learn implicit knowledge structures from raw data without labels.  

**References**  
- Hinton, G. E., & Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural networks. *Science, 313*(5786), 504–507.  
- Kingma, D. P., & Welling, M. (2014). Auto-encoding variational Bayes. *arXiv preprint arXiv:1312.6114.*  
  
:::

---

## Classical vs. representation-based detection

| **Aspect** | **Classical (IF, SVM)** | **Representation-Based (AE, SVDD)** |
|:--|:--|:--|
| **Feature space** | Original data space | Learned latent representation |
| **Model type** | Tree / kernel | Neural network |
| **Nonlinearity** | Limited | High |
| **Data size need** | Moderate | Large |
| **Interpretability** | High | Low |
| **Use case** | Tabular / simple | Images, sensors, embeddings |


::: {.notes}

### Detailed Notes  
- Use the comparison table to summarize how deep representation-based methods differ from classical approaches.  
- Emphasize that representation learning enables **nonlinear manifolds**, while classical methods rely on linear or distance-based relationships.  
- Reinforce that interpretability decreases as flexibility increases.  
- Highlight use cases: autoencoders for sensor and image data, Deep SVDD for embeddings and features.  
- Encourage framing these methods as **extensions, not replacements**, of classical techniques.  

### DEEPER DIVE  
The evolution from classical to deep anomaly detection reflects broader transitions in machine learning:  
- From **explicit geometric reasoning** to **implicit representation learning**.  
- From **distance-based metrics** to **latent space modeling**.  
- From **handcrafted features** to **learned features**.  

This transformation brings both advantages and challenges. Deep methods uncover complex dependencies but often obscure causality and interpretability. As Ribeiro, Singh, and Guestrin (2016) argue, interpretability is crucial for trustworthy AI; thus, combining representation-based detection with post-hoc explainability (e.g., SHAP, LIME) is increasingly standard practice.  

Pedagogically, this slide encourages meta-level reflection: as algorithms grow more powerful, the need for human oversight and conceptual understanding increases proportionally.  

**References**  
- Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). "Why should I trust you?": Explaining the predictions of any classifier. *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, 1135–1144.  
 
:::

---

## Takeaway: understanding and evolution

- **Classical methods:** fast, interpretable, and broadly applicable.  
- **Representation-based methods:** powerful for complex, high-dimensional data.  
- Both aim to model what’s *normal* to detect what’s *not*.  
- This progression mirrors the broader trend in AI — from rule-based systems to learned representations.  


::: {.notes}

### Detailed Notes  
- Conclude this section by synthesizing the progression from classical to representation-based approaches.  
- Reinforce that both aim to **define normality** in order to detect deviations — they differ only in how they represent structure.  
- Emphasize the conceptual through-line: from geometric intuition to deep representation learning.  
- Highlight that mastering classical intuition remains essential to understanding deep models.  
- Use this as a transition into the next module on evaluation and trust.  

### DEEPER DIVE  
The evolution of anomaly detection encapsulates the story of modern AI: increasing abstraction, representation, and scale.  
- **Classical methods** (Isolation Forest, One-Class SVM) operate in explicit spaces, offering speed, transparency, and broad applicability.  
- **Representation-based methods** (Autoencoders, Deep SVDD) operate in latent manifolds, capturing subtle dependencies invisible to linear models.  

Yet both are bound by the same epistemic constraint: without labels, the definition of “anomaly” depends on **context and interpretation**. This underscores that anomaly detection is not a purely technical exercise but a form of *epistemic judgment*.  

From an educational standpoint, this module illustrates how AI advances do not erase foundational methods — they reinterpret them through new architectures. Understanding this lineage fosters intellectual humility and adaptability — key traits of a responsible AI practitioner.  

**References**  
- Ruff, L., Kauffmann, J. R., Vandermeulen, R. A., Montavon, G., Samek, W., Kloft, M., ... & Müller, K.-R. (2021). A unifying review of deep and shallow anomaly detection. *Proceedings of the IEEE, 109*(5), 756–795.  


:::

---


# Evaluation in Unsupervised Learning

---

## Evaluating unsupervised models requires evidence, less related to some type of accuracy

- In **supervised learning**, metrics like accuracy or RMSE are clear.  
- In **unsupervised learning**, there are **no ground truth labels**.  
- The central challenge:  
  - *How do we know if our discovered clusters or anomalies are meaningful?*  
- Evaluation depends on **pattern coherence, visualization, Stability/robustness checks and domain validation**.  


::: {.notes}

### Detailed Notes  
- Open this section by reframing evaluation: unlike supervised learning, **unsupervised models lack ground truth**.  
- Emphasize that metrics like accuracy, precision, and recall do not apply without labels.  
- Highlight that evaluation in unsupervised contexts relies on three pillars: **structure**, **stability**, and **interpretability**.  
- Use examples: cluster coherence, visual inspection, expert validation.  
- Reinforce that evaluation is **interpretive** and **evidence-driven** — guided by plausibility, not prediction error.  

### DEEPER DIVE  
Unsupervised evaluation presents an epistemological challenge: in the absence of labels, “correctness” must be inferred from internal consistency and external utility. The goal is not to prove the model right but to establish that it reveals **meaningful and stable structure**.  

Three primary evaluation lenses dominate:  
1. **Internal validation** — measures the quality of structure (e.g., silhouette scores, compactness, separation).  
2. **Stability validation** — assesses robustness under perturbations (e.g., resampling, initialization changes).  
3. **External validation** — relies on expert interpretation or partial labeling.  

This marks a philosophical shift: the model’s value lies not in predictive power but in **interpretive trust** (Doshi-Velez & Kim, 2017). Evaluation becomes a process of triangulation — aligning mathematical coherence, visual plausibility, and domain meaning.  

Pedagogically, this slide reframes how students think about “model performance.” In unsupervised learning, good models don’t predict better; they *reveal structure more convincingly*.  

**References**  
- Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. *arXiv preprint arXiv:1702.08608.*

:::

---

## Do you have labels?

- When labels exist, use quantitative validation. But beware. 
  - **Silhouette Score:**  Measures how well points fit within their cluster vs. others. Good for checking internal consistency.  
  - **Adjusted Rand Index (ARI):** Compares predicted clusters to known classes.  Useful only when some labeled benchmark exists. But then... are you sure you need to be doing unsupervised learning?   

- When no labels exist: rely on visualization and expert judgment
  - **Visualization:** Use PCA, t-SNE, or UMAP to project results into 2D/3D. Check if clusters or anomalies look plausible.
  - **Domain validation:**  Are discovered groups meaningful to experts?  Do anomalies align with real unusual cases (e.g., known fraud, failed sensors)?  

::: {.notes}

### Detailed Notes  
- Explain that when partial labels or known groupings exist, quantitative metrics can assess clustering quality.  
- Introduce the **Silhouette Score**: measures how similar a point is to its own cluster versus others.  
- Introduce the **Adjusted Rand Index (ARI)**: compares predicted clusters to true labels, adjusting for chance.  
- Emphasize that these metrics are helpful but limited — they depend on label availability or assumptions about geometry.  

- Emphasize that most real-world unsupervised projects lack labeled data.  
- Teach the **human-in-the-loop** evaluation approach: combine algorithmic results with domain expertise.  
- Use PCA, t-SNE, or UMAP visualizations to examine whether clusters or anomalies make sense.  
- Encourage iterative refinement: adjust parameters, re-visualize, and validate with experts.  
- Reinforce that **plausibility and utility** are the primary success criteria.

### DEEPER DIVE  
Quantitative validation metrics formalize the intuitive notion of “good clusters.”  

**Silhouette Coefficient** (Rousseeuw, 1987):  
\[
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
\]  
where \( a(i) \) is the mean intra-cluster distance, and \( b(i) \) is the mean nearest-cluster distance. Scores near 1 indicate well-separated, coherent clusters; scores near 0 suggest overlap.  

**Adjusted Rand Index (ARI)** measures agreement between two partitions (true vs. predicted) while correcting for random chance. It is defined as:  
\[
ARI = \frac{\text{Index} - \text{Expected Index}}{\text{Max Index} - \text{Expected Index}}
\]  

While useful in benchmarking (e.g., comparing algorithms on labeled datasets), these metrics are rarely applicable in business or scientific contexts where no “true” grouping exists. They should be treated as **diagnostic aids**, not definitive evaluations.  

Pedagogically, this slide illustrates a broader idea: quantitative measures can inform confidence but cannot replace interpretation.  

Visualization serves as both an analytical and epistemic validation tool in unsupervised learning. Low-dimensional projections (via PCA, t-SNE, or UMAP) allow qualitative assessment of structural coherence and anomaly plausibility.  

However, human judgment remains indispensable. Experts can evaluate whether discovered groups align with meaningful categories (e.g., customer types, biological pathways). This process mirrors **grounded theory** in qualitative research — patterns emerge through interpretation, not predefinition (Charmaz, 2014).  

This hybrid human–machine validation represents an **interpretive loop**:  
1. Model discovers potential structure.  
2. Humans evaluate relevance and coherence.  
3. Insights inform model refinement.  

This cycle is not a failure of automation but a recognition that knowledge discovery is a **socio-technical process**. Models propose; experts dispose.  

Pedagogically, this slide reinforces humility in AI practice — unsupervised learning reveals possibilities, not truths.  

**References**  
- Charmaz, K. (2014). *Constructing Grounded Theory* (2nd ed.). SAGE Publications.  

- Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. *Journal of Computational and Applied Mathematics, 20*, 53–65.  

:::
  
---

## Practical takeaway: evaluate structure, not prediction

- There is **no universal score** for unsupervised learning.  
- Evaluate through **three complementary lenses:**  
  1. **Structure:** Does the model reveal coherent, stable patterns?  
  2. **Interpretation:** Do results make sense to experts?  
  3. **Impact:** Does it support real-world decision-making or risk reduction?  
 


::: {.notes}

### Detailed Notes  
- Conclude by synthesizing the philosophy of unsupervised evaluation.  
- Reinforce that **structure**, **interpretation**, and **impact** are the three pillars of validation.  
- Encourage documenting both visual evidence and expert rationale.  
- Highlight that the goal is **trustworthy insight**, not perfect scores.  
- Transition to upcoming modules (e.g., model monitoring and interpretability).  

### DEEPER DIVE  
Unsupervised learning demands a new epistemology of evaluation — one grounded in coherence, plausibility, and utility. Because there is no ground truth, the standard of success becomes *trustworthy understanding*.  

Three interdependent dimensions define this:  
1. **Structural coherence** — does the model uncover stable, interpretable relationships?  
2. **Expert interpretability** — do results align with domain understanding or generate plausible hypotheses?  
3. **Decision impact** — does the model improve insight, prediction, or risk management downstream?  

This tripartite framework parallels Lincoln and Guba’s (1985) criteria for qualitative research: *credibility, transferability, and dependability*. Unsupervised evaluation, like ethnography, relies on triangulation rather than single metrics.  

Pedagogically, this final slide anchors the module’s core lesson: unsupervised learning is a **dialogue between models and meaning**. Evaluation is the language that makes that dialogue rigorous and credible.  

**References**  
- Lincoln, Y. S., & Guba, E. G. (1985). *Naturalistic Inquiry*. SAGE Publications.  

:::



